(function(){const a=[{idx:0,href:"/notebook/docs/",title:"About",content:"About  Masaki E. Tsuda 津田 真樹（つだ　まさき） Twitter : teuder  "},{idx:1,href:"/notebook/python/",title:"Python",content:"Python  東京大学のPythonプログラミング入門 リファレンスマニュアル  組み込み関数    コーディング規約 PEP8\n関数定義 # センチメートルをインチに変換する関数 def cm_to_inch(cm): return cm / 2.54 演算子 # 論理演算 x or y x and y not x # 比較演算 x \u0026lt; y x \u0026lt;= y x \u0026gt; y x \u0026gt;= y x == y # 値が等しい x != y # 値が等しくない x is y # 同一のオブジェクトである x is not y # 同一のオブジェクトではない 論理演算 論理値型 bool() x = True y = False # 論理演算 x and y x or y not x x \u0026amp; y # and x | y # or x ^ y # xor 数値計算 数値型 1 # int 整数 1.0 # float 実数 complex(1,2) # 複素数 浮動小数点数は、文字列 \u0026ldquo;nan\u0026rdquo; と \u0026ldquo;inf\u0026rdquo; を、オプションの接頭辞 \u0026ldquo;+\u0026rdquo; または \u0026ldquo;-\u0026rdquo; と共に、非数 (Not a Number (NaN)) や正、負の無限大として受け付けます。\n基本数値演算 print(1+2) print(3-4) print(5*6) print(10/3) # 返値は実数 print(3**2) # べき乗 print(pow(3, 2)) # べき乗 print(10//3) # 割った時の「商」 print(10%3) # 割った時の「余り」 print(divmod(10,3)) # (商, 余り) の tupleが返値 print(abs(-123)) # 絶対値 出力結果\n3 -1 30 3.3333333333333335 9 3 1 x = 1 x += 1 print(x) # 2 数値型の変換 # 実数を整数に変換 # 小数点以下を切り捨て print(int(9.1)) print(int(-9.1)) print(int(0.8)) 9 -9 0 文字列 文字列およびバイト列リテラル\ns1 = \u0026#34;ほげ\u0026#39;あほ\u0026#39;ほげ\u0026#34; # \u0026#34; \u0026#34; で生成するときは内部に \u0026#39; を入れられる s2 = \u0026#39;ほげ\u0026#34;あほ\u0026#34;ほげ\u0026#39; # \u0026#39; \u0026#39; で生成するときは内部に \u0026#34; を入れられる s3 = \u0026#34;\u0026#34;\u0026#34;改行をいれるときは、 クォート3つで生成する \u0026#34;\u0026#34;\u0026#34; print(s1) print(s2) print(s3) ほげ'あほ'ほげ ほげ\u0026quot;あほ\u0026quot;ほげ 改行をいれるときは、 クォート3つで生成する メソッド str.capitalize() 文字列 str の先頭を大文字、残りを小文字にした文字列を返す\nstr = \u0026#34;aBcDe\u0026#34; print(str.capitalize()) #  Abcde\nstr.casefold() 文字列 str の英語以外のアルファベットも、英語のアルファベット小文字化する\nstr = \u0026#34;aBcDe\u0026#34; print(str.casefold()) abcde\nstr.center(width, fillchar) 元の文字列 str を中央に配置した、長さ width の文字列を生成する、足りない部分は文字列 fillchar （デフォルトはスペース）で埋める。\nstr = \u0026#34;aBcDe\u0026#34; print(str.center(10, \u0026#34;x\u0026#34;)) str.count(sub, start, end) 文字列 str 内にある部分文字列 sub が出現する個数を数える。指定された場合は star 文字目から end 文字目の間を検索する。 マッチする個数を数えるとき、一度マッチした部分は以降の検索では除外される。\n# 部分文字列のカウント str2 = \u0026#34;aaaaaAAAAAaaaaaAAAAA日本語はどうですか日本語\u0026#34; print(str2.count(\u0026#34;日本語\u0026#34;)) print(str2.count(\u0026#34;aa\u0026#34;,3)) print(str2.count(\u0026#34;aa\u0026#34;,3, 13)) 2 3 2 str.endswith(suffix, start, end) 文字列 str が文字列 suffix で終わる場合は True を返す。指定された場合は start 番目から end 番目までの部分文字列を判定する。\nstr.find(sub, start, end) 文字列 str に部分文字列 sub が含まれる場合、その最小のインデックスを返します。インデックス start, end が指定されたら部分文字列 str[start:end] の中で検索します。sub が見つからなかった場合 -1 を返します。\n\u0026#34;Python\u0026#34;.find(\u0026#34;th\u0026#34;) # 2 文字列 str に部分文字列 sub が存在するかどうかだけを知りたい場合には in を使う。\n\u0026#34;Py\u0026#34; in \u0026#34;Python\u0026#34; # True str.index(sub, start, end) str.find() と同じだが、sub が見つからないとき ValueError を送出する。\nファイルを文字列として読み込む f = open(\u0026#39;path_to_file\u0026#39;, \u0026#39;r\u0026#39;) data = f.read() f.close() テンプレート文字列の置換 オブジェクトの値を用いてテンプレート文字列の中身を置換する\nformat() メソッド project = \u0026#39;my_project\u0026#39; dataset = \u0026#39;my_dataset\u0026#39; table = \u0026#39;my_table\u0026#39; # format メソッドを使った方法 1  template = \u0026#39;SELECT size_bytes FROM `{project}.{dataset}.{table}`\u0026#39; print(template.format(project=project, dataset=dataset, table=table)) # SELECT size_bytes FROM `my_project.my_dataset.my_table` # format メソッドを使った方法 2 # テンプレートに置換する名前を指定しない場合は、与えたオブジェクトが順に適用される template = \u0026#39;SELECT size_bytes FROM `{}.{}.{}`\u0026#39; print(template.format(project, dataset, table)) # SELECT size_bytes FROM `my_project.my_dataset.my_table` # テンプレート内の {数字} はformat()関数に与えた引数の位置として解釈される template = \u0026#39;SELECT size_bytes FROM `{0}.{1}.{2}`\u0026#39; print(template.format(project, dataset, table)) # SELECT size_bytes FROM `my_project.my_dataset.my_table` f文字列を使った方法 こちらの方が format メソッドより簡単だが、テンプレート文字列をオブジェクトして保存した場合には使えない\nproject = \u0026#39;my_project\u0026#39; dataset = \u0026#39;my_dataset\u0026#39; table = \u0026#39;my_table\u0026#39; # f文字列を使った方法、変数内の値が展開される print(f\u0026#39;SELECT size_bytes FROM `{project}.{dataset}.{table}`\u0026#39;) str.format_mapping(mapping) # dict型の値を作成 d = {\u0026#34;name\u0026#34;: \u0026#34;Mike\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;America\u0026#34;} # dict を使ってテンプレート文字列を置換する str = \u0026#39;{name}was born in {country}\u0026#39; str.format_map(d) 文字列種の判定  str.isalnum() str.isalpha() str.isascii() str.isdecimal() str.isdigit() str.isidentifier() str.islower() str.isnumeric() str.isprintable() str.isspace() str.istitle() str.isupper()   シーケンスコンテナ list tuple range\nこれらに共通の演算\n s と t は同じ型のシーケンス n、 i、 j 、 k は整数 x は s に課された型と値の条件を満たす任意のオブジェクトです。  x in s # s のある要素が x と等しければ True , そうでなければ False x not in s # s のある要素が x と等しければ False, そうでなければ True s + t # s と t の結合 s * n # s 自身を n 回足すのと同じ s[i] # s の 0 から数えて i 番目の要素 s[i:j] # s の i から j までのスライス s[i:j:k] # s の i から j まで、 k 毎のスライス len(s) # s の長さ（要素数） min(s) # s の最小の要素 max() # s の最大の要素 s.index(x) # s 中で値 x が最初に出現するインデックス s.index(x, i) # s の i 番目以降の要素で、値 x が最初に出現するインデックス s.index(x, i, j) # s の i 番目から j 番目の要素の中で x が最初に出現するインデックス  list 同じ型の値を要素として持つシーケンスコンテナ\nl1 = [] # 空のリスト l2 = [1,2,3] l3 = [x**2 for x in range(0,3)] # リスト内包表記を使ったリストの生成 l4 = [x**2 for x in [0,1,2,3]] #  print(l3) print(l4) [0, 1, 4] [0, 1, 4, 9] メソッド sort() l = [3,4,2,9,7] l.sort() # lの要素を直接ソートする、ソートしたリストを返さない print(l) [2, 3, 4, 7, 9] l = [3,4,2,9,7] l.sort(reverse = True) # lの要素を逆順でソートする print(l) [9, 7, 4, 3, 2] 元のリストをそのままにして、ソートされたリストが欲しい時は sorted() を利用する\nl1 = [3,4,2,9,7] l2 = sorted(l1) print(l) print(l2) [3, 4, 2, 9, 7] [2, 3, 4, 7, 9] tuple タプルはイミュータブルの型なので、値は変更できない、基本的に使い捨て\n(1,2,3) # 丸括弧で作成するのが基本 1,2,3 # 丸括弧なしでも作成できる tuple((1,2,3)) # tupleで tuple を初期化 tuple([\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;]) # list で tuple を初期化 名前付き tuple from collections import namedtuple # 名前付きタプルのクラスを定義 Point = namedtuple(\u0026#39;Point\u0026#39;, [\u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;]) # インスタンスを生成 p = Point(x=1, y=2) # インスタンスを表示 print(p) # インデックスで値にアクセス print(p[0]) print(p[1]) # 属性名から値にアクセス z = p.x + p.y print(z) range 整数の範囲を示す\nforループやlistを初期化するときに使うことが多い\nprint(list(range(10))) # 終了10 print(list(range(1, 15))) # 開始1 終了15 print(list(range(1, 15,3))) # 開始1 終了15 間隔3 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] [1, 4, 7, 10, 13] リスト内包表記 [式 for 要素 in イテラブル [if 条件]]\n イテラブル：リストやrangeなど繰り返し処理できるもの 要素：イテラブルから順番に取り出される各要素 式：生成するリストの要素に変換される式 if 条件（任意）：その条件を満たすときだけ式を評価  # 0~9の2乗 squares = [x**2 for x in range(10)] print(squares) # 偶数だけの２乗 even_squares = [x**2 for x in range(10) if x % 2 == 0] print(even_squares) # 文字列リストの中からaで始まる要素のみからなるリスト words = [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;apricot\u0026#39;, \u0026#39;cherry\u0026#39;] a_words = [w for w in words if w.startswith(\u0026#39;a\u0026#39;)] print(a_words) # 入れ子のリストを使うこともできる matrix = [[1, 2], [3, 4], [5, 6]] flattened = [item for row in matrix for item in row] print(flattened) "},{idx:2,href:"/notebook/r/",title:"R",content:"R コーディングスタイル The tidyverse style guide\nなんで？\ngithub にあるパッケージをインストールする devtools::install_github(\u0026quot;jimhester/lookup\u0026quot;) # ブランチを指定してインストールする場合 devtools::install_github(\u0026quot;jimhester/lookup@develop\u0026quot;) ソースコードを確認 lookup パッケージ\ndevtools::install_github(\u0026quot;jimhester/lookup\u0026quot;) lookup::lookup(dplyr::summarise) バージョン管理  RSwitch R本体のバージョン管理 https://twitter.com/hrbrmstr renv, packrat : パッケージのバージョン管理  並列計算 furrr : purrr と同様の使い方で並列に計算できる\n可視化 Data to Viz\nインストール Ubuntu  UBUNTU PACKAGES FOR R Ubuntuに最新版のRをインストールする  環境設定 環境変数 Sys.getenv() Sys.setenv(\u0026quot;LANGUAGE\u0026quot;=\u0026quot;ja_Jp.UTF-8\u0026quot;) 環境設定ファイル Rの設定ファイルには以下がある。\n .Renviron .Rprofile .R/Makevars  ユーザーごとのこれらの設定ファイルは、ユーザーのRホームディレクトリ R_USER に配置しておくとよい。\n# ユーザーのRホームディレクトリの確認方法 \u0026gt; Sys.getenv(\u0026quot;R_USER\u0026quot;) [1] \u0026quot;C:/Users/hoge/Documents\u0026quot; .Renviron 環境変数を指定するシェルスクリプト\nusethis::edit_r_environ() からアクセスできる。\nR_USER=${HOME}/.R R_LIBS_USER=${R_USER}/library R_ENVIRON_USER=${R_USER}/.Renviron R_PROFILE_USER=${R_USER}/.Rprofile R_HISTFILE=${R_USER}/.Rhistory R_HISTSIZE=65535 LANG=C LC_CTYPE=en_US.UTF-8 .Rprofile Rの起動時や終了時に実行したいRのスクリプト\nhttps://cran.r-project.org/doc/manuals/R-intro.html#Customizing-the-environment\n~/.R/Makevars https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Using-Makevars\nCC=/opt/local/bin/gcc-mp-4.7 CXX=/opt/local/bin/g++-mp-4.7 CPLUS_INCLUDE_PATH=/opt/local/include:$CPLUS_INCLUDE_PATH LD_LIBRARY_PATH=/opt/local/lib:$LD_LIBRARY_PATH CXXFLAGS= -g0 -O3 -Wall MAKE=make -j4 ロケール、文字コード ロケールを指定する文字列 ロケールとは表示する言語・文字コードや数字や時刻の表示形式を変更するための設定のこと\n R manual: Locale WindowsでRのロケールを設定するときのメモ  次の３つの情報を指定する文字列\n 言語 ja 領域 jp 文字コード utf-8  ロケールの指定形式はOSにより異なる\n Windows  Japanese_Japan.932 日本語、日本国、Windows拡張Shift-JIS（コードページ932） English_United States.1252 省略した入力として Japanese English でも良い（エンコーディングはデフォルトで言語だけ変えたいとき）   Mac, Linux  ja_JP.UTF-8 en_US.UTF-8    環境変数 メッセージを変えたいだけなら .Renviron ファイルの中で LANGUAGE 環境変数を設定する\n.Renviron\nLANGUAGE=English #LANGUAGE=Japanese 表示されるメッセージの言語は、最初に環境変数 LANGUAGE の設定が読まれる。次に LC_ALL, LC_MESSAGES and LANG\nRでロケールに関連する環境変数\n LC_MESSAGES : R で表示されるメッセージの設定 LC_COLLATE : 並び換えや正規表現に用いる文字の照合順序に影響します。 LC_CTYPE : 文字の判定・操作・文字数のカウントなどに影響します。 LC_MONETARY : 金額に関連する数値、通貨記号の表示に影響します。 LC_NUMERIC : 金額に関係しない数値の表示（小数の区切り文字など）に影響します。 LC_TIME : 日付と時刻の表示に影響します。 LC_ALL : これが設定されていると他のロケール設定（LC_*）よりも優先して、こちらの設定が使われる  Sys.setlocale(\u0026quot;LC_ALL\u0026quot;, \u0026quot;C\u0026quot;) Sys.setlocale(\u0026quot;LC_MESSAGES\u0026quot;, \u0026quot;English\u0026quot;) 現在のロケールを確認する（windows）\n\u0026gt; Sys.getlocale() [1] \u0026#34;LC_COLLATE=Japanese_Japan.932;LC_CTYPE=English_United States.1252;LC_MONETARY=Japanese_Japan.932;LC_NUMERIC=C;LC_TIME=Japanese_Japan.932\u0026#34; Ubuntu でロケールを表示してみた。　ロケールに関連する環境変数はもっと多い\n$ locale LANG=ja_JP.UTF-8 LANGUAGE= LC_CTYPE=\u0026quot;ja_JP.UTF-8\u0026quot; LC_NUMERIC=\u0026quot;ja_JP.UTF-8\u0026quot; LC_TIME=\u0026quot;ja_JP.UTF-8\u0026quot; LC_COLLATE=\u0026quot;ja_JP.UTF-8\u0026quot; LC_MONETARY=\u0026quot;ja_JP.UTF-8\u0026quot; LC_MESSAGES=\u0026quot;ja_JP.UTF-8\u0026quot; LC_PAPER=\u0026quot;ja_JP.UTF-8\u0026quot; LC_NAME=\u0026quot;ja_JP.UTF-8\u0026quot; LC_ADDRESS=\u0026quot;ja_JP.UTF-8\u0026quot; LC_TELEPHONE=\u0026quot;ja_JP.UTF-8\u0026quot; LC_MEASUREMENT=\u0026quot;ja_JP.UTF-8\u0026quot; LC_IDENTIFICATION=\u0026quot;ja_JP.UTF-8\u0026quot; LC_ALL=ja_JP.UTF-8 フォント Windows 環境でフォントを使うときの注意は extrafontパッケージのREADME に詳しく書いてある。\nこちらも悪くない 【ggplot2】 好きなフォントを適用できるようにする\nフォントのインストール フォントをインストールすると以下の場所にインストールされる多分。\nWindows OS が認識するのフォントの場所\nsysfonts::font_paths() WIndowsにインストールされたフォントファイルの一覧をデータフレームで返す\nsysfonts::font_files() 次のセクションでRにフォントをインポートするのに成功したら、この一覧にある family と face が R から利用できるようになるはず。\nRへのフォントのインポート (Windows) 参考　http://www.zg.em-net.ne.jp/~mlab/_src/sc271/82p8ac28bab82c683t83h839383g.pdf\n新しいフォントをインストールした時、*R をメジャーアップデートした時、Rにフォントをインポートする必要がある。 （フォントをOSにインストールするだけではなく、インストールしたフォントをRから利用可能にする必要がある）\nextrafont::font_import() がエラーでフォントのインポートができない。Rttf2pt1 パッケージをダウングレートするとインポートできるようになる。\nremotes::install_version(\u0026#34;Rttf2pt1\u0026#34;, version = \u0026#34;1.3.8\u0026#34;) extrafont::font_import() extrafont::font_import() その後、以下を実行すると R で指定可能なフォント family の一覧が出力される。（ここの出力に表示されないフォントは利用できない。）\ngrDevices::windowsFont() Windowsの場合、さらに毎回Rを起動したときに以下を実行する。\nextrafont::loadfonts(device = \u0026#34;win\u0026#34;) # pdf や postscript で出力する場合は以下も必要らしい #extrafont::loadfonts(\u0026#34;pdf\u0026#34;, quiet = TRUE) #extrafont::loadfonts(\u0026#34;postscript\u0026#34;, quiet = TRUE) 利用可能なフォントの一覧の表示？ これらのフォント名は family で指定できる？\nextrafont::fonts() pdf デバイスで使えるフォントの一覧\nnames(pdfFonts()) # Vector of font family names fonts() # Show entire table fonttable() フォントの指定 Font family とは まずはフォント family と face を理解する。\nfamily = \u0026quot;Roboto\u0026quot; face = \u0026quot;bold\u0026quot; # bold, italic フォントにはいわゆる \u0026ldquo;ＭＳ ゴシック\u0026rdquo; や \u0026ldquo;Roboto\u0026rdquo; など字体の違いのほかに、同じ字体の中で普通（Regular）や太字 (Bold) や斜体 (Italic) などがある。\nフォントの実体はフォントファイルに記録されているが、実は普通・太字・斜体も別のフォントファイルに保存されている。なので厳密には異なるフォントと言うこともできる。\nそこで同じ字体を持つフォントをフォント family と呼ぶ、その中に普通・太字・斜体のことを face と呼ぶ。\nsysfonts パッケージを使った方法  sysfonts showtext パッケージ  # https://fonts.google.com/featured/Superfamilies # Google から name で指定したフォントをインストールして、 # それを R から family で指定した名前で利用可能にする sysfonts::font_add_google( name = \u0026#34;Roboto\u0026#34;, family = \u0026#34;Roboto\u0026#34;) #sysfonts::font_add_google(\u0026#34;Montserrat\u0026#34;, \u0026#34;Montserrat\u0026#34;) フォントの保存場所の確認＆追加\nsysfonts::font_paths() # システムフォントの場所の確認 sysfonts::font_paths(\u0026#39;path\u0026#39;) # ユーザーのフォントの保存場所を追加する # ローカルにあるフォントをRから利用可能なように登録する # フォントは上で指定したフォルダから探されるみたい sysfonts::font_add(family = \u0026#34;Roboto\u0026#34;, regular = \u0026#34;/path/to/font/file\u0026#34;) # sysfonts::font_add(\u0026#39;noto\u0026#39;, \u0026#39;NotoSansCJKjp-Regular.otf\u0026#39;) # Noto # sysfonts::font_add(\u0026#39;hana\u0026#39;, \u0026#39;HanaMinA.ttf\u0026#39;) # 花園明朝 # sysfonts::font_add(\u0026#39;spop\u0026#39;, \u0026#39;HGRPP1.ttc\u0026#39;) # 創英角ポップ # sysfonts でRにロードしたフォント family 名を確認する sysfonts::font_families() パッケージのインストール RStudio Package Manager これを使うと Linux でもコンパイル済みパッケージをインストールできる。 また、特定の日付のCRANの状態を指定してインストールすることができる。\nhttps://packagemanager.rstudio.com/client/\nGet Started\n右上、CLIENT OS を選択\nSetup\nLatest　あるいは Freeze で日付を選ぶ\nBinary\nhttps://packagemanager.rstudio.com/all/__linux__/focal/latest\nInstall System Prerequisites for the Repo’s Packages のコードを実行\n.Rprofile に以下を記述\n# ここは OS により変わる options(repos = c(REPO_NAME = \u0026#34;https://packagemanager.rstudio.com/all/__linux__/focal/latest\u0026#34;)) # Set the default HTTP user agent options(HTTPUserAgent = sprintf(\u0026#34;R/%s R (%s)\u0026#34;, getRversion(), paste(getRversion(), R.version$platform, R.version$arch, R.version$os))) OS の区別 R が実行されているOSを区別する方法\nif(Sys.info()[\u0026#39;sysname\u0026#39;] == \u0026#34;Windows\u0026#34;){ # Windows } else if (Sys.info()[\u0026#39;sysname\u0026#39;] == \u0026#34;Darwin\u0026#34;) { # MacOS } else { # Other } パッケージをアンロードする方法 一度 library() してしまったパッケージをはずす\ndetach(\u0026#34;package:fishwatchr\u0026#34;, unload=TRUE) きになるパッケージ 可視化  gganimate : gifアニメ export : ggplotオブジェクトをパワポに変換する ggridge 複数のヒストグラム比べるやつ  オブジェクトを調べる class() typeof() mode()\nstr() attributes()\n"},{idx:3,href:"/notebook/llm/",title:"LLM: Large Language Models",content:"LLM: Large Language Models 情報収集 https://x.com/SuguruKun_ai/status/1804110087793053890/photo/1\n下の図は、 Claude 3.5 Sonnet に次の入力をして画像を作成し、さらに、各LLMの特徴的なアイコンを追加、日本語に変換させてることで作成した図です。\n\u0026gt; 最新のLLMランキングのトップ5をテーマに、Twitterによくあるリッチな図解資料をsvgで作ってください "},{idx:4,href:"/notebook/gis/bigquerygis/",title:"BigQuery GIS",content:"BigQuery GIS BigQuery標準SQLの地理関数\nBigQuery Geo Viz: BigQueryの地理オブジェクトの可視化\n"},{idx:5,href:"/notebook/gis/sar/",title:"SAR",content:"SAR 合成開口レーダ（SAR）のキホン～事例、分かること、センサ、衛星、波長～\n合成開口レーザー（SAR:Synthetic Aperture Radar）は、マイクロ波を発射し、地表で跳ね返ってきたマイクロ波をとらえるセンサ\nざらざらした表面ほど多く電波が返ってきて白くみえ、水面などつるつるした表面では電波が反射してしまうため黒く見えます。\n観測周波数（バンド）、周波数の違いによって、何に反射して跳ね返ってくるかが異なる。\nよく用いられるのは Lバンド（1〜2GHz）、 Cバンド(4〜8GHz)、Xバンド(8〜12GHz)の3つで順番に波長が短くなっていきます。\n偏波、偏光と同じで、電磁波も波の傾きがある。物体によって偏波に対する反射特性が異なるので、物体の識別に使える\n衛星に搭載されるセンサーにより、発信する電波の傾き、受信する電波の傾きが異なる\n HH : 水平偏波で電波を出して水平で受けること HV : 水平偏波で電波を出して垂直で受けること VH : 垂直偏波で電波を出して水平で受けること VV : 垂直偏波で電波を出して垂直で受けること  "},{idx:6,href:"/notebook/gis/terminology/",title:"用語",content:"用語  Panchromatic PAN 白黒の画像 Multispectral カラー画像 可視光 Visible VIS 近赤外線 Near InfraRed NIR 可視光と近赤外線をまとめて VNIR バンド センサーが感受する電磁波の波長帯域、バンドによって見えるものが違う 極軌道において、北から南への方向を Descending 、南から北への方向を ascending という。北から南に赤道を通過する点を Descending node 、南から北に赤道を通過する点を Descending node という   センサー  合成開口レーダー SAR VIIRS : 光学センサー  Suomi NPP 衛星 NOAA20 衛星   MODIS : 中分解能撮像分光放射計、光学センサー　VIIRSよりも古い光学センサー  Terra 衛星 https://ja.wikipedia.org/wiki/%E3%83%86%E3%83%A9_(%E4%BA%BA%E5%B7%A5%E8%A1%9B%E6%98%9F) Aqua 衛星 https://ja.wikipedia.org/wiki/Aqua_(%E4%BA%BA%E5%B7%A5%E8%A1%9B%E6%98%9F)    "},{idx:7,href:"/notebook/gis/viirs/",title:"VIIRS",content:"VIIRS Visible Infrared Imaging Radiometer Suite\n赤外線〜可視光を含む波長の光を受信する22個のセンサーからなる「装置の名称」。様々な波長で観測することにより地表や海洋の色や夜間の光を観測する。雲、エアロゾル、氷、海の色、植生、夜間光などの分析に利用できる。\nSuomi-NPP（SNPP）, NOAA-20 (旧称 JPSS-1) の２つの衛星に搭載されている。どちらの衛星もNASAとNOAAの共同プロジェクト Joint Polar Satellite System (JPSS) により打ち上げられ運用されている。（NOAA-20 と JPSS-1は同じ衛星、JPSS-1 から NOAA-20 に名前が変わった。SNPP は JPSS-1のための実験衛星という位置付けらしいが、現在も運用中。）\nSNPPのデータは2012年から利用可能で、VIIRSを搭載した衛星は今後しばらく（2030以降まで）は運用を続けられる予定。\n衛星 Suomi-NPP  https://www.restec.or.jp/satellite/suomi-npp https://directory.eoportal.org/web/eoportal/satellite-missions/s/suomi-npp  回帰日数（repeat cycle）は 16 日と書いてあるけど、CLASSのデータの範囲を見ると、2017-06-01 と 2017-06-12 はほぼ同じ地点を通過するので、周期は11日ではないのか？？\n準回帰軌道である。\n 太陽同期軌道(Sun-synchronous orbit)でもあるので、太陽同期準回帰軌道と言える。  Sun-synchronous near-circular polar orbit\nS-NPP衛星軌道情報\nNOAA-20 https://www.restec.or.jp/satellite/jpss-series\nVIIRS データ情報 VIIRSポータルサイト\n SNPP VIIRS NASA VIIRS LAND  最初にこれを読む\nBeginner Guide to VIIRS data\nこのドキュメントが、わかりやすくVIIRSのデータの詳細を理解するのに役だちそう\n VIIRS_SDR_Users_Guide v1.3  データプロダクト SNPPのデータは大きく３つに分けられる、一般ユーザーはSDRとEDRを使う\n RDR (Raw Data Records) : センサーから取得された生データ（一般ユーザーには必要ない） SDR (Sensor Data Records) : センサー値を適切に処理して得られた物理量や品質フラグなどのデータ（一般ユーザーにとっての生データ） EDR (Environmental Data Records) : センサーから得られた値をさらに加工して得られたデータ（雲マスク、植生など）  JPSSのサイトに、利用できるデータプロダクトの一覧がある https://www.star.nesdis.noaa.gov/jpss/JPSS_products.php\nNASAはNOAAとは独立に異なる処理をされたVIIRSのデータプロダクトを提供しているみたい\n VNP46A2  雲や月光の影響を補正した夜間光プロダクト？？ NASA VIIRS データのトレーニングコース    \\\nデータのドキュメント JPSSの公式サイトにあるSNPPのドキュメント集\nJPSSのドキュメントがDataRefugeにある。 DataRefugeは環境や気候関係のデータのアーカイブを進めている？トランプ大統領が気候変動に関するデータの公開を制限しようとした時に対策として生まれた運動？？\nJPSSのデータフォーマットのドキュメントは Common Data Format Control Book - External (CDFCB-X) と呼ばれているらしい。VIIRSドキュメントのコードは 474 、VIIRSのデータフォーマットのドキュメントコードは 474-00001\n CDFCB-X Volume I: Overview  474-00001-01_JPSS-CDFCB-X-Vol-I_0124D 474-00001-01_jpss-cdfcb-x-vol-i_0200c.pdf こっちのが新しいらしい   CDFCB-X Volume II: Raw Data Record Formats CDFCB-X Volume III: Sensor Data Record/Temperature Data Record Formats CDFCB-X Volume IV: Environmental Data Record/Intermediate Product/Application Related Product Formats  CDFCB-X Volume IV Part 1: Overview, IPs, ARPs, and Common Geolocation Data CDFCB-X Volume IV Part 2: Imagery, Atmospheric, and Cloud EDRs CDFCB-X Volume IV Part 3: Land and Ocean/Water EDRs CDFCB-X Volume IV Part 4: Earth Radiation Budget and Space EDRs   CDFCB-X Volume V: Metadata CDFCB-X Volume VI: Ancillary Data, Auxiliary Data, Messages, and Reports CDFCB-X Volume VII: 廃止 CDFCB-X Volume VIII: Look Up Tables and Processing Coefficients  HDF5に格納されているデータの定義は以下のドキュメントにまとまっている\nJoint Polar Satellite System (JPSS) Algorithm Specification Volume II: Data Dictionary for VIIRS RDR/SDR\nHDF5ファイルに格納されているデータの定義については以下のドキュメントに概要がある\n Joint Polar Satellite System (JPSS) Algorithm Specification Volume II: Data Dictionary for VIIRS RDR/SDR. VIIRS SDR Data format  GeoTIFF への変換スクリプト\nhttps://git.earthdata.nasa.gov/projects/LPDUR/repos/nasa-viirs/browse/scripts/VIIRS_HDF5toGeoTIFF.R\nSDR I-band (SVI01 - SVI05) SVI01, SVI02, SVI03, SVI04, SVI05\n位置情報\n– GIMGO: projected onto smooth ellipsoid (WGS84 ellipsoid) – GITCO: parallax-corrected for terrain\nM-band (SVM01 - SVM16) SVM01, SVM02, SVM03, SVM04, SVM05, SVM06, SVM07, SVM08, SVM09, SVM10, SVM11, SVM12, SVM13, SVM14, SVM15, SVM16\n位置情報\n– GMODO: projected onto smooth ellipsoid – GMTCO: parallax-corrected for terrain\nDay Night Band (SVDNB) 全てのM-band (M1~M16)の波長のセンサーの信号を統合して（恐らく）、最も感度と強さのレンジを高めたプロダクト\nファイル記号 : SVDNB\n位置情報\n– GDNBO: projected onto smooth ellipsoid (as of May 2013 there is a discussion of whether or not to produce a terrain-corrected geolocation)\nEDR EDR User Guide\n2020-06-24 にこれまで作成されていた多くのEDRが作成されなくなり、あたらしい形式のものが提供されるようになった。\n以前のEDRプロダクトと新しいプロダクトの対応表はこちら。\nこれら新しい VIIRS の EDR は JPSS_RR product と呼ばれている？？\n現在、多くの EDR は CLASS の JPSS_GRAN から取得できる。\nドキュメント https://www.nodc.noaa.gov/archivesearch/rest/find/document?searchText=%22gov.noaa.class:JPSS_GRAN%22\u0026amp;max=30\u0026amp;f=searchpage\nI-band EDR I-band のSDR (SVI01, SVI02, SVI03, SVI04, SVI05) はそれぞれ対応するEDR (VI1BO, VI2BO, VI3BO, VI4BO, VI5BO) がある\nVI1BO, VI2BO, VI3BO, VI4BO, VI5BO\nM-band EDR M-band は６つのバンドについて対応するEDRがある\n SVM01 (SDR) \u0026ndash;\u0026gt; VM01O (EDR) reflectance SVM04 (SDR) \u0026ndash;\u0026gt; VM02O (EDR) reflectance SVM09 (SDR) \u0026ndash;\u0026gt; VM03O (EDR) reflectance SVM14 (SDR) \u0026ndash;\u0026gt; VM04O (EDR) brightness temperature SVM15 (SDR) \u0026ndash;\u0026gt; VM05O (EDR) brightness temperature SVM16 (SDR) \u0026ndash;\u0026gt; VM06O (EDR) brightness temperature  M-band EDR のための入力SDRは変更される可能性がある。EDRのメタデータ band_id を見ると、どのSDRが使われたのかわかる\nI-band M-band EDRs は SDR を the Ground-Track Mercator projection に再投影し直したもの\nDNB EDR DNBのEDRは Near-Constant Contrast (NCC) product と呼ばれる\nファイル記号 : VNCCO\n局所的な明るさを標準化して、夜と昼にかかわらず、光の反射率（reflectance）の分布を可視化する （理想的には、夜と昼にかかわらず同じような画像を出力するはずだが、技術的には難しい）\nDNB の EDRは、SDRの輝度を反射率に変換して、ground-track Mercator projection に再投影したもの、\nつまり、DNB の EDRは反射率 (reflectance) しか持っていない、DNB の SDR は輝度しか持っていない\n輝度（radiance）\n位置情報\n– GIGTO: I-band EDR geolocation – GMGTO: M-band EDR geolocation – GNCCO: Day/Night Band EDR (NCC) geolocation\nThe cloud mask is 1km resolution and contains probability of cloud for each pixel (I used discretized cloud mack, though.) I wish I had VIIRS data in relational database. To be honest, I\u0026rsquo;m tired of dealing with the raw data.\nCloud Mask  JRR-CloudMask   アルゴリズム改良版: Enterprize Cloud Mask, ECM\n  期間 (xxxx-xx-xx ~ 現在)\n  取得 https://www.avl.class.noaa.gov/saa/products/search?sub_id=0\u0026amp;datatype_family=JPSS_GRAN\u0026amp;submit.x=34\u0026amp;submit.y=0\n  データ形式: NetCDF (Version: 4)\n  リンク\n NOAA JPSS の CLOUD チーム（アルゴリズムの開発元） VIIRS EDRについてのポータル CLOUD MASKの可視化    ドキュメント\n  External User Manual (EUM): NetCDFファイル内部に格納されているデータについて記載\n https://www.star.nesdis.noaa.gov/jpss/documents/UserGuides/JPSS_RR_EUM.pdf https://www.ospo.noaa.gov/Products/Suites/files/JPSS_RR_EUM_June2016.pdf    Algorithm Theoretical Basis Document (ATBT) データの意味についてはこちら\n すべてのVIIRS EDRのATBTへのリンク Enterprize Cloud Mask の ATBT    プレゼン資料\n [How to Use the NOAA Enterprise Cloud Mask (ECM)]https://www.star.nesdis.noaa.gov/jpss/documents/meetings/2015/SJASTM/Session7e-Clouds+Aerosol.pdf) Enterprise Cloud Mask (ECM): Data format basics -　メモ    雲マスクのアルゴリズムは ECM (Enterprize Cloud Mask) と呼ばれている。VIIRSの他のセンサーに対しても適用できるように作られている（VIIRS, MODIS, AVHRR, GOES, ABI, SEVIRI, AHI, COMS, など）。使用するバンドはセンサーにより異なる。\n    参考リンク\n [ABI CLOUD MASK (ACM)、ECMとは異なるクラウドマスク（ECMのもとになっている？？）] (https://www.star.nesdis.noaa.gov/goesr/docs/ATBD/Cloud_Mask.pdf) こちらには CloudMaskQualFlag の説明が載っているが、ECMと同じなのかは不明     VICMO  EDRに格上げされた。VIIRS CLOUD MASK (VCM) (2017-03-08 ~ 2020-06-24) https://www.bou.class.noaa.gov/saa/products/search?sub_id=0\u0026amp;datatype_family=VIIRS_EDR\u0026amp;submit.x=24\u0026amp;submit.y=3 データ形式: HDF5 JPSS Algorithm Specification, Volume II: Data Dictionary for the Cloud Mask 474-00448-02-11_JPSS-DD-Vol-II-Part-11_0200E   IICMO  中間プロダクト Cloud Mask Intermediate Product (2012-05-02 ~ 2017-03-08) https://www.bou.class.noaa.gov/saa/products/search?sub_id=0\u0026amp;datatype_family=VIIRS_IPNG\u0026amp;submit.x=29\u0026amp;submit.y=8 データ形式: HDF5 IICMOのドキュメント    NetCDF4ファイルをRで読み込んでメタデータを表示する\npath_ncdf \u0026lt;- \u0026#34;JRR-CloudMask_v1r1_npp_s201804161754397_e201804161756039_c201804162011060.nc\u0026#34; nc \u0026lt;- ncdf4::nc_open(path_ncdf) print(nc) NetCDF4ファイルに格納された値の取得方法\n# 変数の値を取得 CloudMask \u0026lt;- ncdf4::ncvar_get(nc, \u0026#34;CloudMask\u0026#34;) # グローバル属性の取得 start_orbit_number \u0026lt;- ncdf4::ncatt_get(nc, 0, attname = \u0026#34;start_orbit_number\u0026#34;)$value メタデータの表示例\n主要な変数\n Latitude 緯度 Longitude 経度 CloudMask クラウドマスクの値 0,1,2,3, _FillValue: -128 CloudMaskPacked ビットごとにATBTの  chunkingは各要素のデータサイズ？単位はbite?\nFile JRR-CloudMask_v1r1_npp_s201804161754397_e201804161756039_c201804162011060.nc (NC_FORMAT_NETCDF4): 30 variables (excluding dimension variables): float Latitude[Columns,Rows] (Chunking: [200,256]) (Compression: shuffle,level 2) long_name: Latitude units: degrees_north comments: Pixel latitude in field Latitude (degree) _FillValue: -999 valid_range: -90 valid_range: 90 float Longitude[Columns,Rows] (Chunking: [200,256]) (Compression: shuffle,level 2) long_name: Longitude units: degrees_east comments: Pixel longitude in field Longitude (degree) _FillValue: -999 valid_range: -180 valid_range: 180 int StartRow[] (Contiguous storage) long_name: Start row index int StartColumn[] (Contiguous storage) long_name: Start column index byte CloudMask[Columns,Rows] (Chunking: [200,256]) (Compression: shuffle,level 2) long_name: Cloud Mask coordinates: Longitude Latitude units: 1 _FillValue: -128 valid_range: 0 valid_range: 3 byte CloudMaskBinary[Columns,Rows] (Chunking: [200,256]) (Compression: shuffle,level 2) long_name: Cloud Mask Binary coordinates: Longitude Latitude units: 1 _FillValue: -128 valid_range: 0 valid_range: 1 byte CloudMaskPacked[CldMaskPkedCnst,Columns,Rows] (Chunking: [7,200,256]) (Compression: shuffle,level 2) long_name: Diagnostic Cloud Mask coordinates: Longitude Latitude units: 1 _FillValue: -128 valid_range: -128 valid_range: 127 byte CloudMaskFlag[FlagConst,Columns,Rows] (Chunking: [33,200,256]) (Compression: shuffle,level 2) long_name: Cloud Mask Test coordinates: Longitude Latitude units: 1 _FillValue: -128 valid_range: -128 valid_range: 127 byte Smoke_Mask[Columns,Rows] (Chunking: [200,256]) (Compression: shuffle,level 2) long_name: Smoke Mask coordinates: Longitude Latitude units: 1 _FillValue: -128 valid_range: 0 valid_range: 3 byte Fire_Mask[Columns,Rows] (Chunking: [200,256]) (Compression: shuffle,level 2) long_name: Fire Mask coordinates: Longitude Latitude units: 1 _FillValue: -128 valid_range: 0 valid_range: 3 byte Dust_Mask[Columns,Rows] (Chunking: [200,256]) (Compression: shuffle,level 2) long_name: Dust Mask coordinates: Longitude Latitude units: 1 _FillValue: -128 valid_range: 0 valid_range: 3 byte CloudMaskQualFlag[Columns,Rows] (Chunking: [200,256]) (Compression: shuffle,level 2) long_name: Cloud Mask Quality Flag coordinates: Longitude Latitude units: 1 _FillValue: -128 valid_range: 0 valid_range: 6 float CloudProbability[Columns,Rows] (Chunking: [200,256]) (Compression: shuffle,level 2) long_name: Cloud Probability coordinates: Longitude Latitude units: 1 _FillValue: -999 valid_range: 0 valid_range: 1 float ClearProbClear[] (Contiguous storage) long_name: Percent of Clear and Probably Clear units: % valid_range: 0 valid_range: 100 int NumOfQualityFlag[] (Contiguous storage) long_name: Number of quality flag units: 1 float Cloudy[] (Contiguous storage) long_name: Percent of Pixels that passed a test for cloud and failed a test for cloud edge units: % valid_range: 0 valid_range: 100 float ProbCloudy[] (Contiguous storage) long_name: Percent of Pixels that passed a test for cloud and passed a test for cloud edge units: % valid_range: 0 valid_range: 100 float ProbClear[] (Contiguous storage) long_name: Percent of Pixels that passed no test for cloud but passed tests for spatial heterogenity units: % valid_range: 0 valid_range: 100 float Clear[] (Contiguous storage) long_name: Percent of Pixels that passed no test for cloud and failed a test for spatial heterogenity units: % valid_range: 0 valid_range: 100 int TotalPixel[] (Contiguous storage) long_name: Total Number of pixels units: 1 float TerminatorPixPercent[] (Contiguous storage) long_name: Percent of terminator pixels units: % valid_range: 0 valid_range: 100 int TotalCloudMaskPixel[] (Contiguous storage) long_name: Total Number of cloud Mask pixels units: 1 float MinClrSkyOBS_RTM[Meta10] (Contiguous storage) long_name: Minimum observation - RTM for Clear Sky IR Channel 07 to 16 coordinates: units: Kelvin _FillValue: 0 float MaxClrSkyOBS_RTM[Meta10] (Contiguous storage) long_name: Maximum observation - RTM for Clear Sky IR Channel 07 to 16 coordinates: units: Kelvin _FillValue: 0 float MeanClrSkyOBS_RTM[Meta10] (Contiguous storage) long_name: Mean observation - RTM for Clear Sky IR Channel 07 to 16 coordinates: units: Kelvin _FillValue: 0 float StdDevClrSkyOBS_RTM[Meta10] (Contiguous storage) long_name: Std Dev observation - RTM for Clear Sky IR Channel 07 to 16 coordinates: units: Kelvin _FillValue: 0 float MinAllSkyOBS_RTM[Meta10] (Contiguous storage) long_name: Minimum observation - RTM for All Sky IR Channel 07 to 16 coordinates: units: Kelvin _FillValue: 0 float MaxAllSkyOBS_RTM[Meta10] (Contiguous storage) long_name: Maximum observation - RTM for All Sky IR Channel 07 to 16 coordinates: units: Kelvin _FillValue: 0 float MeanAllSkyOBS_RTM[Meta10] (Contiguous storage) long_name: Mean observation RTM for All Sky IR Channel 07 to 16 coordinates: units: Kelvin _FillValue: 0 float StdDevAllSkyOBS_RTM[Meta10] (Contiguous storage) long_name: Std Dev observation - RTM for All Sky IR Channel 07 to 16 coordinates: units: Kelvin _FillValue: 0 5 dimensions: Columns Size:3200 [1] \u0026quot;vobjtovarid4: **** WARNING **** I was asked to get a varid for dimension named Columns BUT this dimension HAS NO DIMVAR! Code will probably fail at this point\u0026quot; Rows Size:768 [1] \u0026quot;vobjtovarid4: **** WARNING **** I was asked to get a varid for dimension named Rows BUT this dimension HAS NO DIMVAR! Code will probably fail at this point\u0026quot; CldMaskPkedCnst Size:7 [1] \u0026quot;vobjtovarid4: **** WARNING **** I was asked to get a varid for dimension named CldMaskPkedCnst BUT this dimension HAS NO DIMVAR! Code will probably fail at this point\u0026quot; FlagConst Size:33 [1] \u0026quot;vobjtovarid4: **** WARNING **** I was asked to get a varid for dimension named FlagConst BUT this dimension HAS NO DIMVAR! Code will probably fail at this point\u0026quot; Meta10 Size:10 [1] \u0026quot;vobjtovarid4: **** WARNING **** I was asked to get a varid for dimension named Meta10 BUT this dimension HAS NO DIMVAR! Code will probably fail at this point\u0026quot; 34 global attributes: Conventions: CF-1.5 Metadata_Conventions: CF-1.5, Unidata Dataset Discovery v1.0 standard_name_vocabulary: CF Standard Name Table (version 17, 24 March 2011) project: S-NPP Data Exploitation institution: DOC/NOAA/NESDIS/NDE-\u0026gt;S-NPP Data Exploitation, NESDIS, NOAA, U.S. Department of Commerce naming_authority: gov.noaa.nesdis.nde satellite_name: NPP instrument_name: VIIRS title: JPSS Risk Reduction Unique Cloud Mask summary: Cloud Mask history: VIIRS Cloud Mask Version 1.0 processing_level: NOAA Level 2 references: id: c1b243ad-cae9-46a1-a416-f972c10ea5e2 Metadata_Link: JRR-CloudMask_v1r1_npp_s201804161754397_e201804161756039_c201804162011060.nc start_orbit_number: 33516 end_orbit_number: 33516 day_night_data_flag: night ascend_descend_data_flag: 1 time_coverage_start: 2018-04-16T17:54:39Z time_coverage_end: 2018-04-16T17:56:03Z date_created: 2018-04-16T20:11:06Z cdm_data_type: swath geospatial_first_scanline_first_fov_lat: 50.7433967590332 geospatial_first_scanline_last_fov_lat: 44.5372085571289 geospatial_last_scanline_first_fov_lat: 45.7234344482422 geospatial_last_scanline_last_fov_lat: 40.0141525268555 geospatial_first_scanline_first_fov_lon: 101.341705322266 geospatial_first_scanline_last_fov_lon: 141.848983764648 geospatial_last_scanline_first_fov_lon: 101.343467712402 geospatial_last_scanline_last_fov_lon: 138.53092956543 geospatial_lat_units: degrees_north geospatial_lon_units: degrees_east geospatial_bounds: POLYGON((101.342 50.7434,141.849 44.5372,138.531 40.0142,101.343 45.7234,101.342 50.7434)) 生データ取得 ウィスコンシン 直近１週間分のデータ\nftp://ftp.ssec.wisc.edu/pub/eosdb/npp/viirs/\nNOAA CLASS ウェブサイトからSDR, EDRの生データを取得できるが、APIで自動取得することができない\u0026hellip;\n 直近90日分のデータは　NOAAのFTPサイトから入手できる VIIRS SDR VIIRS EDR 古いバージョン VIIRS EDR 新しいバージョン  NASA LAADS NOAA CLASS 同様のSDR/EDRが入手できるが、どうも何らかの処理が加えられているように見える。ただスクレイピングによりデータ取得を自動化できる。\n NASA LAADS VIIRS  VIIRS:Suomi-NPP VIIRS CLOUD MASK    データプロダクト・サービス  Cololad School of Mines: Earth Observation Group : VIIRS  VIIRSを利用した、夜間の焱、夜間光（月次、年次）、夜間の漁船光の検出のデータを提供している 元々はNOAAのグループだったが Cololad School of Mines に移籍した。NOAA時代のサイト https://eogdata.mines.edu/wwwdata/viirs_products/vbd/v23/global-saa/current/ https://eogdata.mines.edu/wwwdata/viirs_products/vbd/v23/global-saa/nrt/    https://payneinstitute.mines.edu/eog/viirs/\nVIIRS を利用した論文 夜間の漁船光の検出  Automatic Boat Identification System for VIIRS Low Light Imaging Data Cross-Matching VIIRS Boat Detections with Vessel Monitoring System Tracks in Indonesia  一般記事 衛星が撮影した夜の地球「夜間光」がお金に変わる!? 概要と利用事例\n"},{idx:8,href:"/notebook/datascience/",title:"Data Science",content:"Data Science データ作成 drawdata.xyz\n手でグラフをお絵描きして対応するデータをダウンロードできる\n精度指標  情報量 divergence  分類モデル 混合行列\nhttps://en.wikipedia.org/wiki/Confusion_matrix\nページの右に色々カテゴリ変数のための精度指標のリストがある\nhttps://en.wikipedia.org/wiki/Receiver_operating_characteristic\n https://qiita.com/K5K/items/5da52e99861483cae876 https://qiita.com/FukuharaYohei/items/be89a99c53586fa4e2e4  "},{idx:9,href:"/notebook/gis/",title:"GIS \u0026 Remote Sensing",content:"GIS \u0026amp; Remote Sensing 情報収集 ポータルサイト  JAXA 衛星利用推進サイト 宙畑 リモートセンシング技術センター  団体コミュニティ  Open Geospatial Consortium 地理情報関係の標準化団体 OSGeo.jp OSGeo4W WindowsのためにOSSのGIS関連ソフトをビルドしているらしい  学習サイト GIS  GIS基礎知識 Geocomputation with R Rでの地理情報解析の方法がめっちゃ充実、sfパッケージベース Spatial Data Science Spatio-Temporal Statistics with R Introduction to visualising spatial data in R 空間情報クラブ  CRS  PROJで利用できるCRSのリスト 地図投影法のリスト RSpatialGuides/OverviewCoordinateReferenceSystems どんな目的、スケールでどの投影法を使うかガイド 地域ごとの適切な投影法の検索エンジン？  リモセン  衛星観測の専門用語  データソース  STEP  ESAのポータルサイトらしい   MarineRegions.org 海洋関係の Shapefile を公開している GeoServer GeoNetwork Natural Earth 国土数値情報 OCEAN DATA VIEWER 気象データ高度利用ポータルサイト GISデータ入手先一覧  衛星データ  NOAA CLASS -NOAA CLASS Data access tutrial LAADS DAAC G-Portal  宇宙開発組織  NASA NOAA JAXA ESA  ファイル形式  GeoJSON WKT, WKB Shapefile PostGIS GeoTiff HDF5  衛星 衛星総覧\n 気象変動観測衛星しきさい:GCOM-C Suomi-NPP : VIIRS JPSS1(NOAA20) : VIIRS Sentinel-1 SAR Sentinel 2 光学 ALOS-2 だいち2 : PALSAR-2 (Phased Array L-band Synthetic Aperture Radar-2) / Lバンド合成開口レーダ  センサー  VIIRS SAR  衛星情報解析サービス  Google Earth Engine Tellus  企業  Orbital Insight  地図サービス JapanMapCompare\n日本の地図サービスの画像を比較できる\n機械学習  torchgeo: 地理情報処理のためのPytorchライブラリ  "},{idx:10,href:"/notebook/gis/r_package/",title:"GIS関連Rパッケージ",content:"GIS関係のRパッケージの一覧  https://cran.r-project.org/web/views/Spatial.html https://cran.r-project.org/web/views/SpatioTemporal.html https://www.r-spatial.org/  一般   sp：地理空間のクラスやメソッドを提供している\n  sf : OGC Simple Featuresという標準に従って開発されている。sfパッケージの公式サイト、OGCが定義するファイル形式の読み書き（WKT, WKB）\n  starts : 多次元arrayで時空間データを表現\n  raster : ラスターデータの可視化と分析\n  spatial.tools : rasterを拡張して並列計算など\n  rgdal : GDALライブラリがサポートするラスターデータ, OGRライブラリがサポートするベクターデータへのバインディング、GeoJSON や Shapefile の読み書き、 PROJ.4がサポートする投影法\n  rgeos : Geometry Engine - Open Source (\u0026lsquo;GEOS\u0026rsquo;)ライブラリへのバインディング\n  GISTools : 空間データをマッピングしたり加工したり\n  metR : 気象関係のデータの扱いを容易にするパッケージらしい\n  データの読み書き  maps : mapdata, mapproj パッケージと共に地理空間データベースを提供 maptools : ESRI ArcGIS/ArcView shapefiles の読み書きなど、地理情報オブジェクトの扱い、GSHHGの海岸線データベースへのアクセス？？ shapefiles : ESRI ArcGIS/ArcView shapefiles の読み書き rpostgis : PostGISが有効化されたPostgreSQLへのコネクション rgrass7 : GRASS v7 とのコネクション RQGIS : RからQGISの機能を利用する  データの取得  geosapi : GeoServerのAPIからデータを取得する？ geonapi : GeoNetworkのAPIからデータを取得する？ rgbif : Global Biodiversity Information Facility (GBIF) にあるデータへのアクセス rnaturalearth : よく使う地図データをRからすぐにアクセスできるようにする sp, sf オブジェクトで提供  計算  lwgeom : PostGISで使われている liblwgeom へのバインディング gdalUtils : GDAL Utilityへのラッパー？ gdistance : grid間の距離やルートを計算する？ geosphere : 距離、面積、角度などの計算？ cshapes : 距離行列の計算 spsurvey : 地理的調査のためのサンプリング手法など？ trip : 動物のトラッキングデータ分析 magclass : 時空間解析 taRifx : ユーティリティ関数 geoaxe : オブジェクトを小さく分割する lawn : Turfjs ブラウザの地理空間解析ライブラリのクライアント rcosmo : 球体に対する計算を提供 areal : Areal Weighted Interpolation の実装  データのエラー調査  landsat : 衛星画像の補正？ cleangeo : 空間オブジェクトのエラーの調査？  カラーパレット  RColorBrewer : Mapでいい感じのカラーパレットを提供 viridis : 視覚障害者にも優しいカラーパレットを提供 classInt : １次元の変数の値を階級に分けるのに使う？？  可視化  ggspatial : ggplot2で可視化する時の便利ツール rasterVis : rasterの可視化 quickmapr : sp, sfオブジェクトをとりあえず可視化できる cartography : いろんな地図作成？ mapmisc : 軽量な地図作成？  可視化ウェブ  mapView, leaflet, leafletR インタラクティブな地図の可視化 RgoogleMaps : Googleマップに問い合わせしたり、Googleマップの画像を背景にしたプロットを描く plotGoogleMaps : Googleマップに描画する plotKML : オブジェクトをKMLで書き出してGoogle Earthで読み込めるようにする？？ ggmap : Google MapやOpen Street Map に描画する？ mapedit : Shinyでleafletで書いた地図を編集できるようにする？？  Cartograms 地図を変形させる可視化？ cartogram : 面積を風船のように伸び縮みさせる\nPoint Pattern Analysis  spatial, spatstat spatgraphs : 点のパターンのグラフ解析  空間統計  gstat, geoR, geoRglm : 統計量、統計モデル？ vardiag : Variogram を書く  Disease mapping and areal data analysis https://cran.r-project.org/web/views/Spatial.html\nSpatial regression https://cran.r-project.org/web/views/Spatial.html\nEcological analysis https://cran.r-project.org/web/views/Spatial.html\n"},{idx:11,href:"/notebook/mac/",title:"Mac",content:"Mac 便利なツール QuickLookプラグイン 以下のGithubレポジトリに開発者にとって便利な様々なQuickLookプラグインが紹介されている。どれも homebrew でインストールできるので簡単\nsindresorhus/quick-look-plugins\n"},{idx:12,href:"/notebook/maritime/",title:"Maritime",content:"海事関係 リンク  Asia Maritime Transparency Initiative: 各国  I think it\u0026rsquo;s good practice to keep code, data and the resulting documents in the same place,　and keep always working in that folder.\n"},{idx:13,href:"/notebook/datascience/bayesian_statistics/",title:"Bayesian Staistics",content:"Bayesian Statistics   Cmdstanr入門とreduce_sum()解説\n  rstan はもう更新されていないみたいなので、今後は Cmdstan のラッパーである Cmdstanr を使った方がよさそう\n  Bayesian statistics\n  rstanの環境構築 \n  初心者セッション3 Introduction to rstan\n  めざせ脱rstan初心者\n  実験ゲームの統計モデリング\n  stan推定後の可視化について\n  Introduction to bayesplot (ppc_ series)\n  Introduction to bayesplot (mcmc_ series)\n  Stanの便利な事後処理関数\n  ggplot2 for RStan\n  bayesplot\n  Stan Advent Calendar 2018\n  RstanにDockerはいいゾ\n"},{idx:14,href:"/notebook/gcp/bigquery_gis/",title:"BigQuery GIS",content:"BigQuery GIS 標準 SQL の地理関数\n地理オブジェクト作成 ST_GEOGPOINT(longitude, latitude) ST_GEOGFROMTEXT(wkt_string) 判定関数 ST_CONTAINS ST_CONTAINS(geography_1, geography_2) geography_2 は geography_1 の内部にある（外にはみ出していない）なら TRUE\nST_WITHIN ST_WITHIN(geography_1, geography_2) geography_1 は geography_2 の内部にある（外にはみ出していない）なら TRUE\nST_CONTAINS とは geography_1 と geography_2 の順序が逆になっている\n最近傍 SELECT a.house_id, # Order the neighboring restaurants accoring to distance and extract a restaurant having shortest distance ARRAY_AGG(b.restaurant_id ORDER BY ST_Distance(a.point, b.point) LIMIT 1)[ORDINAL(1)] as neighbor_restaurant_id_id FROM houses a JOIN restaurants b ON # Extract only restaurant around each house ST_DWithin(a.point, b.point, 10000) -- search radius in meter GROUP BY a.house_id invalidな図形を修正する SELECT ST_GEOGFROMTEXT (string_field_1, make_valid =\u0026gt; TRUE) FROM `ocean_shapefiles_all_purpose.IOTC_shape_feb2021` "},{idx:15,href:"/notebook/gcp/bigquery_bq/",title:"BigQuery: bqコマンド",content:"BigQuery：bqコマンド クエリを実行する : bq query  bq query 'select * from hoge'  -n 0 : クエリの実行結果が標準されるのを抑制する --destination_table=project:dataset.table : クエリの出力をテーブルに書き込む --replace : 出力先のテーブルを置き換える --allow_large_results --use_legacy_sql=false  クエリの一部を bq のパラメタとして与える bq query --use_legacy_sql=false --parameter=percent:INT64:29 \\ 'SELECT * FROM `dataset.my_table` TABLESAMPLE SYSTEM (@percent PERCENT)` Data set を作成する bq mk \\ --dataset \\ --location=US \\ --default_table_expiration 3600 \\ --default_partition_expiration 3600 \\ --description 'description' \\ project_id:dataset_name  location US など default_table_expiration テーブル自動削除までの秒数を指定する、デフォルトでは 3600 、0に設定すると自動削除しない default_partition_expiration テーブルに対するパーティションの自動削除までの秒数 description データセットの説明、' か \u0026quot; で括る project_id:dataset_name プロジェクトIDと、作成するデータセットの名前  パーティションドテーブルの作成 https://cloud.google.com/bigquery/docs/creating-partitioned-tables?hl=ja\nbqコマンドを使う場合\nbq mk --table \\  --schema date:DATE,orbit_number:INT64,lon:FLOAT64,lat:FLOAT64 \\  --time_partitioning_field date \\  --time_partitioning_type DAY \\  --require_partition_filter=FALSE \\ gfw-fra:fra_vbd.fra_vbd_with_overlap --time_partitioning_field を指定して特定のカラムでパーティションをする場合には、テーブル作成時にスキーマを指定する必要がある。\nデータをアップロードする : bq load ローカルファイル（.parquet）から（1つのファイルしか指定できない）\nbq load \\ --replace=TRUE\\ --source_format=PARQUET \\ --autodetect \\ PROJECT:DATASET.TABLE \\ LOCAL_FILE.parquet GCSから取り込み（複数のファイルをまとめて指定できる）\nbq load \\ --source_format=PARQUET \\ PROJECT:DATASET.TABLE \\ \u0026quot;gs://mybucket/00/*.parquet\u0026quot;,\u0026quot;gs://mybucket/01/*.parquet\u0026quot; テーブルを削除する : bq rm bq rm --table project_id:dataset.table  --table または -t でテーブルを削除 --force または -f は確認を省略する  複数のテーブルをまとめて削除する 特定のデータセット内の、名前が特定のパターン pattern に合致するテーブルを一括で削除する\nbq ls -a project_id:dataset_id | grep pattern | xargs -n 1 bq rm -t -f --project_id project_id --dataset_id dataset_id テーブルの一覧 : bq ls bq ls -a project_id:dataset_id パーティションテーブルへの書き込み パーティションを指定してクエリ結果を書き込む\n# 日付でパーティションされたテーブルの2020-01-01のパーティションを置き換える `--replace` bq query --replace --destination_table=dataset.table$20200101 \u0026#39;SELECT * FROM hoge\u0026#39; 特定パーティションにデータをロードすることもできる\nbq load \\ --replace=TRUE\\ --source_format=PARQUET \\ --autodetect \\ PROJECT:DATASET.TABLE$20200101 \\ LOCAL_FILE.parquet スキーマの指定方法 https://cloud.google.com/bigquery/docs/schemas?hl=ja\n INT64 FLOAT64 BOOL DATE DATETIME: タイムゾーンに依存せずに時計に表示される日時を表します。これには年、月、日、時、分、秒、サブ秒が含まれます。絶対的な時刻を表すには、タイムスタンプを使用します TIMESTAMP 値は、タイムゾーンや夏時間などの慣習に関係なく、マイクロ秒精度の絶対的な時刻を表します。 STRING GEOGRAPHY  コマンドラインで入力する場合\n列名:データ型,qtr:STRING,sales:FLOAT,year:STRING JSONファイルで記述する場合\n[ { \u0026quot;description\u0026quot;: \u0026quot;quarter\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;REQUIRED\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;qtr\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;STRING\u0026quot; }, { \u0026quot;description\u0026quot;: \u0026quot;sales representative\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;NULLABLE\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;rep\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;STRING\u0026quot; }, { \u0026quot;description\u0026quot;: \u0026quot;total sales\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;NULLABLE\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;sales\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;FLOAT\u0026quot; } ] "},{idx:16,href:"/notebook/gcp/bigquery/",title:"BigQuery: general",content:"BigQuery BigQuery Advent カレンダー\nWebUI キーボードショートカット Mac の場合は Ctrl の代わりに Cmd にする\n   キー アクション     Ctrl Enter 現在のクエリを実行   Tab 現在の単語をオートコンプリート   選択 Tab 選択範囲のインデントを上げる   選択 Shift Tab 選択範囲のインデントを下げる   Ctrl テーブル名をハイライト表示   Ctrl テーブル名をクリック テーブル スキーマを開く   Ctrl E 選択項目からクエリを実行   Ctrl / 選択行をコメントアウト   Ctrl Shift F クエリを書式設定    Web IU のキーボードショートカット\nPythonやRからBigQueryにクエリを投げる "},{idx:17,href:"/notebook/gcp/bigquery_sql/",title:"BigQuery: SQL",content:"BigQuery: SQL 基本文法  標準 SQL のクエリ構文 標準SQLの演算子 標準SQLの関数と演算子  データセットの中からテーブルの情報を取得する\nselect * dataset.TABLES_SUMMARY\nINFORMAITON＿SCHEMA.COLUMS\nレギュラーSQLをデフォルトにする クエリの前に #standardSQL の記述を追加する。逆にレガシーにしたい場合は #legacySQL を記述する。\n#standardSQL SELECT weight_pounds, state, year, gestation_weeks FROM `bigquery-public-data.samples.natality` ORDER BY weight_pounds DESC LIMIT 10; .bigqueryrc に以下を記述\n[query] --use_legacy_sql=false [mk] --use_legacy_sql=false 実数を丸める関数  ROUND() CEILING() FLOOR() TRUNC()  丸め関数の挙動\n行の抽出 WHERE column IN( sub_query ) 列の値が、サブクエリの出力結果と同じ値をもつ行だけを抽出する\nSELECT * FROM tableA WHERE item IN ( SELECT product FROM tableB ) WHERE EXISTS( sub_query ) サブクエリのなかのwhere句で抽出したいレコードの条件を指定する。\nSELECT * FROM tableA WHERE EXISTS ( SELECT product FROM tableB WHERE tableA.item = tableB.product AND tableA.price = tableB.price ) カラム名を取得する SELECT column_name FROM dataset_name.INFORMATION_SCHEMA.COLUMNS WHERE table_name = 'table_name' カラム名を取得して、INSERT SELECT 用の文字列を生成する\nWITH ColumnNames AS ( SELECT column_name FROM scratch_masaki.INFORMATION_SCHEMA.COLUMNS WHERE table_name = 'viirs_ais_matching_20200101_masaki' ) SELECT CONCAT( 'INSERT dataset.tablename (', ARRAY_TO_STRING(ARRAY(SELECT column_name FROM ColumnNames), ', '), ')'); 文字列 部分文字列: SUBSTR SUBSTR(カラム, 開始位置 [,長さ]) 日付 日付要素を取り出す EXTRACT(要素 FROM DATEカラム) 要素としては以下を指定できる\n DAY MONTH QUARTER: 範囲 [1,4] 内の値 YEAR DAYOFWEEK 日曜が1, 土曜が7 DAYOFYEAR WEEK: 範囲 [0, 53] 内の日付の週番号を返します。週は日曜日から始まり、年の最初の日曜日より前の日付は 0 週目です WEEK(\u0026lt;WEEKDAY\u0026gt;): 範囲 [0, 53] 内の日付の週番号を返します。週は WEEKDAY から始まります。最初の WEEKDAY より前の日付は、第 0 週になります。 WEEKDAY の有効な値は、SUNDAY、MONDAY、TUESDAY、WEDNESDAY、THURSDAY、FRIDAY、SATURDAY です。 ISOYEAR ISOWEEK  ランダムサンプル TABLESAMPLE 句 10%のレコードを抽出する。\nSELECT * FROM dataset.my_table TABLESAMPLE SYSTEM (10 PERCENT) 注意：ただし、TABLESAMPLE を使った場合は、データの抽出は行ごとに行われるのではない。\n大きいテーブルは「データブロック」と呼ばれる単位に分割されてで保存されているのだが、例えば、 TABLESAMPLE SYSTEM (20 PERCENT) とすると全体の 20% のデータブロックがランダムに抽出される。小さいテーブル（1GB未満）では１つのデータブロックに全てのデータが格納されるため、 TABLESAMPLE を使うと全てのデータが抽出される。\nそこで、行ごとに厳密にランダムに抽出したいときは、次の　WHERE rand() を使用する。ただし、WHERE rand() を使用するとテーブル全体をスキャンするのでコストが大きくなることに注意する。TABLESAMPLE を使うとテーブル全体をスキャンしないのでクエリのコストは小さくなる。\nWHERE rand() 10%のレコードを抽出する。\nSELECT * FROM dataset.my_table WHERE rand() \u0026lt; 0.1 ただし、 WHERE rand() を使用するとテーブル全体をスキャンするのでコストが大きくなることに注意する。TABLESAMPLE を使うとテーブル全体をスキャンしないのでクエリのコストは小さくなる。\nSharded Table Sharded Table へのアクセス\nSELECT ssvid FROM `world-fishing-827.pipe_production_v20201001.messages_scored_*` WHERE _TABLE_SUFFIX BETWEEN \u0026#39;20200101\u0026#39; AND \u0026#39;20201231\u0026#39; ARRAY ARRAYの型 SELECT ARRAY\u0026lt;INT64\u0026gt;[1,2,3], ARRAY\u0026lt;STRUCT\u0026lt;INT64, INT64\u0026gt;\u0026gt;, # ARRAY\u0026lt;ARRAY\u0026lt;INT64\u0026gt;\u0026gt;[[1,2],[3,4,],[5,6]], # ARRAYを要素に持つARRAYは作成できない ARRAY\u0026lt;STRUCT\u0026lt;ARRAY\u0026lt;INT64\u0026gt;\u0026gt;\u0026gt;,　# その代わりに ARRAYをSTRUCTで包めばそれを、ARRAYとして格納できる https://cloud.google.com/bigquery/docs/reference/standard-sql/arrays\nARRAY の作成 WITH ownner_pet AS ( SELECT \u0026#39;tony\u0026#39; as owner, [\u0026#39;dog\u0026#39;, \u0026#39;cat\u0026#39;] as pet UNION ALL SELECT \u0026#39;tim\u0026#39; as owner, [\u0026#39;cat\u0026#39;, \u0026#39;bird\u0026#39;] as pet UNION ALL SELECT \u0026#39;nancy\u0026#39; as owner, [\u0026#39;dog\u0026#39;, \u0026#39;fish\u0026#39;, \u0026#39;lizard\u0026#39;] as pet UNION ALL ) pet は arrayとする。\nownner_pet table\n   owner pet     tony dog    cat   tim cat    bird   nancy dog    fish    lizard    ARRAY を展開する LEFT JOIN UNNEST (array) LEFT JOIN UNNEST (array) を使用する\n下の例では ais_identity.n_imo が array\nこれをすると、 ais_identity.n_imo is null の行は除外される\nselect ssvid, best.best_flag, ais_identity.flag_mmsi, best.best_vessel_class, ais_identity.n_shipname, ais_identity.n_callsign, n_imo.value as n_imo_value, n_imo.count as n_imo_count, n_imo.type as n_imo_type from `world-fishing-827.gfw_research.vi_ssvid_v20200801` LEFT JOIN UNNEST(ais_identity.n_imo ) as n_imo WHERE IN UNNEST (array) Array の要素として特定の値が含まれるレコードを抽出する\nSELECT * FROM ownner_pet WHERE IN UNNEST (pet) WHERE EXISTS ( SELECT * FROM UNNEST (array) as A WHERE A = \u0026lsquo;X\u0026rsquo;) Array が特定の条件を満たすレコードを抽出する IN UNNEST でよりも複雑な条件を指定することができる。\nこの例は　IN UNNEST　と同じ結果を返すけど、ARRAYの要素が STRUCT 等の場合や、もっと複雑な条件を指定したい場合は WHERE EXIST を使う\nSELECT * FROM ownner_pet WHERE EXISTS ( SELECT * FROM UNNEST (pat) AS p WHERE p = \u0026#39;bear\u0026#39; ) ARRAY から一部の要素を取り出す 結果は元のテーブルの形式を保持したまま、一部の ARRAY 要素を取り出す\nWITH ownner_pet AS ( SELECT 'tony' as owner, ['dog', 'cat'] as pet UNION ALL SELECT 'tim' as owner, ['cat', 'bird'] as pet UNION ALL SELECT 'nancy' as owner, ['dog', 'fish', 'lizard'] as pet ) SELECT owner, ARRAY (SELECT p FROM UNNEST (pet) as p WHERE p LIKE 'c%' ) as pet FROM ownner_pet ARRAY の要素のソート # STRING を要素に持つ ARRAY カラム WITH ownner_pet AS ( SELECT \u0026#39;tony\u0026#39; as owner, [\u0026#39;dog\u0026#39;, \u0026#39;cat\u0026#39;] as pet UNION ALL SELECT \u0026#39;tim\u0026#39; as owner, [\u0026#39;cat\u0026#39;, \u0026#39;bird\u0026#39;] as pet UNION ALL SELECT \u0026#39;nancy\u0026#39; as owner, [\u0026#39;dog\u0026#39;, \u0026#39;fish\u0026#39;, \u0026#39;lizard\u0026#39;] as pet ) SELECT owner, ARRAY (SELECT * FROM UNNEST (pet) as p ORDER BY p ) as pet FROM ownner_pet # STRUCT を要素に持つ ARRAY カラム WITH ownner_pet AS ( SELECT \u0026#39;tony\u0026#39; as owner, [STRUCT(\u0026#39;dog\u0026#39; as species, 2 as count), (\u0026#39;cat\u0026#39;, 1)] as pet UNION ALL SELECT \u0026#39;tim\u0026#39; as owner, [(\u0026#39;cat\u0026#39;, 3), (\u0026#39;cow\u0026#39;, 1), (\u0026#39;horse\u0026#39;,3)] UNION ALL SELECT \u0026#39;nancy\u0026#39; as owner, [(\u0026#39;fish\u0026#39;, 10), (\u0026#39;bird\u0026#39;, 3), (\u0026#39;horse\u0026#39;,3)] ) SELECT owner, ARRAY (SELECT AS STRUCT * FROM UNNEST (pet) as p ORDER BY p.count, p.species) as pet FROM ownner_pet UDF を使った ARRAY 処理 CREATE FUNCTION SORT_ARRAY(arr ANY TYPE, ascending BOOL) AS ( IF (ascending, ARRAY (SELECT * FROM UNNEST (arr) a ORDER BY a), ARRAY (SELECT * FROM UNNEST (arr) a ORDER BY a DESC) ) ); WITH ownner_pet AS ( SELECT \u0026#39;tony\u0026#39; as owner, [\u0026#39;dog\u0026#39;, \u0026#39;cat\u0026#39;] as pet UNION ALL SELECT \u0026#39;tim\u0026#39; as owner, [\u0026#39;cat\u0026#39;, \u0026#39;bird\u0026#39;] as pet UNION ALL SELECT \u0026#39;nancy\u0026#39; as owner, [\u0026#39;dog\u0026#39;, \u0026#39;fish\u0026#39;, \u0026#39;lizard\u0026#39;] as pet ) SELECT owner, SORT_ARRAY(pet, TRUE) as pet FROM ownner_pet 番号を使った ARRAY 要素へのアクセス WITH sequences AS (SELECT [0, 1, 1, 2, 3, 5] AS some_numbers UNION ALL SELECT [2, 4, 8, 16, 32] AS some_numbers UNION ALL SELECT [5, 10] AS some_numbers) SELECT some_numbers, some_numbers[OFFSET(1)] AS offset_1, some_numbers[ORDINAL(1)] AS ordinal_1 FROM sequences; ARRAYの要素の結合 ARRAY_CONCAT() のような関数を使用して複数の配列を結合し、ARRAY_TO_STRING() を使用して配列を文字列に変換できます。\nUser Defined Functions # 定数をUDFとして保存する CREATE TEMP FUNCTION START_DATE() AS (TIMESTAMP(\u0026#39;2012-04-01\u0026#39;)); # 使い捨て関数 CREATE TEMP FUNCTION SORT_ARRAY(arr ANY TYPE, ascending BOOL) AS ( IF (ascending, ARRAY (SELECT * FROM UNNEST (arr) a ORDER BY a), ARRAY (SELECT * FROM UNNEST (arr) a ORDER BY a DESC) ) ); # テーブルを作成するときと同じようにUDFを保存することもできる CREATE OR REPLACE FUNCTION `your_project.your_dataset.SORT_ARRAY`(arr ANY TYPE, ascending BOOL) AS ( IF (ascending, ARRAY (SELECT * FROM UNNEST (arr) a ORDER BY a), ARRAY (SELECT * FROM UNNEST (arr) a ORDER BY a DESC) ) ); STRUCT # STRUCT を要素に持つカラム # これだとstructとしてまとめる意義があまりないかも WITH ownner_pet AS ( SELECT \u0026#39;tony\u0026#39; as owner, STRUCT(\u0026#39;dog\u0026#39; as species, 2 as count) as pet UNION ALL SELECT \u0026#39;tim\u0026#39; as owner, (\u0026#39;cat\u0026#39;, 3) UNION ALL SELECT \u0026#39;nancy\u0026#39; as owner, (\u0026#39;fish\u0026#39;, 10) ) select * from ownner_pet STRUCT は ARRAY の要素にするのがメインの使い方？？\n# STRUCT を要素に持つ ARRAY カラム WITH ownner_pet AS ( SELECT \u0026#39;tony\u0026#39; as owner, [STRUCT(\u0026#39;dog\u0026#39; as species, 2 as count), (\u0026#39;cat\u0026#39;, 1)] as pet UNION ALL SELECT \u0026#39;tim\u0026#39; as owner, [(\u0026#39;cat\u0026#39;, 3), (\u0026#39;cow\u0026#39;, 1), (\u0026#39;horse\u0026#39;,3)] UNION ALL SELECT \u0026#39;nancy\u0026#39; as owner, [(\u0026#39;fish\u0026#39;, 10), (\u0026#39;bird\u0026#39;, 3), (\u0026#39;horse\u0026#39;,3)] ) select * from ownner_pet WINDOW関数の結果でフィルター QUALIFY SELECT date, lat_bin, lon_bin, OrbitNumber, AVG(SATZ_GDNBO) as angle, # WINDOW FUNCITON RANK() OVER (PARTITION BY date, lat_bin, lon_bin ORDER BY AVG(SATZ_GDNBO)) AS rank FROM viirs GROUP BY date, lat_bin, lon_bin, OrbitNumber QUALIFY # Extract records having minimum AVG(SATZ_GDNBO) rank = 1 ) RECORD型 構造体のように複数の型の値を１つにまとめる。\nType: RECORD型 Mode: NULLABLE\nモードが NULLABLE なら1行につき1行の値しか持たない、この場合は identity.ssvid という列があるだけと思って問題ない\nidentity	RECORD	NULLABLE ssvid	STRING	NULLABLE MMSI (Maritime Mobile Service Identity) as source specific vessel ID (SSVID) n_shipname	STRING	NULLABLE Ship name recorded in AIS messages (normalized) n_callsign	STRING	NULLABLE International Radio Call Sign recorded in AIS (normalized) imo	STRING	NULLABLE Identity number given by the International Maritime Organization flag\nモードが REPEATED なら、1行につき複数行を持つ、展開するには LEFT JOIN UNNEST を使う。結果的に行数は増える。\nSELECT matched, # identity は NULLABLE なので . を使ってアクセスできる。 # 行数も増えない identity.ssvid, identity.n_shipname, # activity の要素へのアクセス a.first_timestamp FROM `world-fishing-827.vessel_database.all_vessels_v20211001` # activity は REPEATED なので LEFT JOIN UNNEST で展開する # 行数が増える # これは自分に自分を LEFT JOIN しているのでテーブル名が省略される LEFT JOIN UNNEST(activity) a WHERE identity.ssvid = \u0026#39;353154000\u0026#39; 行の抽出 WHERE EXIST HAVING QUALIFY\narray (selsct distinct as struct from unnest(registry) where list_uvi like \u0026ldquo;iccat\u0026rdquo;) as registry\nfrom all_vessels\nwhere array_length(registory) \u0026gt;0\nJSON # 値を文字列として抽出 JSON_EXTRACT(列, \u0026quot;$.要素名\u0026quot;) # 値を整数として抽 JSON_EXTRACT_SCALAR(列, \u0026quot;$.要素名\u0026quot;) パーティションドテーブルを作成する CREATE TABLE データセット.テーブル名 (スキーマ) PARTITION BY パーティションに利用する列 OPTIONS( オプション ) CREATE TABLE mydataset.newtable (transaction_id INT64, transaction_date DATE) PARTITION BY transaction_date OPTIONS( partition_expiration_days=3, require_partition_filter=true ) ワイルドカードでのテーブルの指定 シャーデットテーブルなど https://cloud.google.com/bigquery/docs/querying-wildcard-tables\nselect * from mydataset.my_sharded_table_* select * from mydataset.my_sharded_table_2020* select * from mydataset.my_sharded_table_* where _TABLE_SUFFIX BETWEEN \u0026#39;20200101\u0026#39; AND \u0026#39;20200301\u0026#39; "},{idx:18,href:"/notebook/miscellaneous/c_plus_plus/",title:"C++",content:"C++ https://hackingcpp.com/\n"},{idx:19,href:"/notebook/miscellaneous/english/",title:"english",content:"English 数学  数学用語 英語 一覧 Exponents(指数)の英語表現  "},{idx:20,href:"/notebook/miscellaneous/jinjia2/",title:"jinja",content:"jinja2 jinja2 はテンプレートエンジン\nテンプレートエンジンとは、要するに、テンプレートの一部を、別の変数の値で置き換えて、新しいドキュメント（htmlとか）を作成するためのツール。 webページの生成とかにも使われているらしい。\njinja2は、コマンドラインツールとしても、pythonパッケージとしても存在する。\nインストール コマンドラインツールのインストール\n pip install jinja2-cli 使い方 Rの glueパッケージと同様に、テンプレートの中で {{変数}} で記述した部分が変数の値で置き換わる。\n"},{idx:21,href:"/notebook/miscellaneous/jupyter/",title:"jupyter",content:"Visual Studio Code ショートカット jupyter の起動\n 起動しているサーバーのリスト jupyter notebook list "},{idx:22,href:"/notebook/miscellaneous/pc/",title:"PC",content:"PC CPU 4Gamer: Zen 3アーキテクチャ採用の新世代CPUはゲームにおける性能が大きく向上した: Ryzen 9 5900X, Ryzen 7 5800X\nメモリ規格 見れば全部わかるDDR4メモリ完全ガイド、規格からレイテンシ、本当の速さまで再確認\nオーバークロックメモリの基本と実際の性能、見れば全部わかるDDR4メモリ完全ガイド\nsigle rank, dual rank\nhttp://blog.livedoor.jp/ocworks/archives/52125318.html\nDDR4 メモリ規格\nDIMM デスクトップ用メモリ、 SO-DIMM ノートパソコン用メモリ\nRDIMM/UDIMM メモリにレジスタを備えているか、いないか、RDIMMはサーバー用（マザボ）、一般的なやつはUDIMM\nECC/non-ECC メモリのエラーチェック機能、CPUやマザボ側も対応している必要がある。基本的にはサーバー向けの機能\nメモリクロック DDR4−3200 = 3200 MHz データ転送レート クロック信号の周波数(DRAM frequency) ＝データ転送レートの半分 1600 MHz\nメモリの帯域幅 PC4-25600 １秒間に転送可能なデータ量 25600 MB/s\nメモリタイミング、レイテンシ 16-18-18-38 先頭の数字 CASレイテンシ CL16と表記されることもある CL16 = メモリコントローラの要求に対してメモリが応答するまで16クロック分の時間がかかる 小さい方が早い\nメモリクロックは1600 MHz\n1クロック = 1/1600,000,000 秒 16クロックは 16/1600,000,000 = 1/100,000,000 秒 = 10ns\nメモリランク チップ構成、シングルランク、デュアルランク、シングルの方が良いらしい\nSPDとXMP SPDとは、Serial Presence Detect 2666MHz 1.20V メモリの仕様をシステム側に伝えるための仕組み、何もしないとこのUEFI側はこの仕様でメモリを動作させる\nメモリがXMPに対応していると、UEFIででXMPを有効化すると、自動でXMPデータに基づく動作設定（オーバークロック設定）が適用される\nSPDは標準仕様に準拠した設定（デフォルトではこちらの設定で動作する）、XMPを有効にするとオーバークロック設定で動作する XMPはインテルの技術だが、AMDマザボでもマザボメーカーの努力により利用可能\n部品   X570 AORUS ELITE (rev. 1.0)\n  F4-3200C16Q-128GVK\n  Ryzen 5900X\n  ZOTAC GAMING GeForce RTX 3080 Trinity\n  plextor px-512m9pey\n  Kingston SSD A2000 1000GB 1TB M.2 2280 NVMe PCIe 3D TLC\n  Intel SSD 660p Series [M.2 PCI-E SSD 1TB]\n  CPU : AMD Ryzen 9 3900X Matisse [3.8GHz/12Core/TDP105W] 搭載モデル（標準構成価格220,620円） CPUグリス: Noctua NT-H1 [マイクロ粒子ハイブリッド化合物、高熱伝導率タイプ]（+2,040円） CPU-FAN : サイコムオリジナルAsetek 670LS + Enermax UCTB12P x2 [240mm水冷ユニット] ※メンテナンスフリー（+7,460円） MOTHER : GIGABYTE X570 AORUS ELITE [AMD X570chipset]（標準） MEMORY : 64GB[16GB*4枚] DDR4-3200 [メジャーチップ・8層基板] Dual Channel（+28,550円） READER : なし（標準） HDD/SSD : Intel SSD 660p Series [M.2 PCI-E SSD 1TB]（+8,730円） SSD-Option: なし（標準） HDD/SSD2 : TOSHIBA MD05ACA800 [高信頼 8TB 7200rpm 128MB]（+26,330円） OptDrive : なし（-2,050円） VGA : なし（Ryzen はオンボードグラフィック非対応）（-44,900円） ExCard : オンボードサウンド（標準） LAN : Gigabit LAN [1000BASE T] オンボード CASE : 【黒】CoolerMaster CM694（標準） CASE-Option: なし（標準） POWER : SilverStone SST-ST75F-GS V3 [750W/80PLUS Gold]★フルモジュラー高品質電源がお買い得！★（標準） OS : Microsoft(R) Windows10 Home (64bit) DSP版（標準） "},{idx:23,href:"/notebook/miscellaneous/tmux/",title:"tmux",content:"tmux インストール sudo apt-get install tmux\nhttps://qiita.com/shin-ch13/items/9d207a70ccc8467f7bab\n使い方 初めて tmux を実行する\ntmux 現在のシェルのバックグラウンドで tmux セッションが開始され、セッション管理下の shell が起動する。\ntmux はキーボードのみで操作する。\nデフォルトでは Ctrl と b を一度同時押ししてから、別のキーを押すことで tmux を操作する。\nCtrl b ではないキーの組み合わせに変更することもできる (~/.tmux.conf で設定する)。\nここでは Ctrl b の代わりに prefix と表記する。\nセッション作成 tmux new -s 名前 ``` ### セッションから離れる(detach) 元のターミナルのシェルに戻る。tmux のセッションはバックグラウンドで動き続ける。 `prefix d` ### セッションの一覧 セッションの外にいるとき ``` tmux list-sessions ``` セッションの中にいるとき ``` Ctrl-b s ``` セッション一覧を表示させた後で、↑↓キーでセッションを選んでリターンで選択できる。 ### 既存のセッションに再接続 ``` tmux a -t セッション名 ``` ### 既存のセッションを消去 ``` tmux kill-session -t 名前 ``` ## .tmux.conf 実行中の tmux サーバーに設定を再読み込みさせる ``` tmux source-file ~/.tmux.conf ``` セッションの中にいるときにも設定を入力できる。 `prefix` を押してから続けて以下を入力すると、マウスでスクロールすることができるようになる ``` :set -g mouse on ``` ### .tmux.conf 例 ``` # tmux起動時のシェルをzshにする set-option -g default-shell /bin/zsh # tmuxを256色表示できるようにする set-option -g default-terminal screen-256color set -g terminal-overrides 'xterm:colors=256' # prefixキーをC-qに変更 # set -g prefix C-q # C-bのキーバインドを解除 # unbind C-b # ステータスバーをトップに配置する set-option -g status-position top # 左右のステータスバーの長さを決定する set-option -g status-left-length 90 set-option -g status-right-length 90 # #P =\u0026gt; ペイン番号 # 最左に表示 set-option -g status-left '#H:[#P]' # Wi-Fi、バッテリー残量、現在時刻 # 最右に表示 set-option -g status-right '#(wifi) #(battery --tmux) [%Y-%m-%d(%a) %H:%M]' # ステータスバーを1秒毎に描画し直す set-option -g status-interval 1 # センタライズ（主にウィンドウ番号など） set-option -g status-justify centre # ステータスバーの色を設定する set-option -g status-bg \u0026quot;colour238\u0026quot; # status line の文字色を指定する。 set-option -g status-fg \u0026quot;colour255\u0026quot; # vimのキーバインドでペインを移動する bind h select-pane -L bind j select-pane -D bind k select-pane -U bind l select-pane -R # vimのキーバインドでペインをリサイズする bind -r H resize-pane -L 5 bind -r J resize-pane -D 5 bind -r K resize-pane -U 5 bind -r L resize-pane -R 5 # | でペインを縦分割する bind | split-window -h # - でペインを縦分割する bind - split-window -v # 番号基準値を変更 set-option -g base-index 1 # マウス操作を有効にする set-option -g mouse on bind -n WheelUpPane if-shell -F -t = \u0026quot;#{mouse_any_flag}\u0026quot; \u0026quot;send-keys -M\u0026quot; \u0026quot;if -Ft= '#{pane_in_mode}' 'send-keys -M' 'copy-mode -e'\u0026quot; # コピーモードを設定する # コピーモードでvimキーバインドを使う setw -g mode-keys vi # 'v' で選択を始める bind -T copy-mode-vi v send -X begin-selection # 'V' で行選択 bind -T copy-mode-vi V send -X select-line # 'C-v' で矩形選択 bind -T copy-mode-vi C-v send -X rectangle-toggle # 'y' でヤンク bind -T copy-mode-vi y send -X copy-selection # 'Y' で行ヤンク bind -T copy-mode-vi Y send -X copy-line # 'C-p'でペースト bind-key C-p paste-buffer ```"},{idx:24,href:"/notebook/miscellaneous/vscode/",title:"vscode",content:"Visual Studio Code ショートカット ショートカットエディタ Preferences \u0026gt;\u0026gt; Keyboard Shortcuts\n【Mac版】 VSCode キーボードショートカット\n コマンドパレット shift cmd P タブの移動 opt cmd → 単語単位でカーソル移動 opt →  正規表現検索・置換 検索置換のショートカット opt cmd F\n検索BOXの横の .* ボタンを押す\n正規表現の記号    記号 意味 例     ^ 行頭    $ 行末    . 任意の１文字    .* 任意の１文字が0文字以上連続する    .+ 任意の１文字が１文字以上連続する    [] []内の任意の１文字 [ABC]+ AまたはBまたはCが1文字以上連続する    検索で認識された文字列を置換の際に使う 検索BOXで正規表現を()でくくり、置換BOXの中で$1や$2で指定する。\n検索BOX： 「(.+)と(.+)」\n置換BOX： 【$1】が【$2】\n高度な編集 選択範囲を移動させる\n文字列を選択 + alt + 矢印\n"},{idx:25,href:"/notebook/miscellaneous/windows/",title:"Windows",content:"Windows ディスクのマウント/アンマウント コマンドプロンプトを管理者権限で起動し、以下のコマンド\n# マウント mountvol D:\\ \\\\?\\Volume{b57198fe-cc68-4509-b771-f81d0fdae196}\\ # アンマウント（Dドライブをアンマウントする） mountvol D:\\ /P b57198fe-cc68-4509-b771-f81d0fdae196 の部分はボリューム名というらしい、mountvol コマンドをただ打つとヘルプと現在認識されているディスクのボリューム名が一覧で表示される。\nPS C:\\Users\\hoge\u0026gt; mountvol ボリューム マウント ポイントを作成、削除、一覧を表示します。 MOUNTVOL [ドライブ:]パス ボリューム名 MOUNTVOL [ドライブ:]パス /D MOUNTVOL [ドライブ:]パス /L MOUNTVOL [ドライブ:]パス /P MOUNTVOL /R MOUNTVOL /N MOUNTVOL /E MOUNTVOL drive: /S パス マウント ポイントを常駐させる既存の NTFS ディレクトリ を指定します ボリューム名 マウント ポイントのターゲットとなるボリューム名を指定しま す。 /D 指定されたディレクトリからボリューム マウント ポイント を削除します。 /L 指定されたディレクトリのマウントされているボリューム の一覧を表示します。 /P 指定されたディレクトリからボリューム マウント ポイントを削除 してボリュームをマウント解除し、ボリュームをマウントできな くします。 ボリューム マウント ポイントを作成して、もう一度ボリュームを マウントできるようにします。 /R システムに存在しないマウント ポイント ディレクトリとレジストリ 設定を削除します。 /N 新しいボリュームの自動マウントを無効にします。 /E 新しいボリュームの自動マウントを再び有効にします。 /S EFI システム パーティションを与えられたドライブにマウントします。 現在のマウント ポイントとボリューム名の考えられる値: \\\\?\\Volume{d2265820-bb12-01d6-1028-1369895deb00}\\ E:\\ \\\\?\\Volume{b57198fe-cc68-4509-b771-f81d0fdae196}\\ D:\\ \\\\?\\Volume{7f4f03ed-23d7-489e-81dc-1333ac3da622}\\ F:\\ \\\\?\\Volume{76a0f51c-c474-4ca5-a2ba-9d469890533e}\\ *** マウント ポイントなし *** \\\\?\\Volume{0d94bb6d-8073-4afe-a5ad-2ea4f4477814}\\ C:\\ \\\\?\\Volume{9b4800bb-ee55-4d7b-8d43-d27a508cb4b9}\\ H:\\ \\\\?\\Volume{0c910d1b-3fd3-4197-ab02-c4bc40f08a75}\\ G:\\ \\\\?\\Volume{9b35ef88-f1bc-4843-8386-aa63f8dd4f0f}\\ *** マウント ポイントなし *** \\\\?\\Volume{9d84eb26-32f1-11eb-acb3-001bdc0e914c}\\ I:\\ "},{idx:26,href:"/notebook/miscellaneous/wsl/",title:"wsl",content:"WSL Windows Subsystem for Linux の Tips\n https://xn--xdocs-c53drk.microsoft.com/ja-jp/windows/wsl/install\nインストールしているLinux仮想マシンと、そのWSLバージョンの確認 wsl -l -v PS C:\\Users\\XXX\u0026gt; wsl -l -v NAME STATE VERSION * Ubuntu-18.04 Running 1 Ubuntu-20.04 Running 1 WSL2を有効化する powershellを管理者権限で開いて、 以下のコマンドを実行\ndism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart 更にBIOSからCPUの仮想化を有効にする必要がある。（PCのメーカーにより名称が異なる） だいたいCPUのAdvanced Settingのところにあるぽい。\nIntel Virtualization Technology, AMD-V, Hyper-V VT-X Vanderpool SVM Linux をインストールするときのデフォルトのWSLバージョンを指定する wsl --set-default-version 2 既にインストールした Linux 仮想マシンのWSLバージョンを変更する WSL1 から WSL2に変更\nwsl --set-version \u0026lt;distro name\u0026gt; 2 WSL2 でOnedrive 内のファイルのパーミッション変更を許可するための設定 /etc/wsl.conf をエディタで開き\nsudo nano /etc/wsl.conf 以下の記述を追加して\n[automount] options = \u0026quot;metadata\u0026quot; wslを再起動する。PowerShellで 以下のコマンドを入力する。\nwsl --shutdown "},{idx:27,href:"/notebook/mac/homebrew/",title:"homebrew",content:"homebrew 基本コマンド  brew search [TEXT|/REGEX/] brew info [FORMULA...] brew install FORMULA brew update brew upgrade [FORMULA...] brew uninstall FORMULA... brew list [FORMULA...] brew doctor : 診断  インストール先： /usr/local/Cellar\nもう少し詳細には：　/usr/local/Cellar/パッケージ名/バージョン\n"},{idx:28,href:"/notebook/miscellaneous/hugo/",title:"Hugo",content:"Hugo Hugoはマークダウンから静的サイトのためのhtmlを生成するソフトウェア\n Hugoフォルダの構成  \u0008/contents フォルダの中にサイトの内容を記述したマークダウン形式のファイルを配置する。\nマークダウンの内容の先頭には、ファイルの情報を記述する（YAML、TOML、JSONのいずれかの形式）。\n"},{idx:29,href:"/notebook/maritime/glossary/",title:"glossary",content:"用語集 組織 RFMO Regional Fisheries Management Office 地域の漁業管理組織 IMO International Maritime Organization 国際海事専門機関\nRFMO（マグロ類） WCPFC Western and Central Pacific Fisheries Commission 中西部太平洋まぐろ類委員会 IATTC Inter-American Tropical Tuna Commission 全米熱帯まぐろ類委員会 ICCAT International Commission for the Conservation of Atlantic Tunas 大西洋まぐろ類保存国際委員会 IOTC Indian Ocean Tuna Commission インド洋まぐろ類委員会 CCSBT Commission for the Conservation of Southern Bluefin Tuna みなみまぐろ保存委員会 tuna RFMO\nRFMO（その他） NPFC North Pacific Fisheries Commission 北太平洋漁業委員会 SPRMO South Pacific Regional Fisheries Management Organisation 南太平洋地域漁業管理機構 CCAMLR Commission for the Conservation of Antarctic Marine Living Resources 南極の海洋生物資源の保存に関する委員会 NEAFC North East Atlantic Fisheries Commission 北東大西洋漁業委員会 SEAFO South East Atlantic Fisheries Organisation 南東大西洋漁業機関 SIOFA Southern Indian Ocean Fisheries Agreement 南インド洋漁業協定 NPAFC North Pacific Anadromous Fish Commission 北太平洋溯河性魚類委員会\n組織その他 JFA Japan Fisheries Agency 水産庁 FRA Fisheries Research and Education Agency; 国立研究開発法人　水産研究・教育機構 ANCORS Australian National Centre for Ocean Resources and Security オーストラリアのFRAに相当する？ MRAG Asia Pacific オーストラリアにある漁業資源管理？コンサルタント MRAGが何の略なのかわからない Marine Resouce A. G. FFA Pacific Islands Forum Fisheries Agency JAFIC Japan Fisheries Information Service Center 日本漁業情報サービスセンター 社団法人？ VMSのデータベースはここにおいてあり、情報の解析などもやっているらしい 水産海洋学会 http://www.jsfo.jp/sympo/history.html\nRFMO用語 CPC ICCATに加盟している国、または、協力的加盟国、地域、組織、 Contracting Parties and Cooperating non-Contracting Parties, Entities or Fishing Entities (CPCs) CCM WCPFCに加盟している国、または、協力的加盟国、地域、組織 CMM Conservation and Management Measure, NPFC で採択された規制 TCC Technical and Compliance Committee WCPFCなどの中にある委員会の一つ MCS tool Monitoring, Control and Surveilance 漁船の動きを監視するツール AIS VMS http://www.fao.org/3/V4250E/V4250E03.htm CDS Catch Documentation Schemes 漁獲された時点から、流通の間、水産品を追跡するシステム http://www.fao.org/in-action/globefish/fishery-information/resource-detail/en/c/426994/ ROP Regional Observer Programme RFMOが要請している船にオブザーバーを乗せるプログラム？ 海事用語 IMO番号 IMO Ship identification number 船に割り当てられて、廃船になるまで変わらない １ノット 船舶の速度の単位。1ノットは1時間に1海里（約1,852メートル）を進む速度。1海里は陸上での1マイル（約1,609メートル）とは異なる。 コールサイン 無線局に対して一意に割り当てられる番号。 船や飛行機などの無線機なども一意に振られている？？ 全長 LOA: Length Overall 総トン数 Gross Tonnage GT 国内総トン数：日本国内ではトンは船舶の体積を表す 1000⁄353 m3 = 1トン = 約2.832 861立方メートル、船舶のトン数の測度に関する法律 国際総トン数：重量の単位、IMOにより制定 FOC Flag of Convinience ship 便宜置籍船、船が登録された国（旗国）と、船を実際に運用している国が異なる船のこと 旗船 Flag State 登録された船の所属国 Port State Control PSC 入港時の立入検査 IMO でPSCについての監督手続きに関する決議 外国船舶監督官 PSCO Port State Control Officer 転載 transshipment 船から船への荷物や人の積み替え cargo ship 貨物船 Reefer refrigerated cargo ship 冷凍船 bunker （燃料）補給船 MMSI Maritime Mobile Service Identity 海上移動業務識別コード、AISなどの通信機器を識別するために発行されるID\n条約・協定  PSMA: Agreement on Port State Measures (PSMA)  違法漁業防止寄港国措置協定 https://www.fao.org/port-state-measures/en/    水産学・漁業用語 水産資源評価の専門用語の解説 漁労体 ぎょろうたい Fishing unit 海面漁業経営体 かいめんぎょぎょうけいえいたい Marine fishery establishment 水揚機関 みずあげきかん Catch landing organization 航海数 こうかいすう Frequency of fishing trips 出漁日数 しゅつりょうにっすう Number of fishing days 漁労日数 ぎょろうにっすう Number of operating days 漁獲量 ぎょかくりょう Catch 漁獲努力あたりの漁獲量 CPUE catch per unit effort 操業水域区分 そうぎょうすいいきくぶん Major fishing areas for statistical purposes 指定湖沼 していこしょう Designated lakes（and marshes） 魚群 a school of fish 漁場 fishing grounds\n漁業・漁法 漁業種別 沿岸漁業 えんがんぎょぎょう Coastal fishery 沖合漁業 おきあいぎょぎょう Offshore fishery 遠洋漁業 えんようぎょぎょう Distant water fishery 海面漁業 かいめんぎょぎょう Marine fishery 内水面漁業 ないすいめんぎょぎょう Inland water fishery 養殖業　ようしょくぎょう Aquaculture\n漁法 Gear type 漁船の漁獲装置の種類 底びき網漁業 そこびきあみぎょぎょう Trawl fishery 船びき網漁業 ふなびきあみぎょぎょう Boat seine fishery 地びき網漁業 じびきあみぎょぎょう Dragnet fishery まき網漁業 まきあみぎょぎょう Purse seine fishery 刺網漁業 さしあみぎょぎょう Gill net fishery 敷網漁業 しきあみぎょぎょう Lift net fishery 定置網漁業 ていちあみぎょぎょう Set net fishery はえ縄漁業 はえなわぎょぎょう Long line fishery 釣漁業 つりぎょぎょう Angling fishery 遊漁 遊漁 ゆうぎょ Recreational fishing 遊漁案内業 ゆうぎょあんないぎょう Recreational fishing guide 海面船釣遊漁船業者 かいめんふなづりゆうぎょせんぎょうしゃ Recreational boat fishing guide 遊漁採捕量 ゆうぎょさいほりょう Catch quantity by recreational boat fishing 養殖 養殖施設 ようしょくしせつ Aquaculture facility 養殖施設面積 ようしょくしせつめんせき Facilities area for marine aquaculture 投餌量 とうじりょう Quantity of feeding 収獲量〔海面養殖業〕 しゅうかくりょう〔かいめんようしょくぎょう〕 Yield〔Marine aquaculture〕 種苗販売量 しゅびょうはんばいりょう Quantity of sold seedlings 漁の道具 FADs Fish Aggregating Devices マグロなどの魚を引きつけるために海に浮かべる機械、巻き網で使うらしい\n魚種 (Pacific) saury サンマ chub mackerel マサバ blue mackerel ゴマサバ Horse mackerel アジ billfishes カジキ一般 Marlin マカジキ Swordfish メカジキ groundfish 底生魚 toothfish マジェランアイナメ 南極周辺の深海魚、ギンダラの代わりに使われている\n海洋 ECS East China Sea WCPO Western Central Pacific Ocean\n持続可能な漁業  FIP : Fishery Improvement Project  持続可能な漁業を推進するために地域漁業者と進めているプロジェクト一般（シーフードレガシーなどが進めている）   IUU : Illegal Unreported Unregulated \u0026ldquo;three no\u0026rdquo; vessels : No boat name and number, No homeport, No boat certification. 主に中国漁船における用語、中国政府においても登録されていない、管理外になっている？？  観測技術 AIS Automatic Identification System 船が位置情報を発信している。基本的には船の衝突防止のためのシステムなのでAIS装置は他のAIS装置の信号を受信できる。International Maritime Organization (IMO) により一定サイズ（300GT）以上の船には搭載が義務付けられている。 人工衛星や地上の基地局により受信される。データを受信した企業がデータを売っている。数秒に１度発信されるが、受診する衛星側の問題で受信間隔は変わる。AIS CLASS-A と CLASS-B があり、CLASS-Bは電波弱い。 VMS vessel monitoring system VMSは一般名称なので実際に使われているシステムは国により異なる。 こちらも船が発信する信号を衛星で捉えている場合が多いらしい。 VMSの搭載の規制は国レベルで行われている。基本的には政府の内部データである。AISよりも時間解像度が低いことが多いらしい VIIRS Visible Infrared Imaging Radiometer Suite 可視近赤外放射計群 地表面から発せられる可視光や赤外線を検出する受動センサー、Suomi NPP衛星に搭載されている SAR Synthetic Aperture Radar 合成開口レーダー 人工衛星からレーダーを発しその反射波で地形を観測する。天気や昼夜に依存しないで観測できるが、ノイズもある。 DSC Digital Selective Calling デジタル選択呼出装置、AISや無線などで、MMSIにより識別される電波の送受信装置、救難信号などを発する\n人工衛星 ALOS だいち 陸域観測衛星 SARを搭載 ALOS-2＝大地２ AISとSARの両方のデータを受信できるから、AISとSARの検出のマッチングが容易にできる？ SUOMI NPP 2011年に打ち上げられた。VIIRSを搭載している。 NOAA/NASAにより運用されている。 JPSS-1（旧称 NOAA-20） VIIRSを搭載している\n"},{idx:30,href:"/notebook/gcp/",title:"Google Cloud Platform",content:"Google Cloud Platform  BigQuery  gloud コマンド アカウントとプロジェクトの確認 現在のデフォルトのアカウントとプロジェクトの確認\ngcloud config list [core] account = xxxx@gmail.com disable_usage_reporting = True Your active configuration is: [default] アカウントの切り替え gcloud auth login 表示されるURLにアクセスして、切り替えたい google アカウントにログインする、次にブラウザに表示されたコードをコピーして、ターミナルに入力する。\nプロジェクトの切り替え gcloud config set project [PROJECT_ID] Updated property [core/project]. Compute Engine SSHでのログイン https://qiita.com/NewGyu/items/3a65e837519297951e79\nローカルでアカウントが有効になっているか確かめる\n$ gcloud config list gcloud コマンドで ssh\ngcloud compute ssh --project=プロジェクトID --zone=ゾーン インスタンス名 Google Cloud Strage バケットの中身を確認する gsutil ls gs://BUCKET ローカルファイルをGCSにアップロードする gsutil cp LOCAL_FILE gs://MY_BUCKET/ # 並列でアップロードする場合（100M単位で分割） gsutil -o GSUtil:parallel_composite_upload_threshold=100M　cp LOCAL_FILE gs://MY_BUCKET/ "},{idx:31,href:"/notebook/r/leaflet/",title:"leaflet",content:"leaflet leaflet はインタラクティブな地図の可視化をするためのパッケージ\nhttps://rstudio.github.io/leaflet/\n例 df \u0026lt;- data.frame( lon = rnorm(10, 140, 1), lat = rnorm(10, 35.7, 1), val1 = LETTERS[1:10], val2 = runif(10, 1, 100) ) df %\u0026gt;% mutate(caption = paste(\u0026#34;lon=\u0026#34;, lon, \u0026#34;\u0026lt;br\u0026gt; lat = \u0026#34;, lat)) %\u0026gt;% leaflet::leaflet() %\u0026gt;% leaflet::addTiles() %\u0026gt;% leaflet::setView(lng = 140, lat = 35.7, zoom = 7) %\u0026gt;% leaflet::addCircleMarkers( lng = ~ lon, lat = ~ lat, color = rainbow(10), popup = ~ caption radius = 10, weight = 2, ) leaflet::leaflet() ggplot() と同様に、データを最初に leaflet() に渡して描画オブジェクトを作成し、それにレイヤーを %\u0026gt;% で追加してゆく。\nleaflet::leaflet() %\u0026gt;% leaflet::addTiles() %\u0026gt;% # 背景とする地図レイヤー leaflet::setView(lng=-100,lat=-2,zoom=5) # 初期位置とズームレベル leaflet( data = NULL, width = NULL, height = NULL, padding = 0, options = leafletOptions(), elementId = NULL, sizingPolicy = leafletSizingPolicy(padding = padding) ) マーカーを表示する マーカーとは地図上の点に対して「アイコン」または「円」を表示する。マーカーをマウスでクリックすると情報が表示される。\n点の位置情報として使えるのは緯度経度の2次元\n sp パッケージの SpatialPoints か SpatialPointsDataFrame sf パッケージの POINT, sfc_POINT （MULTIPOINT はサポートされていない） 緯度・経度をカラムとして持つデータフレーム (カラム名が lat/latitude and lon/lng/long/longitude の場合は自動で見つかる) 緯度経度を格納したベクター  addMarkers() addMarkers(map, lng = NULL, lat = NULL, layerId = NULL, group = NULL, icon = NULL, popup = NULL, popupOptions = NULL, label = NULL, labelOptions = NULL, options = markerOptions(), clusterOptions = NULL, clusterId = NULL, data = getMapData(map)) addCircleMarkers() addCircleMarkers(map, lng = NULL, lat = NULL, radius = 10, layerId = NULL, group = NULL, stroke = TRUE, color = \u0026quot;#03F\u0026quot;, weight = 5, opacity = 0.5, fill = TRUE, fillColor = color, fillOpacity = 0.2, dashArray = NULL, popup = NULL, popupOptions = NULL, label = NULL, labelOptions = NULL, options = pathOptions(), clusterOptions = NULL, clusterId = NULL, data = getMapData(map)) addLabelOnlyMarkers addLabelOnlyMarkers(map, lng = NULL, lat = NULL, layerId = NULL, group = NULL, icon = NULL, label = NULL, labelOptions = NULL, options = markerOptions(), clusterOptions = NULL, clusterId = NULL, data = getMapData(map)) その他関数 addControl(map, html, position = c(\u0026quot;topleft\u0026quot;, \u0026quot;topright\u0026quot;, \u0026quot;bottomleft\u0026quot;, \u0026quot;bottomright\u0026quot;), layerId = NULL, className = \u0026quot;info legend\u0026quot;, data = getMapData(map)) addTiles(map, urlTemplate = \u0026quot;//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\u0026quot;, attribution = NULL, layerId = NULL, group = NULL, options = tileOptions(), data = getMapData(map)) addWMSTiles(map, baseUrl, layerId = NULL, group = NULL, options = WMSTileOptions(), attribution = NULL, layers = \u0026quot;\u0026quot;, data = getMapData(map)) addPopups(map, lng = NULL, lat = NULL, popup, layerId = NULL, group = NULL, options = popupOptions(), data = getMapData(map)) highlightOptions(stroke = NULL, color = NULL, weight = NULL, opacity = NULL, fill = NULL, fillColor = NULL, fillOpacity = NULL, dashArray = NULL, bringToFront = NULL, sendToBack = NULL) addCircles(map, lng = NULL, lat = NULL, radius = 10, layerId = NULL, group = NULL, stroke = TRUE, color = \u0026quot;#03F\u0026quot;, weight = 5, opacity = 0.5, fill = TRUE, fillColor = color, fillOpacity = 0.2, dashArray = NULL, popup = NULL, popupOptions = NULL, label = NULL, labelOptions = NULL, options = pathOptions(), highlightOptions = NULL, data = getMapData(map)) addPolylines(map, lng = NULL, lat = NULL, layerId = NULL, group = NULL, stroke = TRUE, color = \u0026quot;#03F\u0026quot;, weight = 5, opacity = 0.5, fill = FALSE, fillColor = color, fillOpacity = 0.2, dashArray = NULL, smoothFactor = 1, noClip = FALSE, popup = NULL, popupOptions = NULL, label = NULL, labelOptions = NULL, options = pathOptions(), highlightOptions = NULL, data = getMapData(map)) addRectangles(map, lng1, lat1, lng2, lat2, layerId = NULL, group = NULL, stroke = TRUE, color = \u0026quot;#03F\u0026quot;, weight = 5, opacity = 0.5, fill = TRUE, fillColor = color, fillOpacity = 0.2, dashArray = NULL, smoothFactor = 1, noClip = FALSE, popup = NULL, popupOptions = NULL, label = NULL, labelOptions = NULL, options = pathOptions(), highlightOptions = NULL, data = getMapData(map)) addPolygons(map, lng = NULL, lat = NULL, layerId = NULL, group = NULL, stroke = TRUE, color = \u0026quot;#03F\u0026quot;, weight = 5, opacity = 0.5, fill = TRUE, fillColor = color, fillOpacity = 0.2, dashArray = NULL, smoothFactor = 1, noClip = FALSE, popup = NULL, popupOptions = NULL, label = NULL, labelOptions = NULL, options = pathOptions(), highlightOptions = NULL, data = getMapData(map)) addGeoJSON(map, geojson, layerId = NULL, group = NULL, stroke = TRUE, color = \u0026quot;#03F\u0026quot;, weight = 5, opacity = 0.5, fill = TRUE, fillColor = color, fillOpacity = 0.2, dashArray = NULL, smoothFactor = 1, noClip = FALSE, options = pathOptions(), data = getMapData(map)) addTopoJSON(map, topojson, layerId = NULL, group = NULL, stroke = TRUE, color = \u0026quot;#03F\u0026quot;, weight = 5, opacity = 0.5, fill = TRUE, fillColor = color, fillOpacity = 0.2, dashArray = NULL, smoothFactor = 1, noClip = FALSE, options = pathOptions()) "},{idx:32,href:"/notebook/r/rhdf5/",title:"rhdf5",content:"rhdf5 HDF5形式のファイルを扱うためのパッケージ\nCRANにはなく bioconductor のパッケージらしいので、bioconductor のレポジトリからインストールする\nrhdf5 - HDF5 interface for R\nインストール install.packages(\u0026quot;BiocManager\u0026quot;) BiocManager::install(\u0026quot;rhdf5\u0026quot;) 基本的な関数 ファイルの構造を表示する h5ls(\u0026quot;myhdf5file.h5\u0026quot;) ファイルの中身を読み取る D = h5read(\u0026quot;myhdf5file.h5\u0026quot;,\u0026quot;foo/A\u0026quot;) ファイルを開く h5f = H5Fopen(\u0026quot;myhdf5file.h5\u0026quot;) # アクセス h5f$df "},{idx:33,href:"/notebook/r/ggplot2/",title:"ggplot2",content:"ggplot2  ggplot2: Elegant Graphics for Data Analysis The R Graph Gallery  色々なグラフの例とコード    基本的な使い方 使い方の例\nset.seed(7) data_df \u0026lt;- data.frame( var1 = 1:10, var2 = rnorm(10), var3 = rep(LETTERS[1:3], length.out = 10)) ggplot(data = data_df) + geom_point(aes(x = var1, y = var2, color = var3)) + geom_line(aes(x = var1, y = var2, color = var3)) ggplot() 関数で ggplot オブジェクトを作成し、その ggplot オブジェクトに対して、 + 演算子を使って作図したいグラフの種類（geom_*() 関数）を指定する。 geom_*() 関数の中で aes() 関数を使って作図に使用する変数を指定する。\nggplot() ggplot(data = NULL, mapping = aes(), ...)  data : プロットしたいデータを含むデータフレームを渡す。データフレームは tidydata の形式になっていることが想定されている。 mapping : 作図に使用する列名を aes() 関数を介して指定する。 例 : aes(x = longitude, y = latitude, color = height, fill = height)  geom_*() グラフの種類ごとに geom_*() 関数が存在する\nちなみに geom は geometry （ジオメトリ）の略、グラフの基礎となる構造を指定する\ngeom_line() geom_point() aes() aes() 関数は、 ggplot() や geom_*() の中で使う。具体的には aes() の出力を ggplot() や geom_*() の　mapping 引数に渡す。基本的には geom_*() 関数の中で使うのが一般的。\naes() 関数は、散布図や折れ線などのグラフの座標（x, y）や線の色（color）、塗りつぶしの色（fill）、サイズ (size) などに 「使用する列」 を指定する。\nちなみに aes　は aesthetic（エステティック） の略、軸や色の指定に使用する変数を指定する\n# 都市の位置（ longitude, latitude）に点をプロット # 都市名 (city) で色分け # 点のサイズは人口 (population) に比例 data(data_df) + geom_point(aes(x = longitude, y = latitude, color = city, size = population)) データのグループ分け 例えば、 geom_path() などでグループごとに別々の線を書きたい場合などに使う\naes(x = X, y = Y, group = A) # 変数Aの値を使ってグループ分け aes(x = X, y = Y, group = interaction(A , B)) # 複数変数を使う場合 グラフの種類: geom_*() geom は geometry （ジオメトリ）の略、グラフの基礎となる構造を指定する\n散布図 geom_point(aes(x, y)) 点の種類 shape\n  color = 外部色　フチなし\n  1 丸, color 色指定 外部が塗りつぶされる\n  color = 縁の色 縁あり\n  19\n  geom_point(aes(fill=id, size=id), colour=\u0026ldquo;black\u0026rdquo;, shape=21, stroke = 2)\n折れ線・経路 geom_line(aes(x, y)) geom_path(aes(x, y)) geom_line() や geom_path() で x 軸に指定した変数が factor の場合は aes(group=1) を指定する。\nヒストグラム・密度分布 連続変数 x の値のビンごとの度数、頻度を、棒グラフ、曲線、折線で描画する\ngeom_histogram() # 棒グラフ（デフォルトは頻度分布） geom_density() # なめらかな曲線（デフォルトは密度文王） geom_freqpoly() # 折線（デフォルトは頻度） 頻度分布と密度分布の切り替え いずれの geom_* でも、aes() の中で y を指定することで縦軸をカウント ..count.. 、密度（%） ..density.. のどちらにも対応できる\ngeom_histogram(aes(x, y = ..density..)) geom_density( aes(x, y = ..count.. )) 色分けした変数の位置 position = \u0026quot;identity\u0026quot; # 重ね描き position = \u0026quot;stack\u0026quot; # 積み上げ position = \u0026quot;dodge\u0026quot;` # 隣接 position = \u0026quot;fill\u0026quot; # 割合 ビンの切り方: stat_bin() stat_bin() の binwidth から下の引数は geom_*() の中でも指定できる。\nつまり、次の２つの書き方は等価\ngeom_histgram(aes(x), binwidth = 0.1) geom_histgram(aes(x)) + stat_bin(binwidth = 0.1) stat_bin( mapping = NULL, data = NULL, geom = \u0026quot;bar\u0026quot;, position = \u0026quot;stack\u0026quot;, ..., binwidth = NULL, # ビン幅 bins = NULL, # ビン数 center = NULL, boundary = NULL, breaks = NULL, # ビンの切れ目 closed = c(\u0026quot;right\u0026quot;, \u0026quot;left\u0026quot;), pad = FALSE, na.rm = FALSE, orientation = NA, show.legend = NA, inherit.aes = TRUE ) x が離散変数なら stat_count() の方がいい\n連続変数の離散化 # 変数 x を10個の等間隔のbinに切る cut_interval(x, 10) 棒グラフ 1変数（カテゴリ変数）だけ指定すると、指定されたカテゴリ変数の値の数をそれぞれカウントした値が Y 軸になる。stat = \u0026quot;count\u0026quot; がデフォルトなので、暗黙に stat_count() が使用される。\nggplot(df) + geom_bar(aes(x = category)) # 以下は上と同義 #geom_bar(aes(x = category), stat = \u0026#34;count\u0026#34;) X 軸（カテゴリ変数）と Y 軸（量的変数）の値を別々に指定する場合 stat = \u0026quot;identity\u0026quot;\nggplot(df) + geom_bar(aes(x = category, y = value), stat = \u0026#34;identity\u0026#34;) stat = \u0026quot;identity\u0026quot; は geom_bar(aes(x = category, y = value)) + stat_identity() と同じ意味になる？\n棒の間に隙間を開けない時は width = 1\ngeom_bar(aes(x = category), width = 1) 積み上げ棒グラフ X 軸（カテゴリ変数）と Y 軸（量的変数）のほかに、変数Z（カテゴリ変数）で色分けした積み上げ棒グラフ aes(fill = z)\nggplot(df) + geom_bar(aes(x = category, y = value, fill = category2), stat = \u0026#34;identity\u0026#34;) 積み上げ棒グラフの縦軸を割合にする position = \u0026quot;fill\u0026quot;\nggplot(df) + geom_bar(aes(x = category, y = value, fill = category2), stat = \u0026quot;identity\u0026quot;, position = \u0026quot;fill\u0026quot;) + scale_y_continuous(labels = percent) 棒の順序を変える 数が多い順に棒を並べ替えるなど。基本的には、予め、x軸（カテゴリ）ごとのy軸の値（カウントなど）を計算しておく必要がある。つまり、 stat = \u0026quot;identity\u0026quot; を指定するやり方が前提。\n変数 y 軸の値を使って x 軸の順番を並べ替える。\n# 昇順の場合 ggplot(df, aes(x=reorder(x, y), y=y), stat = \u0026quot;identity\u0026quot;) # 降順の場合 ggplot(df, aes(x=reorder(x, desc(y)), y=y), stat = \u0026quot;identity\u0026quot;) reorder(x, y) は基本的に relevel() の特殊なバージョン、つまり、変数 x を factor に変換し、その level の順序を 変数 y の値に戻づいて設定する。\n棒の向きを水平にする 横向きの棒グラフを作成するには最後に coord_flip() を付け加えるだけ\nggplot(df) + geom_bar(aes(x = category, y = value, fill = category2), stat = \u0026#34;identity\u0026#34;) + coord_flip() 箱ひげ図・バイオリンプロット geom_violin( mapping = NULL, data = NULL, stat = \u0026quot;ydensity\u0026quot;, position = \u0026quot;dodge\u0026quot;, ..., draw_quantiles = NULL, trim = TRUE, scale = \u0026quot;area\u0026quot;, na.rm = FALSE, orientation = NA, show.legend = NA, inherit.aes = TRUE ) 線分 # 垂直線 geom_vline(xintercept = 10) # 水平線 geom_hline(yintercept = 110) # 傾いた線 geom_abline(intercept = 37, slope = -5)+ linetype=\u0026ldquo;dashed\u0026rdquo;\nテキストラベル # 文字列 geom_text() # Boxで囲まれた文字列 geom_label() # 同じ annotate(\u0026quot;text\u0026quot;, x = bbox[\u0026quot;xmin\u0026quot;], y = bbox[\u0026quot;ymax\u0026quot;], label = \u0026quot;hoge\u0026quot;, vjust=1, hjust=1) annotate(\u0026quot;label\u0026quot;, x = bbox[\u0026quot;xmin\u0026quot;], y = bbox[\u0026quot;ymax\u0026quot;], label = \u0026quot;hoge, vjust=1, hjust=1) p \u0026lt;- ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() p + annotate(\u0026#34;text\u0026#34;, x = 4, y = 25, label = \u0026#34;Some text\u0026#34;) p + annotate(\u0026#34;text\u0026#34;, x = 2:5, y = 25, label = \u0026#34;Some text\u0026#34;) p + annotate(\u0026#34;rect\u0026#34;, xmin = 3, xmax = 4.2, ymin = 12, ymax = 21, alpha = .2) p + annotate(\u0026#34;segment\u0026#34;, x = 2.5, xend = 4, y = 15, yend = 25, colour = \u0026#34;blue\u0026#34;) p + annotate(\u0026#34;pointrange\u0026#34;, x = 3.5, y = 20, ymin = 12, ymax = 28, colour = \u0026#34;red\u0026#34;, size = 1.5) p + annotate(\u0026#34;text\u0026#34;, x = 2:3, y = 20:21, label = c(\u0026#34;my label\u0026#34;, \u0026#34;label 2\u0026#34;)) p + annotate(\u0026#34;text\u0026#34;, x = 4, y = 25, label = \u0026#34;italic(R) ^ 2 == 0.75\u0026#34;, parse = TRUE) p + annotate(\u0026#34;text\u0026#34;, x = 4, y = 25, label = \u0026#34;paste(italic(R) ^ 2, \\\u0026#34; = .75\\\u0026#34;)\u0026#34;, parse = TRUE) 点の形 geom_point() の shape 引数に数値を指定することで点の形を指定する\n色の指定 色分けに使用する変数は aes() の中で aes(color = var1, fill = var2) のように指定する。\ncolor （線や点の色）に対しては scale_color_gradient()\nfill （塗りつぶし色）に対しては scale_fill_gradient()\nをそれぞれ使用する。\n色もある種の軸であるので　scale_x_continuous() と同様に scale_color_continuous() や scale_fill_continuous() のように scale_*_()関数の一種として扱われている。\n色の種類 連続値に対する色つけ : scale_*_gradient 連続値: integer, numeric\ninteger は factor にしないと離散値とはみなされない\nscale_colour_gradient(..., low = \u0026#34;#132B43\u0026#34;, high = \u0026#34;#56B1F7\u0026#34;, space = \u0026#34;Lab\u0026#34;, na.value = \u0026#34;grey50\u0026#34;, guide = \u0026#34;colourbar\u0026#34;, aesthetics = \u0026#34;colour\u0026#34;) scale_fill_gradient(..., low = \u0026#34;#132B43\u0026#34;, high = \u0026#34;#56B1F7\u0026#34;, space = \u0026#34;Lab\u0026#34;, na.value = \u0026#34;grey50\u0026#34;, guide = \u0026#34;colourbar\u0026#34;, aesthetics = \u0026#34;fill\u0026#34;) scale_colour_gradient2(..., low = muted(\u0026#34;red\u0026#34;), mid = \u0026#34;white\u0026#34;, high = muted(\u0026#34;blue\u0026#34;), midpoint = 0, space = \u0026#34;Lab\u0026#34;, na.value = \u0026#34;grey50\u0026#34;, guide = \u0026#34;colourbar\u0026#34;, aesthetics = \u0026#34;colour\u0026#34;) scale_fill_gradient2(..., low = muted(\u0026#34;red\u0026#34;), mid = \u0026#34;white\u0026#34;, high = muted(\u0026#34;blue\u0026#34;), midpoint = 0, space = \u0026#34;Lab\u0026#34;, na.value = \u0026#34;grey50\u0026#34;, guide = \u0026#34;colourbar\u0026#34;, aesthetics = \u0026#34;fill\u0026#34;) scale_colour_gradientn(..., colours, values = NULL, space = \u0026#34;Lab\u0026#34;, na.value = \u0026#34;grey50\u0026#34;, guide = \u0026#34;colourbar\u0026#34;, aesthetics = \u0026#34;colour\u0026#34;, colors) scale_fill_gradientn(..., colours, values = NULL, space = \u0026#34;Lab\u0026#34;, na.value = \u0026#34;grey50\u0026#34;, guide = \u0026#34;colourbar\u0026#34;, aesthetics = \u0026#34;fill\u0026#34;, colors) 離散値に対する色つけ 離散値: character, factor\ninteger は factor にしないと離散値とはみなされない\nレイヤーごとに異なる色の軸を指定する ggnewscale::new_scale(\u0026#34;color\u0026#34;) ggnewscale::new_scale(\u0026#34;fill\u0026#34;) ggnewscale::new_scale_color() ggnewscale::new_scale_fill() new_scale(new_aes)\nnew_scale_colour()\nタイトル labs( title = waiver(), # タイトル subtitle = waiver(), # サブタイトル caption = waiver(), # キャプション、右下 tag = waiver() # タグ、上左 ) タイトルとサブタイトル ggtitle(label, subtitle = waiver()) 軸のスケールや目盛の設定: scale_*() 軸に対する様々な設定は scale_軸_データ型() 関数により行う。\n 軸の種類： x, y, color, fill, alpha, size, linetype, radius, shape 軸のデータ型: condinuous,descrete,date, datetime, binned  scale_軸_データ型()\nscale_[x,y,color,fill]_[condinuous,descrete,date]( breaks=c(1,2,3,4), # 目盛位置 labels = c(\u0026#34;01\u0026#34;,\u0026#34;02\u0026#34;, \u0026#34;03\u0026#34;,\u0026#34;04\u0026#34;), # 目盛に表示する値のラベル limits=c(0,120), # 表示する値の範囲 # 表示する値の範囲を絞った時に、範囲外の値をどのように表示するか # oob = scales::censor, # NAに置換する # oob = scales::squish, # 範囲外の値に変換する # oob = scales::squish_infinite, # 無限値を範囲外の値に変換する oob = scales::censor(), trans = \u0026#34;log10\u0026#34;, # 軸のスケールを変換する関数  name = \u0026#34;X axis\u0026#34;, # 軸の名前 guide = guide_axis(n.dodge = 2), # 目盛のラベルが重なっているときに位置をずらす expand = c(0,0) # x軸, y軸に対して、上下の隙間の大きさ、c(0,0)は隙間なし ) geom_barやgeom_histgramでの軸の変換\n軸の名前 X軸ラベル、Y軸ラベル\nxlab(label) ylab(label) 2軸グラフ # Value used to transform the data coeff \u0026lt;- 30 ggplot(count_df, aes(x=date)) + geom_line( aes(y=n_vessel), color = \u0026#34;red\u0026#34;) + geom_line( aes(y=n_data / coeff), color=\u0026#34;blue\u0026#34;) + # Divide by 10 to get the same range than the temperature scale_y_continuous( # Features of the first axis name = \u0026#34;First Axis\u0026#34;, # Add a second axis and specify its features sec.axis = sec_axis(~.*coeff, name=\u0026#34;Second Axis\u0026#34;) ) 凡例 凡例を消す 特定の凡例（colour）を消す１\nguides(colour=FALSE) 特定の凡例（colour）を消す２\nscale_colour_discrete(guide=FALSE) 全ての凡例を消す\ntheme(legend.position = \u0026#39;none\u0026#39;) 凡例のタイトルを消す\ntheme(legend.title = element_blank()) 凡例の位置 # テキストで位置を指定 # \u0026#34;none\u0026#34;、\u0026#34;left\u0026#34;、\u0026#34;right\u0026#34;、\u0026#34;bottom\u0026#34;、\u0026#34;top\u0026#34; theme(legend.position=\u0026#34;right\u0026#34;) # 右 # 数値ベクトルc(x,y)で位置を指定 theme(legend.position=c(0,0)) # 左下 theme(legend.position=c(1,1)) # 右上 theme(legend.position=c(0.5,0.5)) # 中央 凡例の点のサイズ変更 デフォルトでは geom_*() で指定したサイズで凡例の点も表示されるが、点が小さい時には困る。凡例だけで大きいサイズでプロットしたい場合。\nggplot()+ # guides() 関数の中で指定する場合 guides()(color = guide_legend(override.aes = list(size = 5)))+ # scale_*() 関数の中で指定する場合 scale_fill_manual( values = c( \u0026#34;A\u0026#34; = \u0026#34;blue\u0026#34;, \u0026#34;B\u0026#34; = \u0026#34;cyan\u0026#34;, \u0026#34;C\u0026#34; = \u0026#34;yellow\u0026#34; ), guide = guide_legend(override.aes = list(size = 5)) ) 凡例の名前（変数名）を変更する labs(shape=\u0026#34;Male/Female\u0026#34;, colour=\u0026#34;Male/Female\u0026#34;) 画像として保存する : ggsave() gsave( filename, plot = last_plot(), device = NULL, path = NULL, scale = 1, width = NA, height = NA, units = c(\u0026#34;in\u0026#34;, \u0026#34;cm\u0026#34;, \u0026#34;mm\u0026#34;), dpi = 300, limitsize = TRUE, ...)  filename : ファイル名、拡張子で出力形式は自動で判別される plot : ggplotオブジェクト、デフォルトでは最後にプロットしたものが使われる device : 出力形式：\u0026ldquo;eps\u0026rdquo;, \u0026ldquo;ps\u0026rdquo;, \u0026ldquo;tex\u0026rdquo; (pictex), \u0026ldquo;pdf\u0026rdquo;, \u0026ldquo;jpeg\u0026rdquo;, \u0026ldquo;tiff\u0026rdquo;, \u0026ldquo;png\u0026rdquo;, \u0026ldquo;bmp\u0026rdquo;, \u0026ldquo;svg\u0026rdquo; or \u0026ldquo;wmf\u0026rdquo; (windows only) path : 保存先のパス filename と合体する scale	: 指定した出力サイズを scale 倍する width : 幅 height : 高さ units : 幅と高さの単位 (\u0026ldquo;in\u0026rdquo;, \u0026ldquo;cm\u0026rdquo;, \u0026ldquo;mm\u0026rdquo;) dpi : ラスター画像の解像度 dot per inch、文字列でも指定できる \u0026ldquo;retina\u0026rdquo; (320), \u0026ldquo;print\u0026rdquo; (300), or \u0026ldquo;screen\u0026rdquo; (72) limitsize : TRUE だと 50x50インチより大きいサイズでプロットしない、エラーを防ぐため  テーマ テーマの要素の色を変える\n詳しくはこちら\nhttps://www.rdocumentation.org/packages/ggplot2/versions/3.3.2/topics/theme\ncolor \u0026lt;- \u0026#34;black\u0026#34; theme(#rect = element_rect(colour = color, fill = color), #text = element_text(color = \u0026#34;white\u0026#34;), # プロット領域の背景 plot.background = element_rect(colour = color, fill = color), # パネル全体の背景 panel.background = element_rect(colour = color, fill = color), #legend.key = element_rect(colour = color, fill = color), #panel.border = element_rect(fill = NA, colour = \u0026#34;white\u0026#34;, size = 1), #panel.grid.major = element_line(colour = \u0026#34;grey60\u0026#34;), #panel.grid.minor = element_line(colour = \u0026#34;grey30\u0026#34;), #axis.text = element_text(colour = \u0026#34;white\u0026#34;), )+ 複数のプロットを１つをまとめる patchwork パッケージを使うのが楽ちん\nhttps://qiita.com/nozma/items/4512623bea296ccb74ba\n基本的には + 演算子で複数の ggplot オブジェクトを1つにまとめる\nlibrary(ggplot2) library(patchwork) p1 \u0026lt;- ggplot(mtcars) + geom_point(aes(mpg, disp)) p2 \u0026lt;- ggplot(mtcars) + geom_boxplot(aes(gear, disp, group = gear)) p1 + p2 p1 + p2 は p1 と p2 を左右に並べる\np1 と p2 を縦方向に並べるときは plot_layout(ncol = 1) を加える\np1 + p2 + plot_layout(ncol = 1, heights = c(3, 1)) 間隔を開けたいときは plot_spacer()\np1 + plot_spacer() + p2 複雑な構造を指定するときは {} を使用する\n# p1 は左、p2は右上、p3は右下 p1 + {p2 + p3 + plot_layout(ncol = 1)} 図全体のタイトルや、サブ図ごとの図表番号などの指定\np1 + p2 + plot_annotation( title = \u0026#34;Title\u0026#34;, subtitle = \u0026#34;Subtitle\u0026#34;, caption = \u0026#34;Caption\u0026#34;, tag_levels = \u0026#34;A\u0026#34;, tag_prefix = \u0026#34;fig \u0026#34;, tag_suffix = \u0026#34;:\u0026#34; ) 凡例が共通の時は1つにまとめることができる。\np1 + p2 + plot_layout(guides = \u0026#34;collect\u0026#34;) \u0026amp; # さらに凡例の位置を下にする theme(legend.position=\u0026#39;bottom\u0026#39;) 特定の変数の値を使って図を分離する facet_grid() や facet_wrap() を使う\nfacet_grid() は図を2次元に配置する\nggplot(diamonds, aes(x=carat, y=price)) + geom_point(aes(colour=clarity)) + facet_grid(. ~ color) # Horizontal 横に並べる #facet_grid(color ~ .) # Vertical 縦に並べる テキストボックスを追加する ggtext パッケージを使うのが楽\nlibrary(ggtext) gglot()+ geom_textbox(aes(x, y, label, )) 引数\n nudge_x, nudge_y ボックスの位置座標からのオフセット box.padding : 長さ4のベクトル、ボックス外の余白を指定 grid::unit(c(1,1,1,1), \u0026ldquo;cm\u0026rdquo;)) box.margin : 長さ4のベクトル、ボックス外の余白を指定 box.r : 長さ1のベクトル、ボックスの角の丸みを指定 grid::unit(c(1), \u0026ldquo;cm\u0026rdquo;)) width, height : 長さ1のベクトル、ボックスの幅と高さを指定 grid::unit(c(1), \u0026ldquo;cm\u0026rdquo;)) minwidth, maxwidth: ボックスの幅の最小最大値 minheight, maxheight: ボックスの高さの最小最大値  aes()\n x : ボックス位置のx座標 y : ボックス位置のy座標 label : テキスト colour : テキストとボックスの縁線の色 box.colour : ボックスの縁線の色 box.size : ボックスの縁線の太さ fill : ボックスの色 alpha : 透明度 halign ボックス外のテキストの水平位置 0なら左寄せ、1なら右寄せ valign ボックス外のテキストの垂直位置 0なら下寄せ、1なら上寄せ orientation : テキストの向き \u0026ldquo;upright\u0026rdquo;, \u0026ldquo;left-rotated\u0026rdquo;, \u0026ldquo;right-rotated\u0026rdquo;, \u0026ldquo;inverted\u0026rdquo;. hjust : 0ならボックスの左端を座標に合わせる、1なら右端を座標に合わせる vjust : 0ならボックスの下端を座標に合わせる、1なら上端を座標に合わせる text.colour : テキストの色 size : フォントサイズ family : フォント fontface : bold italic lineheight : ? group : ?  "},{idx:34,href:"/notebook/miscellaneous/git/",title:"git",content:"Git Gitの基本コマンド\n.gitignoreの仕様詳解\nHappy Git and GitHub for the useR\nサル先生のGit入門\nサブコマンド    サブコマンド 実行内容     clone リポジトリのクローンを作成する   init リポジトリを新規作成する、または既存のリポジトリを初期化する   remote リモートリポジトリを関連付けする   fetch リモートリポジトリの内容を取得する   pull リモートリポジトリの内容を取得し、現在のブランチに取り込む（「fetch」と「merge」を行う）   push ローカルリポジトリの変更内容をリモートリポジトリに送信する   add ファイルをインデックスに追加する（コミットの対象にする）   rm ファイルをインデックスから削除する   mv ファイルやディレクトリの名前を変更する   reset ファイルをインデックスから削除し、特定のコミットの状態まで戻す   status ワークツリーにあるファイルの状態を表示する   show ファイルの内容やコミットの差分などを表示する   diff コミット同士やコミットとワークツリーの内容を比較する   commit インデックスに追加した変更をリポジトリに記録する   tag コミットにタグを付ける、削除する、一覧表示する   log コミット時のログを表示する   grep リポジトリで管理されているファイルをパターン検索する   branch ブランチを作成、削除、一覧表示する   checkout ワークツリーを異なるブランチに切り替える   merge 他のブランチやコミットの内容を現在のブランチに取り込む   rebase コミットを再適用する（ブランチの分岐点を変更したり、コミットの順番を入れ替えたりできる）   config 現在の設定を取得、変更する    レポジトリを作成する、コピーする init : 既存のフォルダをレポジトリにする git init して登録されたフォルダがレポジトリとなる。他の人は、このレポジトリを clone することで、開発に参加できる。\nこの git init して作成されたオリジナルのレポジトリは origin と呼ばれる。\nclone : 既存のレポジトリをコピーする 既にあるレポジトリの内容を複製して、ローカルにレポジトリのクローンを作成する\nこのクローンに、変更を加えることで、元レポジトリの開発に参加できる。\ngit clone オリジン・レポジトリのURL.git\nクローンしたレポジトリのオリジナルのレポジトリの確認方法\ngit remote -v\n既にあるレポジトリへの操作 add 作成したファイルを git の管理下に追加する（ステージングする）。一度追加したら同じファイルを再度追加する必要はない。\ngit は add されたファイルの変更を検出し、\ngit add path_to_file git add . # カレント以下の全てのファイル git add *.java # 拡張子が .java add の取り消し git reset HEAD [ファイル名/ファイルパス] commit ステージングしたファイルの変更内容を記録する（コミットする）\ngit commit -m 'コメント' push リモートレポジトリに反映する 現状のローカルレポジトリの最新 コミット 状態を、リモートレポジトリに反映する（プッシュする）\ngit push レポジトリ ブランチ 例：git push origin master\nここで origin レポジトリは、このレポジトリの大本である、リモートにあるレポジトリを意味する（clone するもとになったやつ）\nブランチ ブランチとは１つのレポジトリで複数の状態（例えば正式版、開発版）などを保持しておく仕組み。ブランチを切り替えるとフォルダの中身も変化する。開発版のブランチに切り替えてファイルを編集すると正式版を壊すことなく、開発版を変更して、開発版が完成したら、それを正式版のブランチにマージするといったことができる。\nレポジトリを作成したときにできる最初のブランチが master\nローカルレポジトリとリモートレポジトリの\n既存のブランチを確認する git branch ブランチの作成 : branch git branch ブランチ名 # ブランチの作成 git checkout -b ブランチ名 #ブランチの作成とそのブランチへの切り替え git branch ブランチ名 だとブランチを作成するだけだが、git checkout -b ブランチ名 にするとブランチを作成して、そのブランチに切り替える（git branch ブランチ名; git checkout ブランチ名; と同じ）\nブランチの切り替え : checkout ブランチを切り替えると、フォルダの中身の状態が、そのブランチの最新の状態になる\ngit checkout ブランチ名 カレントブランチの確認 カレントブランチには * が付く\n$ git branch --contains * develop master ブランチの削除 ローカルにあるブランチを削除する\ngit branch \u0026ndash;delete [ブランチ名] git branch -d [ブランチ名] git branch -D [ブランチ名]\nブランチをマージする branchA に branchB の内容を統合する\n# まずは branchA にチェックアウトして git checkout branchA # そこに branchB をマージする git merge branchB リモートにあるブランチをローカルに持ってくる # 最初にリモートブランチにチェックアウトして git checkout origin/[ブランチ名] # 次にしそれをローカルブランチとしてコピーする git checkout -b [ブランチ名] 最初のコマンドを打った後に次のようなメッセージが出る。 最初のコマンドを売った状態だと、リモートレポジトリの内容を見て、試しに変更したりできるけど、変更を commit してもリモートにもローカルにも反映されないらしい。変更をローカルレポジトリとしてコピーするために2番目のコマンドを使う。その後は、commit した変更を保存することができる。\nNote: checking out 'origin/[ブランチ名]'. You are in 'detached HEAD' state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -b with the checkout command again. Example: git checkout -b \u0026lt;new-branch-name\u0026gt; HEAD is now at a3fe87c updated where the tables are saved, and fixed source and source_ssvid fields. Also made small edit to extract_top_match.sql.j2 ローカルで作成したブランチをリモートにプッシュする # ローカルに新しいブランチを作成する git checkout -b [ブランチ名] # ローカルブランチをリモートのレポジトリにプッシュする # この場合、「リモートレポジトリのmainブランチ」(origin)が、作成したブランチの上流ブランチとして設定される。 git push --set-upstream origin [ブランチ名] ローカルブランチをコピーして別のローカルブランチを作る # コピーしたいローカルブランチ(hoge)にチェックアウトする git checkout hoge # そこで新しいレポジトリ(hoge_copied)を作る git checkout -b hoge_copied ローカルとリモートのブランチ名を変更する ブランチ名を hoge から foo に変更する例\n# ローカルのブランチ名変更 git branch -m hoge foo # リモートのブランチを消す git push origin :hoge # 変更済みローカルブランチをプッシュ git push origin foo レポジトリの状態を表示する : status git status 新たに作成されたけど add されていないファイルや、編集されたけど commit されていないファイルなどを表示する。\nファイル/フォルダを復元する 以前にコミットした状態に復元する\ngit checkout [コミット番号] [ファイルパス] ファイルへの操作 基本的に、名前の変更や削除など、ファイル・ディレクトリに対する操作は全て git を通して行う。そうしないと git がそれらの変更をトラッキングできない。\nファイル名を変えて内容を変更するとき、git を使わないでファイル名を変更すると、元のファイルとは別のファイルとして扱われるので、どこを変更したのかわからなくなる。\n# ファイルの削除 git rm # ファイルの移動・名前変更 git mv ファイル名を変更する・移動する : git mv git mv ローカルで削除したファイル・フォルダを、リモートレポジトリからも削除する git rm を使ってファイルを削除した場合には、その変更を commit \u0026amp; push すればリモートレポジトリからもファイルは削除される。 しかし、それ以外の方法でファイルを削除して　commit \u0026amp; push　してもリモートレポジトリにはファイルが残ってしまう。\nリモートレポジトリにあるファイルを削除するには以下のようにする。\n# キャッシュを削除する git rm --cached /path/to/消したいファイル.txt git rm --cached -r /path/to/消したいフォルダ # コミット git commit -m \u0026#34;remove some files\u0026#34; # リモートに反映 git push ローカルフォルダには、ファイルやフォルダがあっても、リモートレポジトリにはアップロードされないようにしたい。 既に、リモートレポジトリにアップロードされてしまったファイル・フォルダを、リモートレポジトリから削除したい\n 後から、.gitignore に追加削除した場合 .gitignore の内容と関係なく、後から、特定のファイル・フォルダを除外したい場合  .gitignore に新たにファイルやフォルダを追加した場合も同じ\nあるいは、以前は追跡していたファイル・フォルダの追跡をやめて、リモートレポジトリから削除したい場合\n# .gitignore に追加した後、あるいは、ファイルやフォルダを削除した後 # キャッシュを削除する git rm --cached /path/to/消したいファイル.txt git rm --cached -r /path/to/消したいフォルダ # .gitignore を編集したなら git add .gitignore # git status # コミット git commit -m \u0026quot;remove some files\u0026quot; # リモートに反映 git push origin master Githubへのプルリク プルリク（pull request）は git ではなく Github や Gitlab の機能\nブランチをマージする前にチェックして、Github 上でマージできる\n初心者向けGithubへのPullRequest方法\n  リモートリポジトリをローカルに clone する。\n git clone https://github.com/hoge/hoge.git    ローカルで、編集用に新しいブランチ new_branch を作成し、そこに切り替える\n cd hoge git branch new_branch git checkout new_branch    編集し、add commit する\n ファイルを編集する git add 変更したファイル git commit -m 'コメント'    リモートの push する\n  pull requestする\n  git clone https://github.com/teuder/test.git\n【cloneされたリモートリポジトリ】 = 【origin リポジトリ】\n"},{idx:35,href:"/notebook/r/raster/",title:"raster",content:"raster # GooTiffの読み込み data \u0026lt;- raster::raster(\u0026quot;path/data.tif\u0026quot;) 値と座標のデータフレームに変換する\ndf \u0026lt;- raster::rasterToPoint(data) 緯度経度と値のデータフレームからラスタデータを作成する test \u0026lt;- raster::rasterize(x = data[c(\u0026quot;lon\u0026quot;,\u0026quot;lat\u0026quot;)], # データフレームの経度、緯度のカラムの指定、n行2列の行列でもいい y = raster::raster(ncols=32512, nrows=24576), # 雛形となるラスタオブジェクト（ピクセル数を指定する） field = data$rad, # 値のベクター fun = mean, # セルに複数の点が含まれるとき、セルの値を計算する関数 filename = \u0026quot;./output/peru_20190828_32512_24576.tif\u0026quot; # ファイル出力も一緒にやる場合 ) #raster::writeRaster(test, \u0026quot;./output/peru_20190828_4608_6096.tif\u0026quot;, overwrite=TRUE) 関数 rasterize ## S4 method for signature 'matrix,Raster' rasterize(x, y, field, fun='last', background=NA, mask=FALSE, update=FALSE, updateValue='all', filename=\u0026quot;\u0026quot;, na.rm=TRUE, ...) ## S4 method for signature 'SpatialPoints,Raster' rasterize(x, y, field, fun='last', background=NA, mask=FALSE, update=FALSE, updateValue='all', filename=\u0026quot;\u0026quot;, na.rm=TRUE, ...) ## S4 method for signature 'SpatialLines,Raster' rasterize(x, y, field, fun='last', background=NA, mask=FALSE, update=FALSE, updateValue='all', filename=\u0026quot;\u0026quot;, ...) ## S4 method for signature 'SpatialPolygons,Raster' rasterize(x, y, field, fun='last', background=NA, mask=FALSE, update=FALSE, updateValue='all', filename=\u0026quot;\u0026quot;, getCover=FALSE, silent=TRUE, ...) ラスタの各セルに、空間データオブジェクト（点、線、ポリゴン）と紐づいた値を、付与する。\nポリゴン：ポリゴンがセルの中心に被っていたら値が転送される。線：その線と触れているセル全てに値が転送される。この挙動を、ポリゴンを線としてラスタ化した後で、ポリゴンをポリゴンとしてラスタ化することで、合体させることができる？？\nxが点を表す場合、各点がセルと紐づく。セル同士の境界に点が落ちる場合は右側あるいは下側のセルに配置される。セルに付与される値は、点の値と関数　fun により決定される。\nx 各セルに値を転送したい地物のオブジェクト 点を表すオブジェクト（SpatialPointsオブジェクト、２列行列、２列データフレーム）、 SpatialLinesオブジェクト、 SpatialPolygons*オブジェクト、 あるいはその拡張オブジェクト\ny 値を転送したいラスタオブジェクト（Raster*）（既存のラスタオブジェクト）\nfield 数値あるいは文字列。ラスタに転送する値。スカラ値、あるいは、数値ベクター（長さはxの地物の数と同じ） x が Spatial*DataFrame の場合は、カラム名を指定することができる。 指定しない場合、その属性インデックスが\nIf missing, the attribute index is used (i.e. numbers from 1 to the number of features). You can also provide a vector with the same length as the number of spatial features, or a matrix where the number of rows matches the number of spatial features\nfun function or character. To determine what values to assign to cells that are covered by multiple spatial features. You can use functions such as min, max, or mean, or one of the following character values: \u0026lsquo;first\u0026rsquo;, \u0026lsquo;last\u0026rsquo;, \u0026lsquo;count\u0026rsquo;. The default value is \u0026lsquo;last\u0026rsquo;. In the case of SpatialLines*, \u0026lsquo;length\u0026rsquo; is also allowed (currently for planar coordinate systems only).\nIf x represents points, fun must accept a na.rm argument, either explicitly or through \u0026lsquo;dots\u0026rsquo;. This means that fun=length fails, but fun=function(x,\u0026hellip;)length(x) works, although it ignores the na.rm argument. To use the na.rm argument you can use a function like this: fun=function(x, na.rm)if (na.rm) length(na.omit(x)) else (length(x), or use a function that removes NA values in all cases, like this function to compute the number of unique values per grid cell \u0026ldquo;richness\u0026rdquo;: fun=function(x, \u0026hellip;) {length(unique(na.omit(x)))} . If you want to count the number of points in each grid cell, you can use fun=\u0026lsquo;count\u0026rsquo; or fun=function(x,\u0026hellip;){length(x)}.\nYou can also pass multiple functions using a statement like fun=function(x, \u0026hellip;) c(length(x),mean(x)), in which case the returned object is a RasterBrick (multiple layers).\nbackground numeric. Value to put in the cells that are not covered by any of the features of x. Default is NA\nmask logical. If TRUE the values of the input Raster object are \u0026lsquo;masked\u0026rsquo; by the spatial features of x. That is, cells that spatially overlap with the spatial features retain their values, the other cells become NA. Default is FALSE. This option cannot be used when update=TRUE\nupdate logical. If TRUE, the values of the Raster* object are updated for the cells that overlap the spatial features of x. Default is FALSE. Cannot be used when mask=TRUE\nupdateValue numeric (normally an integer), or character. Only relevant when update=TRUE. Select, by their values, the cells to be updated with the values of the spatial features. Valid character values are \u0026lsquo;all\u0026rsquo;, \u0026lsquo;NA\u0026rsquo;, and \u0026lsquo;!NA\u0026rsquo;. Default is \u0026lsquo;all\u0026rsquo;\nfilename character. Output filename (optional)\nna.rm If TRUE, NA values are removed if fun honors the na.rm argument\ngetCover logical. If TRUE, the fraction of each grid cell that is covered by the polygons is returned (and the values of field, fun, mask, and update are ignored. The fraction covered is estimated by dividing each cell into 100 subcells and determining presence/absence of the polygon in the center of each subcell\nsilent Logical. If TRUE, feedback on the polygon count is suppressed. Default is FALSE\n\u0026hellip; Additional arguments for file writing as for writeRaster\n"},{idx:36,href:"/notebook/r/sf/",title:"sf",content:"sf 定義されたクラス  sfg ：個別の地物オブジェクト sfc ： sfg オブジェクトのリスト、リストの各要素が１地物に相当 sf ： sfc オブジェクトを geometry 列としてもつデータフレーム、１行が１地物、 geometory 列以外の列は地物がもつ値  地物（sfgオブジェクト）の型  POINT：点 LINESTRING：線分 POLYGON：多角形 MULTIPOINT：点の集合 MULTILINESTRING：線分の集合 GEOMETORYCOLLECTION：様々な型のデータの集合  sfオブジェクトの読み込み・書き出し https://r-spatial.github.io/sf/articles/sf2.html\npref_simple_sf \u0026lt;- sf::st_read(\u0026#34;../output/pref_simple.shp\u0026#34;) sf::st_write(pref_simple_sf, \u0026#34;../output/pref_simple.shp\u0026#34;, layer_options = \u0026#34;ENCODING=UTF-8\u0026#34;) sf オブジェクトの作成 緯度経度列を持つデータフレームから作成する : st_as_sf() １行が１点を表すデータフレーム df （緯度 lat 経度 lon）から sf オブジェクトを作成する。coords = c(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;)、はXY座標に相当するカラム名を指定している。crs = 4326 で座標参照系を設定している。EPSGコード 4326 は WGS84 を表す。 remove = FALSE は coords = c(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;) で指定した列を削除しないという意味。\n# df は 各行が1POINTを表していて、その座標が \u0026#34;lon\u0026#34;, \u0026#34;lat\u0026#34; という列名で保持されているデータフレーム sf \u0026lt;- df %\u0026gt;% st_as_sf(coords = c(\u0026#34;lon\u0026#34;, \u0026#34;lat\u0026#34;), dim = \u0026#34;XY\u0026#34;, remove = FALSE, crs = 4326) WKT列を持つデータフレームから作成する # squid_area_df の geometry には WKT文字列が格納されている squid_area_sf \u0026lt;- sf::st_as_sf(squid_area_df, wkt = \u0026#34;geometry\u0026#34;, crs=4326) データフレームから作成する : st_set_geometry() すでにある sfc オブジェクトを、データフレームに、geometry 列として追加する。\ndata_sf \u0026lt;- st_set_geometry(data_df, geom_sfc)\nWKT形式の文字列ベクトルから作成 area_wkt \u0026lt;- \u0026#34;Polygon ((165.06947186552821449 48.95836601433594382, 169.89568502076190271 45.13709077274640435, -149.95899646070347444 44.98161022438796408, -149.95031841566071762 53.86930346517581114, -163.23108263759604597 52.2378116240458894, -168.71541576854343703 50.45207289660321948, 164.95978520290927349 51.55926363240160981, 165.06947186552821449 48.95836601433594382))\u0026#34; # SFC の作成 target_sfc \u0026lt;- sf::st_as_sfc(area_wkt) sf::st_crs(target_sfc) \u0026lt;- 4326 # SFCからSFの作成 data_df \u0026lt;- tibble::tibble(data=1) target_sf \u0026lt;- sf::st_set_geometry(data_df, target_sfc) 処理関数 sfオブジェクトの切り抜き st_crop() \nland_crop \u0026lt;- st_crop(land, c(xmax=180, xmin=-180, ymin = -80, ymax=80)) 日付変更線で地物に切れ目を作る -180:180 の経度で作成された地物は、日付変更線をまたいでいるときにうまく描画・処理できない。そこで、日付変更線のラインで地物を分断する。\nsf::st_wrap_dateline(options = c(\u0026quot;WRAPDATELINE=YES\u0026quot;, \u0026quot;DATELINEOFFSET=0\u0026quot;), quiet = FALSE) 日付変更線の切れ目をつなぐ https://stackoverflow.com/questions/56146735/visual-bug-when-changing-robinson-projections-central-meridian-with-ggplot2/56155662#56155662\n型の変換 地物の型の変換 st_cast()\nsf を data.frame に変換する sfオブジェクトをデータフレームに変換する（この時、df には geometry 列（リスト列=sfcオブジェクト）が残る）\ndata_df \u0026lt;- as.data.frame(data_sf) sf オブジェクトから geometory 列を削除すると、ただのデータフレームになる。そのために、st_set_geometry() はデータフレームにgeometry列をくっつける関数だけど、NULLを渡すとsfオブジェクトからgeomeotry列を削除できる。\ndf \u0026lt;- sf::st_set_geometry(data_sf, NULL) WKT形式に変換する lwgeom::st_astext(x, digits = options(\u0026quot;digits\u0026quot;), ..., EWKT = FALSE) lwgeom::st_asewkt(x, digits = options(\u0026quot;digits\u0026quot;)) 地物同士の位置関係の判定 バックエンドのライブラリを切り替える 新しいバージョンの sf パッケージでは地理情報処理のバックエンドとしてS2ライブラリを使用するようになった。S2ライブラリは処理が高速なので通常はそのままでよい。しかし、S2ライブラリは地球を球面として扱うが、平面として扱った処理をしたい場合はS2ライブラリを無効にすると良い。\n# sf 1.0 以降 s2 パッケージを使うようになった # s2を使わない以前の計算方法にしたいときは以下を実行する sf::sf_use_s2(FALSE) predicate whether x touches/contains/within/ y\nsparse=FALSE にすると論理値ベクトルを返す。\n地理情報処理関数 st_intersects(x, y, sparse = TRUE, ...)\nst_disjoint(x, y = x, sparse = TRUE, prepared = TRUE)\nst_touches(x, y, sparse = TRUE, prepared = TRUE)\nst_crosses(x, y, sparse = TRUE, prepared = TRUE)\nst_within(x, y, sparse = TRUE, prepared = TRUE)\nst_contains(x, y, sparse = TRUE, prepared = TRUE)\nst_contains_properly(x, y, sparse = TRUE, prepared = TRUE)\nst_overlaps(x, y, sparse = TRUE, prepared = TRUE)\nst_equals(x, y, sparse = TRUE, prepared = FALSE)\nst_covers(x, y, sparse = TRUE, prepared = TRUE)\nst_covered_by(x, y, sparse = TRUE, prepared = TRUE)\nst_equals_exact(x, y, par, sparse = TRUE, prepared = FALSE)\nst_is_within_distance(x, y, dist, sparse = TRUE)\nsf::st_wrap_dateline(options = c(\u0026quot;WRAPDATELINE=YES\u0026quot;, \u0026quot;DATELINEOFFSET=180\u0026quot;), quiet = FALSE)\nggplot2 プロット時に投影図法を指定する\nlibrary(sf) library(ggplot2) land \u0026lt;- read_sf(\u0026#34;data/ne_10m_land/ne_10m_land.shp\u0026#34;) land_crop \u0026lt;- st_crop(land, c(xmax=180, xmin=-180, ymin = -80, ymax=80)) ggplot(land_crop)+ geom_sf()+ #coord_sf(crs = sf::st_crs(\u0026#39;+proj=moll\u0026#39;)) # モルワイデ図法 coord_sf(crs = sf::st_crs(\u0026#39;+proj=wag6\u0026#39;)) # Wagner VI projection 30 day map challenge Twitter の #30DayMapChallenge で地理情報可視化の例がたくさんある\nR の sf \u0026amp; ggplot メインで挑戦した人がBookdownでまとめてくれている\nhttps://twitter.com/tjukanov/status/1187713840550744066\nhttps://rud.is/books/30-day-map-challenge/\nデータベースへの読み書き https://r-spatial.github.io/sf/articles/sf2.html\nNate の中心合わせ https://github.com/natemiller/mapping\ninvalidな図形を修正する sf::st_is_valid(target_sf, reason = TRUE) valid_sf \u0026lt;- sf::st_make_valid(target_sf) Shapefileとして保存する dir.create(\u0026#34;joint_fishries_area\u0026#34;) sf::write_sf(joint_fisheries_areas_with_values_sf, \u0026#34;joint_fishries_area/joint_fishries_area.shp\u0026#34;) 特定のエリア内の点を削除する points_dfから target_area_sf 内にある点を削除する。\ntarget_area_sf は 1つのエリア（1行）だけ含んでいるという前提\npoints_new_df \u0026lt;- points_df %\u0026gt;% sf::st_as_sf( coords = c(\u0026#34;lon\u0026#34;, \u0026#34;lat\u0026#34;), dim = \u0026#34;XY\u0026#34;, remove = FALSE, crs = 4326 ) %\u0026gt;% filter(!as.logical(sf::st_within(., target_area_sf, sparse = FALSE))) %\u0026gt;% sf::st_set_geometry(NULL) "},{idx:37,href:"/notebook/r/bigrquery/",title:"bigrquery",content:"bigrquery R から BigQuery を操作するためのパッケージ\nコネクションの作成 ds \u0026lt;- DBI::dbConnect( drv = bigrquery::bigquery(), project = \u0026quot;project_name\u0026quot;, dataset = \u0026quot;dataset_name\u0026quot;, use_legacy_sql = FALSE ) ds \u0026lt;- bq_dataset( project = \u0026quot;project_name\u0026quot;, dataset = \u0026quot;dataset_name\u0026quot;,) 大きなデータをダウンロードしようとするとエラーが起きる。以下を設定すると回避できる。最新版では修正されているかもしれない\noptions(scipen = 20) 自動で認証するためのアクセストークン https://gargle.r-lib.org/articles/non-interactive-auth.html\nbigrquery::bq_auth(path = \u0026quot;access_token.json\u0026quot;) データのダウンロード データのアップロード bq_tbl \u0026lt;- bigrquery::bq_table(\u0026#34;project\u0026#34;, \u0026#34;dataset\u0026#34;, \u0026#34;table_name\u0026#34;) job \u0026lt;- bigrquery::bq_table_upload(bq_tbl, values = data_df, quiet = FALSE) テーブルの作成・削除 bq_tbl \u0026lt;- bigrquery::bq_table(\u0026#34;project\u0026#34;, \u0026#34;dataset\u0026#34;, \u0026#34;table_name\u0026#34;) job \u0026lt;- bigrquery::bq_table_upload(bq_tbl, values = data_df, quiet = FALSE) bigrquery::bq_table_create(bq_tbl) bigrquery::bq_table_delete(bq_tbl) "},{idx:38,href:"/notebook/r/dplyr/",title:"dplyr",content:"dplyr データフレームに対する操作\n列の選択 select()\n列選択のヘルパー関数 これらの関数は select() の中で使う\nall_of(x) 変数 x に記録されたカラムを選択する\n行の選択: slice() 行番号を指定して抽出:slice() # 1,3,5行目を抽出 df %\u0026gt;% slice(c(1,3,5)) 先頭から n 行を抽出: slice_head() # 最初の3行を抽出 df %\u0026gt;% slice_head(n = 3) 最後から n 行を抽出: slice_tail() # 最後の2行を抽出 df %\u0026gt;% slice_tail(n = 2) ランダムにサンプル: slice_sample() # ランダムに3行を抽出 df %\u0026gt;% slice_sample(n = 3) # ランダムに全体の30%を抽出 df %\u0026gt;% slice_sample(prop = 0.3) 列の値で抽出: slice_min(), slice_max() # valが小さい方から2行を抽出 df_num %\u0026gt;% slice_min(val, n = 2) # valが大きいほうから2行を抽出 df_num %\u0026gt;% slice_max(val, n = 2) 各行の複製を作成: # 各行を3回ずつ複製 df %\u0026gt;% slice(rep(1:n(), each = 3)) 行の並べ替え: arrange() グループ化: group_by() 集計: summarize() レコード数のカウント: n(), tally(), count() 以下の3つは全て同じ結果を返す。カラム x の値ごとにレコード数をカウントする。\ndf %\u0026gt;% group_by(x) %\u0026gt;% summarize(n = n()) df %\u0026gt;% group_by(x) %\u0026gt;% tally() df %\u0026gt;% count(x) add_count() と add_tally() は、上の例の summarize() を mutate() に変えたもの、元の df の行数は変えずにグループごとのカウント列を追加する\ndf %\u0026gt;% group_by(x) %\u0026gt;% mutate(n = n()) df %\u0026gt;% group_by(x) %\u0026gt;% add_tally() df %\u0026gt;% add_count(x) "},{idx:39,href:"/notebook/r/forcats/",title:"forcats",content:"forcats forcats パッケージは factor 型を操作するためのパッケージ\nhttps://kazutan.github.io/kazutanR/forcats_test.html\n関数の一覧\nlibrary(forcats) ls(\u0026quot;package:forcats\u0026quot;) [1] \u0026quot;%\u0026gt;%\u0026quot; \u0026quot;as_factor\u0026quot; \u0026quot;fct_anon\u0026quot; [4] \u0026quot;fct_c\u0026quot; \u0026quot;fct_collapse\u0026quot; \u0026quot;fct_count\u0026quot; [7] \u0026quot;fct_cross\u0026quot; \u0026quot;fct_drop\u0026quot; \u0026quot;fct_expand\u0026quot; [10] \u0026quot;fct_explicit_na\u0026quot; \u0026quot;fct_infreq\u0026quot; \u0026quot;fct_inorder\u0026quot; [13] \u0026quot;fct_inseq\u0026quot; \u0026quot;fct_lump\u0026quot; \u0026quot;fct_lump_lowfreq\u0026quot; [16] \u0026quot;fct_lump_min\u0026quot; \u0026quot;fct_lump_n\u0026quot; \u0026quot;fct_lump_prop\u0026quot; [19] \u0026quot;fct_match\u0026quot; \u0026quot;fct_other\u0026quot; \u0026quot;fct_recode\u0026quot; [22] \u0026quot;fct_relabel\u0026quot; \u0026quot;fct_relevel\u0026quot; \u0026quot;fct_reorder\u0026quot; [25] \u0026quot;fct_reorder2\u0026quot; \u0026quot;fct_rev\u0026quot; \u0026quot;fct_shift\u0026quot; [28] \u0026quot;fct_shuffle\u0026quot; \u0026quot;fct_unify\u0026quot; \u0026quot;fct_unique\u0026quot; [31] \u0026quot;first2\u0026quot; \u0026quot;gss_cat\u0026quot; \u0026quot;last2\u0026quot; [34] \u0026quot;lvls_expand\u0026quot; \u0026quot;lvls_reorder\u0026quot; \u0026quot;lvls_revalue\u0026quot; [37] \u0026quot;lvls_union\u0026quot; レベルを並べ替える 既存の factor 型のレベルそのものは変更せずに、レベルの順序を変更する。可視化のときに便利\n# レベルが登場する順にならべかえ forcats::fct_inorder(x) # 各レベルの頻度順に並べ替え forcats::fct_infreq(x) # マニュアルで並べ替え forcats::fct_relevel(x, c(\u0026#34;level_C\u0026#34;, \u0026#34;level_A\u0026#34;, \u0026#34;level_B\u0026#34;)) # 別の変数 y の値を使って並べ替え # ... には na.rm = TRUE とか forcats::fct_reorder(x, y, fun = median, ..., .desc = FALSE) レベルをマージする # ある割合以下のレベルをOtherにまとめる forcats::fct_lump_prop(x, prop = 0.01)) # マニュアルで既存のレベルをまとめて新しいレベルを作る forcats::fct_collapse(gss_cat$partyid, new_level_1 = c(\u0026#34;Level_A\u0026#34;, \u0026#34;Level_B\u0026#34;), new_level_2 = \u0026#34;Level_C\u0026#34;, new_level_3 = c(\u0026#34;Level_D\u0026#34;, \u0026#34;Level_E\u0026#34;), ) 新しいレベルを追加する # 欠損値を明示的にレベルに加える forcats::fct_explicit_na(x, na_level = \u0026#34;Missing\u0026#34;) "},{idx:40,href:"/notebook/r/future/",title:"future",content:"future future パッケージは１つのPC内での並列計算、および、複数PCをまたいだ分散計算を実行するためのパッケージ\n"},{idx:41,href:"/notebook/r/gganimate/",title:"gganimate",content:"gganimate transition_manual() コマ撮りアニメのように、１枚１枚のコマが切り替わるようなアニメーション\ntransition_manual(frames, ..., cumulative = FALSE)  frames でコマ分けに使いたいカラムを指定する。 cumulative = TRUE なら前のコマに重ねて次のコマを描画する  また、次の変数が定義される\n previous_frame 前のフレームの値 current_frame 現在のフレームの値 next_frame 次のフレームの値  使用例\nanim \u0026lt;- ggplot(mtcars, aes(factor(gear), mpg)) + geom_boxplot() + transition_manual(gear) + ggtitle('Now showing {current_frame}') transition_states() コマとコマの間を補完する画像（データ）を生成する\ntransition_states(states, transition_length = 1, state_length = 1, wrap = TRUE)  states ベースとなるコマ分けに使用するカラム transition_length ベースとなるコマとコマの間で、補完されるコマを表示する相対的な長さ state_length ベースとなるコマを表示する相対的な長さ wrap アニメーションを循環させるか、TRUE なら最後のコマは最初のコマに遷移する  "},{idx:42,href:"/notebook/r/graph/",title:"graph",content:"グラフ関連パッケージ igraph, tidygraph, sfnetworks の3つのグラフ関連パッケージをまとめて解説\n igraph : 低レベルの基本的グラフ構造とアルゴリズムを定義 igraph クラス tidygraph : igraphをtidyverseで扱いやすくするラッパー tbl_graph クラス（nodeとedgeを格納した2つのテーブルを合体したオブジェクト） sfnetworks : tidygraph と sf の合体、位置情報を持ったノードとエッジを使える  igraph グラフのデータ構造とグラフアルゴリズムを定義しているパッケージ。tidygraph, sfnetworksの基盤となっている。\nigraphでは：頂点は vertices と表記される\nグラフの作成 \u0026amp; 属性の追加 # 3ノードの輪グラフ g \u0026lt;- igraph::make_ring(3) # 頂点と辺を確認 #0 列 3 行のデータフレーム # 行番号は頂点のインデックスになっている igraph::as_data_frame(g, what = \u0026#34;vertices\u0026#34;) # from to に始点終点のノード番号が格納されている igraph::as_data_frame(g, what = \u0026#34;edges\u0026#34;) # from to # 1 1 2 # 2 2 3 # 3 1 3 #頂点に属性nameを付ける igraph::V(g)$name \u0026lt;- c(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;) # name # A A # B B # C C #エッジに属性weightを付ける igraph::E(g)$weight \u0026lt;- c(10, 20, 30) # from to weight # 1 A B 10 # 2 B C 20 # 3 A C 30 # グラフの頂点に任意の属性を追加して igraph::V(g)$hoge \u0026lt;- \u0026#34;hoge\u0026#34; igraph::E(g)$fuga \u0026lt;- c(\u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;) # データフレームに変換してそれを確認 igraph::as_data_frame(g, what = \u0026#34;vertices\u0026#34;) # name hoge # A A hoge # B B hoge # C C hoge igraph::as_data_frame(g, what = \u0026#34;edges\u0026#34;) # from to weight fuga # 1 A B 10 A # 2 B C 20 B # 3 A C 30 C グラフ要素へのアクセス # 頂点名 igraph::V(g)$name # [1] \u0026#34;A\u0026#34; \u0026#34;B\u0026#34; \u0026#34;C\u0026#34; # エッジの重み igraph::E(g)$weight # [1] 10 20 30 ノードやエッジをデータフレームに変換する # 頂点とエッジ一覧 igraph::as_data_frame(g, what = \u0026#34;vertices\u0026#34;) igraph::as_data_frame(g, what = \u0026#34;edges\u0026#34;)  sfnetworks R から BigQuery を操作するためのパッケージ\nコネクションの作成 "},{idx:43,href:"/notebook/r/lubridate/",title:"lubridate",content:"lubridate 日付を扱うパッケージ\n文字列から　Date, POSIXct オブジェクトの生成 ymd(..., quiet = FALSE, tz = NULL, locale = Sys.getlocale(\u0026quot;LC_TIME\u0026quot;), truncated = 0) ymd_hms(..., quiet = FALSE, tz = \u0026quot;UTC\u0026quot;, locale = Sys.getlocale(\u0026quot;LC_TIME\u0026quot;), truncated = 0) ymd_hm(..., quiet = FALSE, tz = \u0026quot;UTC\u0026quot;, locale = Sys.getlocale(\u0026quot;LC_TIME\u0026quot;), truncated = 0) ymd_h(..., quiet = FALSE, tz = \u0026quot;UTC\u0026quot;, locale = Sys.getlocale(\u0026quot;LC_TIME\u0026quot;), truncated = 0) タイムゾーン tz() tz(x) tz(x) \u0026lt;- value 日付・時刻オブジェクト x について設定されたタイムゾーンを出力する。\n日付・時刻オブジェクト x のタイムゾーンを変更する。この結果、下の例のように「xの絶対的な時刻が変化する」ことに注意する。\n\u0026ldquo;2016-04-19 16:49:00 UTC\u0026rdquo; → \u0026ldquo;2016-04-19 16:49:00 JST\u0026rdquo;\nx \u0026lt;- ymd_hms(\u0026#34;2016-04-19 16:49:00\u0026#34;, tz=\u0026#34;UTC\u0026#34;) print(x) # [1] \u0026#34;2016-04-19 16:49:00 UTC\u0026#34; tz(x) \u0026lt;- \u0026#34;Japan\u0026#34; # [1] \u0026#34;2016-04-19 16:49:00 JST\u0026#34; with_tz() with_tz(time, tzone = \u0026quot;\u0026quot;) time で指定された特定の絶対時刻について、tzone で指定したタイムゾーンでのローカル時刻を表示する。\nこの場合、time 元の絶対的な時刻は変化しない\nx \u0026lt;- ymd_hms(\u0026#34;2016-04-19 16:49:00\u0026#34;, tz=\u0026#34;UTC\u0026#34;) print(x) # [1] \u0026#34;2016-04-19 16:49:00 UTC\u0026#34; with_tz(x, tz = \u0026#34;Japan\u0026#34;) # [1] \u0026#34;2016-04-20 01:49:00 JST\u0026#34; 年間通算日 : day of the year date を年間通算日　(day of the year)（1年のうち何番目の日であるか） に変換する。\nlubridate::yday() "},{idx:44,href:"/notebook/r/purrr/",title:"purrr",content:"purrr ベクトルやリストなどの各要素に対する繰り返し処理\n基本関数 map_*\nmap2_*\nreduce()\naccumulate\npmap purrr::pmap(.l, .f, \u0026hellip;)\n3つ以上の引数を受け取る関数を適用する\n渡すリスト（データフレームでもいい）の(.l)の要素名と、適用する関数 (.f) の引数名を一致させる必要がある。\npmap(data.frame(a = 1:3, b = 4:6), function(a, b) {a * b}) purrr::pluck() リストの要素へのアクセス [[]]と同等 # リストLの1番目の要素 L[[1]] と同等 # pluckの方が %\u0026gt;% のチェーンの中で使いやすい pluck(L, 1) その他関数 エラー処理 plyr::failwith() purrr::possibly pmap\npartial\nデータフレームの各行に対する繰り返し furrr future::plan(future::multiprocess(), workers = 10L) result \u0026lt;-furrr::future_map() list を vector に変換する purrr::flatten_*() 例えば、purrr::map()で返ってくる値が、文字列ベクトルを要素として持つ List である場合に、リストの要素を１つの文字列ベクトルとしてまとめたいことがある。\n# ベクトルを要素として持つリスト `.x` をベクトルに変換する。 purrr::flatten_lgl(.x) purrr::flatten_int(.x) purrr::flatten_dbl(.x) purrr::flatten_chr(.x) purrr::flatten_raw(.x) # 階層性のあるlistを一段階解消する。返値はリスト purrr::flatten(.x) # リストの要素をデータフレームに結合して返す purrr::flatten_dfr(.x, .id = NULL) purrr::flatten_dfc(.x) "},{idx:45,href:"/notebook/r/package_development/",title:"R Package Development",content:"Rパッケージ開発 パッケージにデータを含める  data/  ユーザーもアクセスできる。例データなど   R/sysdata.rda に含める  ユーザーはアクセスできない。関数内で使用するデータなど   生データを保存したい場合？  inst/extdata におく    data/  データは .RData ファイルとして保存する。 save() を使う。 ファイルには１つのオブジェクトだけを含める。 ファイル名は保存されたオブジェクト名と一致する必要がある。  devtools::use_data() を使うと上のルールに従ったデータを簡単に作成できる。\n他のデータ形式でもいいらしい\nDESCRIPTION ファイルに　LazyData: true の記述があるとデータは、実際に使用される時までRにロードされることはない。\nR/sysdata.rda sysdata.rda に保存されたオブジェクトはユーザーからは見えない。\ndevtools::use_data(internal = TRUE) を使う。\n"},{idx:46,href:"/notebook/r/r_python/",title:"R_Python",content:"RとPython  Rとnumpy dplyr と Pandas  "},{idx:47,href:"/notebook/r/reticulate/",title:"reticulate",content:"reticulate R から Python を呼び出すためのパッケージ\n使用する Python を指定する .Rprofile で RETICULATE_PYTHON 環境変数に、使用するPythonを指定するのが安全\nSys.setenv(RETICULATE_PYTHON = '/usr/local/bin/python3') pipenv で Python 環境を管理している場合\nPipenv で作成した環境を利用する R のプロジェクトフォルダに pipenv の環境が作成されている前提\nカレントフォルダはR のプロジェクトフォルダにいる前提\nPipenv で作成した環境を reticulate で利用する。\nvenv \u0026lt;- system(\u0026#34;pipenv --venv\u0026#34;, inter = TRUE) reticulate::use_virtualenv(venv, required = TRUE) reticulate::py_config() Windows の場合は venv のパスの文字列をRの書式に合わせるために変換をかませる\nvenv \u0026lt;- system(\u0026#34;pipenv --venv\u0026#34;, inter = TRUE) venv \u0026lt;- stringr::str_replace_all(stringr::str_sub(venv, 1, -2), \u0026#34;\\\\\\\\\u0026#34;, \u0026#34;/\u0026#34;) reticulate::use_virtualenv(venv, required = TRUE) reticulate::py_config() reticulate で使用されているPython環境を確認する reticulate::py_config() Python コンソールを確認する reticulate::repl_python() Pythonコンソールから出たいときは exit を入力する。\nR から Python のオブジェクトにアクセスする reticulate::py を通して Python のオブジェクトにアクセスできる。\nx = \u0026#34;hoge\u0026#34; # R reticulate::py$x "},{idx:48,href:"/notebook/r/rmarkdown/",title:"Rmarkdown",content:"Rmarkdown Rmarkdown (.Rmd) から、html などに出力するパッケージは２つある\nknitr と rmarkdown、rmarkdown は内部で knitr を使用している。rmarkdown は knitr の後継となるべく開発されているのかも知れない。\nドキュメント  公式: R Markdown: The Definitive Guide rmarkdownパッケージで楽々ドキュメント生成  用語  chunk YAMLヘッダ  出力形式の指定 公式 2.4 Output formats\n.Rmd ファイルの先頭の YAML ヘッダの output: セクションに記述する\n--- title: \u0026quot;Title of this document\u0026quot; output: rmarkdown::github_document --- output: で指定できるオプションは以下の通り\n html_document github_document pdf_document word_document latex_document md_document odt_document rtf_document context_document powerpoint_presentation beamer_presentation ioslides_presentation slidy_presentation  出力先の変更 デフォルトでは .Rmd ファイルと同じフォルダに出力される。\nrmarkdown::render() を使って出力する場合には、以下のようにする。\nrmarkdown::render('my.Rmd', output_file = 'folder/my.html') RStudio の Knit ボタンを押したときの出力先を指定するには .Rmd ファイルの先頭の YAML ヘッダの knit: セクションに以下のように output_dir を指定する。パスの指定は .Rmd ファイルからの相対パス。\n--- title: \u0026quot;Title of this document\u0026quot; output: rmarkdown::github_document knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_dir = \u0026quot;../share/markdown\u0026quot;) }) --- chunk options {r, fig.height=5}\n図のサイズ fig.height=5 5インチ\n eval=TRUE : このchunkを実行するかどうか echo=TRUE : chunkのコードを出力に含めるか results='hide' : \u0026lsquo;hide\u0026rsquo; ならテキスト出力が隠される。 \u0026lsquo;asis\u0026rsquo; ならそのまま出力する collapse=TRUE : TRUEならRのコードとテキスト出力をまとめて出力する。デフォルトはFALSE warning=TRUE, message=TRUE, error=TRUE : これらを出力するかどうか include=FALSE : コードやテキスト出力を隠す。echo = FALSE, results = 'hide', warning = FALSE, message = FALSE と同等 cache=TRUE : TRUEにしておくと、chunkを変更していない場合は、次回Rmarkdownをレンダリングするときには、この chunk の実行を省略する。 cache.lazy=FALSE : cache=TRUE でRのオブジェクトを保存しておいたRDSファイルを読込んだらエラーになった。cache.lazy=FALSE をセットしたら正常に実行された。 fig.width=1, fig.height=2, fig.dim = c(1, 2) : 図のサイズ（インチ） out.width='80%', out.height : 図のサイズ、出力ドキュメントの幅に応じて決める fig.align='center' : 図の配置 \u0026lsquo;left\u0026rsquo;, \u0026lsquo;center\u0026rsquo;, or \u0026lsquo;right\u0026rsquo;. dev='png' : 画像デバイス \u0026lsquo;pdf\u0026rsquo; \u0026lsquo;png\u0026rsquo; \u0026lsquo;svg\u0026rsquo; \u0026lsquo;jpeg\u0026rsquo; fig.cap : 図のキャプション child='path/to/other/document' : 外部ファイルの埋め込み  ドキュメント全体で共通の chunk option を設定する\n```{r, setup, include=FALSE} knitr::opts_chunk$set(fig.width = 8, collapse = TRUE)  chunk option の中で、Rの式を評価できるので、事前の実行結果を利用して、以降のchunkを実行するかどうかを制御することもできる。 x \u0026lt;- TRUE y = rnorm(100)  # 既存の画像ファイルを埋め込む knitr::include_graphics('images/hex-rmarkdown.png')  # テーブルを埋め込む knitr::kable(iris[1:5, ], caption = 'A caption')  高度なテーブル操作は `kableExtra` パッケージを使う せめて自分が接する子供たちにとって良き機会となれる大人でありたいものです。 "},{idx:49,href:"/notebook/r/rpart/",title:"rpart",content:"rpart 決定木\nrpart.object\nhttps://rdrr.io/cran/rpart/man/rpart.object.html\nrpart() による学習結果のオブジェクト\nrpart.object の要素\n frame: データフレーム、tree を表現する  var: 分割に使用される変数、\u0026lt;leaf\u0026gt; は末端ノード n wt dev yval complexity ncompete nsurrogate yval2   where: 整数ベクトル、訓練データのレコード数と同じ長さ。訓練データの各レコードがどのリーフに落ちたかを表す。値は frame の行番号 call: このオブジェクトを作成するときに記述されたRコードのイメージ terms: terms.object フォーミュラをサマライズしたもの、ユーザーは基本使わない splits: 分割を記述する matrix。列は、\u0026ldquo;count\u0026rdquo; \u0026ldquo;ncat\u0026rdquo; \u0026ldquo;improve\u0026rdquo; \u0026ldquo;index\u0026rdquo; \u0026ldquo;adj\u0026rdquo;、各行は分割に使われる変数名 csplit: 整数行列、少なくとも１つの分割変数が factor である場合に作成される method: 文字列、(\u0026ldquo;class\u0026rdquo;, \u0026ldquo;exp\u0026rdquo;, \u0026ldquo;poisson\u0026rdquo;, \u0026ldquo;anova\u0026rdquo; or \u0026ldquo;user\u0026quot;のうちのどれか)、この tree を作成するときに使われた方法 cptable: 数値行列、complexityパラメタに基づいて決定された最適な枝刈りの情報？ 列名 \u0026ldquo;CP\u0026rdquo; \u0026ldquo;nsplit\u0026rdquo; \u0026ldquo;rel error\u0026rdquo; \u0026ldquo;xerror\u0026rdquo; \u0026ldquo;xstd\u0026rdquo; variable.importance: 名前付きベクトル、変数重要度、 numresp: 整数スカラー、目的変数の値の数、factorのレベルの数 parms: 学習時に与えられたパラメタの値 control: 学習時に与えられたパラメタの値 functions: rpartオブジェクトのMethodとして使われる関数 summary(), print() and text() ordered: 名前付き論理ベクトル、要素名は変数名、値はその変数が順序付きfactorであるかを表す na.action: stats::model.frame から返される値、NA の取り扱いを決める  rpart.object の属性 attributes()\n xlevels: 説明変数にある factor 型のレベル ylevels: 目的変数の factor 型のレベル  rpart:::predict.rpart() rpart.object を使って predict() したときに呼び出されるメソッド\nhttps://www.rdocumentation.org/packages/rpart/versions/4.1-15/topics/predict.rpart\npredict(object, newdata, type = c(\u0026quot;vector\u0026quot;, \u0026quot;prob\u0026quot;, \u0026quot;class\u0026quot;, \u0026quot;matrix\u0026quot;), na.action = na.pass, …) type: 分類ではデフォルトは prob クラス確率, class なら予測された目的変数 (facor) の値、vector なら目的変数 factor の levels 属性の要素番号、回帰の場合は デフォルトは vector で目的変数の値（多分）\ntyppe=\u0026quot;vector\u0026quot;　の時は、rpart.object$frame$yval の値が返されるらしい（各データが落ちたリーフの yval の値） これを使ってリーフ番号を取得できる\nrpart関係の別のライブラリ  rpart.plot itree: rpartの拡張らしい、rpartの作者もかかわっている treeClust  rpart.plot::rpart.predict() 新しいデータに対して予測する\nrpart::predict.rpart() と同じだが、予測値のノード番号とルールを出力できる\nrpart.plot::rpart.predict(object, newdata, type = c(\u0026quot;vector\u0026quot;, \u0026quot;prob\u0026quot;, \u0026quot;class\u0026quot;, \u0026quot;matrix\u0026quot;), na.action = na.pass, nn=FALSE, rules=FALSE, ...)  nn: TRUE なら ノード番号の列も返す rules: TRUE なら、ルールを文字列で記述した列も返す ...: rpart.rules() に渡される引数、例えば clip.facs=TRUE  rpart.plot::rpart.rules rpart.object から 各リーフに至るルールを表示する\nrpart.rules(x = stop(\u0026quot;no 'x' argument\u0026quot;), style = \u0026quot;wide\u0026quot;, cover = FALSE, nn = FALSE, roundint = TRUE, clip.facs = FALSE, varorder = NULL, ...) しかし、リーフの最大値は1000に制限されているので、それ以上に複雑なtreeのルールを生成したいときは、パッケージをいじる必要がある\nrpart.plot/R/rpart.rules.R の中の maxrules \u0026lt;- 1e3 をもっと大きい値に書き換える\nそして、ローカルのソースからインストールする install.packages(\u0026quot;./rpart.plot/\u0026quot;, type=\u0026quot;source\u0026quot;, repos = NULL)\nstyle = \u0026ldquo;tall\u0026rdquo;\n出力される値（class c(\u0026quot;rpart.rules\u0026quot;, \u0026quot;data.frame\u0026quot;)）\n target : 分類：クラス確率の文字列  行名: おそらく rpart.object$frame の行名に対応する？\ntreeClust::rpart.predict.leaves() rpart.predict.leaves(rp, newdata, type = \u0026quot;where\u0026quot;)  type  \u0026quot;where\u0026quot; : \u0026ldquo;rpart.object\u0026rdquo; の要素 frame の行番号を返す \u0026quot;leaf\u0026quot; : 実際のリーフ番号（frame の行名）を返す    "},{idx:50,href:"/notebook/r/rstudio/",title:"RStudio",content:"RStudio インストール WSL の Ubuntu 18.04 にインストール Ubuntuの場合と同じ、事前に Rをインストールしておく\nsudo apt-get install gdebi-core wget https://download2.rstudio.org/server/bionic/amd64/rstudio-server-1.3.959-amd64.deb sudo gdebi rstudio-server-1.3.959-amd64.deb しかし、次のようなエラーが出た\n$sudo gdebi rstudio-server-1.3.959-amd64.deb Traceback (most recent call last): File \u0026quot;/usr/bin/gdebi\u0026quot;, line 38, in \u0026lt;module\u0026gt; from GDebi.GDebiCli import GDebiCli File \u0026quot;/usr/share/gdebi/GDebi/GDebiCli.py\u0026quot;, line 103 def get_dependencies_info(self): ロケールを設定して再度インストールして解決\nexport LC_ALL=en_US.UTF-8 RStudio Server の起動 sudo service rstudio-server start その後、ブラウザから http://localhost:8787/ にアクセスする\nおまけ\nWSLにインストールしたRStudio Serverの起動を楽にする\nRStudio Server のアップデート RStudio Server のパージョンを新しくする\n実行中のRセッションの確認\nsudo rstudio-server active-sessions 実行中のRセッションの停止\nsudo rstudio-server suspend-all オフラインモードにする\nsudo rstudio-server offline 新しいバージョンのRStudio Serverをインストール\nDownload RStudio Server\nsudo gdebi \u0026lt;rstudio-server-package.deb\u0026gt; # or sudo yum install --nogpgcheck \u0026lt;rstudio-server-package.rpm\u0026gt; アップデートしたサーバーを再起動\nsudo rstudio-server restart sudo rstudio-server online "},{idx:51,href:"/notebook/r/rstudio_server/",title:"Rstudio server",content:"Rstudio server プラウザから使えるRStudio\n起動再起動\nsudo rstudio-server start sudo rstudio-server stop sudo rstudio-server restart \u0026gt; rstudio-server --help Usage: rstudio-server {status|start|stop|restart|test-config|verify-installation|suspend-session|suspend-all|force-suspend-session|force-suspend-all|kill-session|kill-all|offline|online|active-sessions|version} "},{idx:52,href:"/notebook/r/rvest/",title:"rvest",content:"rvest スクレイピングするためのパッケージ\n取り出したい要素の確認 スクレイピングしたいウェブページをブラウザ（Chrome）で開く → F12 を押すと、デベロッパーツールが開く → Elements タブの中でソースコードの一部をマウスでポイントするとウェブページの対応する部分の色が変わる → 取り出したい要素のソースの場所がわかったら、右クリック → Copy \u0026gt; Copy to XPath\nでその要素へのXPathが取得できる\n# ウェブページのソースを取得 html \u0026lt;- rvest::read_html(\u0026#34;https://www.npfc.int/vessels/1536\u0026#34;) # xpath を使って特定の要素をテキストとして取り出す html_node(html, xpath = \u0026#34;/html/body/div/div/div/div/div/div/div/section/div/div[3]/div/div[2]\u0026#34; ) %\u0026gt;% html_text2() # その要素がテーブルである場合にデータフレームとして取り出す html_node(html, xpath = \u0026#34;/html/body/div/div/div/div/div/div/div/section/div/div[3]/div/div[4]/div[1]\u0026#34; ) %\u0026gt;% html_table() #page \u0026gt; div \u0026gt; div \u0026gt; div \u0026gt; div \u0026gt; section \u0026gt; div \u0026gt; div.region.region-content \u0026gt; div \u0026gt; div.col-sm-6.bs-region.bs-region\u0026ndash;top-left \u0026gt; div.field.field\u0026ndash;name-vty-id.field\u0026ndash;type-entity-reference.field\u0026ndash;label-above \u0026gt; div.field__items document.querySelector(\u0026quot;#page \u0026gt; div \u0026gt; div \u0026gt; div \u0026gt; div \u0026gt; section \u0026gt; div \u0026gt; div.region.region-content \u0026gt; div \u0026gt; div.col-sm-6.bs-region.bs-region\u0026ndash;top-left \u0026gt; div.field.field\u0026ndash;name-vty-id.field\u0026ndash;type-entity-reference.field\u0026ndash;label-above \u0026gt; div.field__items\u0026quot;)\n/html/body/div[2]/div[1]/div/div/div/div/div/div/section/div/div[3]/div/div[2]/div[6]/div[2]\n/html/body/div[2]/div[1]/div/div/div/div/div/div/section/div/div[3]/div/div[2]/div[6]/div[1]\n/html/body/div[2]/div[1]/div/div/div/div/div/div/section/div/div[3]/div/div[2]/div[6]/div[2]\n"},{idx:53,href:"/notebook/r/stars/",title:"stars",content:"stars 時空間のハイパーキューブを扱うためのパッケージらしい\nstars\nsfオブジェクトをラスター化する\nst_rasterize(sf, template = st_as_stars(st_bbox(sf), values = NA_real_, ...), file = tempfile(), driver = \u0026quot;GTiff\u0026quot;, options = character(0), ...)   template : ラスターのグリッドを指定する\n st_as_stars(.x = st_bbox(sf), nx = 4064, ny = 3232,values = NA_real_) .x にはバウンディングボックスを指定する nx, ny には縦横のグリッドの数 value は欠測値を埋める    SELECT date ,id_eog ,id_fra ,lon ,lat ,rad ,Lon_DNB ,Lat_DNB ,Rad_DNB ,distance ,IF(distance \u0026lt; 100, 1, 0) AS match_flg FROM scratch_masaki.eog_fra_distance_info_ecs\n "},{idx:54,href:"/notebook/r/stringr/",title:"stringr",content:"stringr 文字列を操作するパッケージ\n正規表現 パターンを含む要素の抽出 str_detect(string, pattern, negate = FALSE)\n文字列ベクトル string を渡して、 pattern と一致する文字列に対して TRUE を返す。negate = FALSE なら反転した結果を返す。\nstr_starts(string, pattern, negate = FALSE)\nstr_ends(string, pattern, negate = FALSE)\n文字列 string の先頭または末端に pattern と一致する文字列があるなら TRUE を返す\nNo worries.\nI did score calibration, and value of calibrated score is halved in the areas where footprints overlap.\nSo I could estimate the number of AIS vessels that should be detected by VIIRS in the Squid Fishing Area by summing up all the calibrated scores in the area.\nCorrently I am trying to compare it with the number of VIIRS-AIS matching result.\n"},{idx:55,href:"/notebook/r/tidymodels/",title:"tidymodels",content:"tidymodels tidymodels は統一的なインターフェースでモデリングを行うための、いくつかのパッケージをまとめたもの。\n rsample ：訓練データ/テストデータの分割 recipe ：データの前処理（変数変換、サンプリングなど） parsnip ：モデリング yardstic ：モデルの精度評価  データの分割 : rsample パッケージ Train/Test 分割 : initial_split() set.seed(10) dm_split \u0026lt;- rsample::initial_split(data, prop = 3/4, strata = NULL, breaks = 4, ...) # 時系列データの分割の場合 # dm_split \u0026lt;- initial_time_split(data, prop = 3/4, lag = 0, ...) # 訓練データとテストデータのと取り出し train_df \u0026lt;- rsample::training(dm_split) test_df \u0026lt;- rsample::testing(dm_split)  prop : 訓練データの割合 strata : 層化サンプリングに使用する変数を指定する。（ここで指定した変数の値の頻度分布は、訓練データとテストデータで同じになる） breaks : 整数スカラー。層化サンプリングしたい変数が連続変数の時に、連続地をいくつのビンに分けるか指定する。 lag : テストと訓練の間にラグを含めるための値。これはラグのある予測変数が使用される場合に便利です。  交差検証のための分割 : vfold_cv() K交差検証\nrsample::vfold_cv(data, v = 10, repeats = 1, strata = NULL, breaks = 4, ...)  data : データフレーム v : CVの分割数 (fold数) repeats : CVの分割を何回繰り返すか。総計算回数は v*repeats になる。 strata : 層化サンプリング。CV分割の時に、fold間で割合を維持したい変数を指定する（例えば目的変数がカテゴリ変数の時に指定する） breaks : 整数スカラー。層化サンプリングしたい変数が連続変数の時に、連続地をいくつのビンに分けるか指定する。  グループK交差検証\nrsample::group_vfold_cv(data, group = NULL, v = NULL, ...)  data : データフレーム group : グルーピングに使いたい変数名１つ v : データを分割したい数、 NULL の場合はグルーピング変数のユニークな値の数になる  Foldのデータを参照する 各Foldのデータを取り出すには以下の関数を使用する\n analysis() : 訓練データを参照する assessment() : 検証データを参照する  vfold_cv() 関数の出力値は vfold_cv オブジェクト、これは実際にはネストされたデータフレームである。\ndf_cv \u0026lt;- rsample::vfold_cv(data, v = 10, repeats = 1) \u0026gt; class(df_cv) [1] \u0026#34;vfold_cv\u0026#34; \u0026#34;rset\u0026#34; \u0026#34;tbl_df\u0026#34; \u0026#34;tbl\u0026#34; \u0026#34;data.frame\u0026#34; ある１つの訓練・検証データのペアを取り出す\ntrain \u0026lt;- rsample::analysis(df_cv$splits[[1]]) validation \u0026lt;- rsample::assessment(df_cv$splits[[1]]) リピートなしの分割\n\u0026gt; df_cv \u0026lt;- rsample::vfold_cv(dm_info_train_df, strata = \u0026quot;month\u0026quot;, v = 2, repeats = 1) \u0026gt; df_cv # 2-fold cross-validation using stratification # A tibble: 2 x 2 splits id \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; 1 \u0026lt;rsplit [363/364]\u0026gt; Fold1 2 \u0026lt;rsplit [364/363]\u0026gt; Fold2 リピートありの分割\n\u0026gt; df_cv \u0026lt;- rsample::vfold_cv(dm_info_train_df, strata = \u0026quot;month\u0026quot;, v = 2, repeats = 2) \u0026gt; df_cv # 2-fold cross-validation repeated 2 times using stratification # A tibble: 4 x 3 splits id id2 \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; 1 \u0026lt;rsplit [363/364]\u0026gt; Repeat1 Fold1 2 \u0026lt;rsplit [364/363]\u0026gt; Repeat1 Fold2 3 \u0026lt;rsplit [363/364]\u0026gt; Repeat2 Fold1 4 \u0026lt;rsplit [364/363]\u0026gt; Repeat2 Fold2 前処理 : recipes パッケージ 訓練や新規データへのモデルの適用の前に実施する変数変換やデータサンプリングの手順をレシピオブジェクトとして定義する。\nrecipes パッケージで実施する前処理とは、特徴量作成というよりも、対数変換や標準化などデータの本質的な意味は変えないが、アルゴリズムの学習を適切に行えるようにするための処理のことを指す。\nそのため、この前処理は特徴量作成の後モデリングの直前に実施する。\nレシピオブジェクトの作成 : recipe() 最初に、変数加工のためのパラメタ（標準化のための平均と分散など）を決めるために使う訓練データ（一般には訓練データ）\nそのデータに含まれる目的変数と説明変数をフォーミュラとして与える。\nモデル式（目的変数と説明変数）を指定\nrecipes::recipe(x, formula = NULL, ..., vars = NULL, roles = NULL)  x, data : 変数加工のためのパラメタ（標準化のための平均と分散など）を決めるために使う訓練データ formula : モデル式、目的変数と説明変数の指定。log(x) とか x:y などの関数を含めてはいけない。マイナス符号をつけるのもダメ。 vars : 変数名を格納した文字列ベクトル roles : vars と同じ長さの文字列ベクトル。各変数の役割を指定する。\u0026quot;outcome\u0026quot;, \u0026quot;predictor\u0026quot;, \u0026quot;case_weight\u0026quot;, or \u0026quot;ID\u0026quot; 他の値でもいい  mod_rec \u0026lt;- df_rental %\u0026gt;% recipes::recipe(formula = Y ~ Area) 各前処理ステップの定義 : step_*() データに対する加工は step_*() 関数を使う\ndplyr\n step_filter	Filter rows using dplyr step_arrange	Sort rows using dplyr step_mutate	Add new variables using \u0026lsquo;mutate\u0026rsquo; step_mutate_at	Mutate multiple columns step_rename	Rename variables by name step_rename_at	Rename multiple columns step_slice	Filter rows by position using dplyr  数値データ変換\n step_center	Centering numeric data step_cut	Cut a numeric variable into a factor step_discretize	Discretize Numeric Variables step_normalize	Center and scale numeric data step_log	Logarithmic Transformation step_logit	Logit Transformation step_BoxCox	Box-Cox Transformation for Non-Negative Data step_hyperbolic	Hyperbolic Transformations step_inverse	Inverse Transformation step_invlogit	Inverse Logit Transformation step_range	Scaling Numeric Data to a Specific Range step_relu	Apply (Smoothed) Rectified Linear Transformation step_scale	Scaling Numeric Data step_sqrt	Square Root Transformation step_YeoJohnson	Yeo-Johnson Transformation  カテゴリ変数 factor\n step_factor2string	Convert Factors to Strings step_integer	Convert values to predefined integers step_bin2factor	Create a Factors from A Dummy Variable step_novel	Simple Value Assignments for Novel Factor Levels step_num2factor	Convert Numbers to Factors step_other	Collapse Some Categorical Levels step_ordinalscore	Convert Ordinal Factors to Numeric Scores step_relevel	Relevel factors to a desired level step_string2factor	Convert Strings to Factors step_unknown	Assign missing categories to \u0026ldquo;unknown\u0026rdquo; step_unorder	Convert Ordered Factors to Unordered Factors  変数取捨選択\n  step_shuffle	Shuffle Variables\n  step_rm	General Variable Filter\n  step_zv	Zero Variance Filter\n  step_nzv	Near-Zero Variance Filter\n  step_corr	High Correlation Filter\n  step_lincomb	Linear Combination Filter\n  #変数追加\n step_count	Create Counts of Patterns using Regular Expressions step_ratio	Ratio Variable Creation  時系列\n step_date	Date Feature Generator step_holiday	Holiday Feature Generator step_lag	Create a lagged predictor  サンプリング\n step_downsample	Down-Sample a Data Set Based on a Factor Variable step_sample	Sample rows using dplyr step_upsample	Up-Sample a Data Set Based on a Factor Variable  統計モデリング\n step_dummy	Dummy Variables Creation step_interact	Create Interaction Variables step_intercept	Add intercept (or constant) column step_regex	Create Dummy Variables using Regular Expressions  地理情報\n step_geodist	Distance between two locations step_spatialsign	Spatial Sign Preprocessing  欠損値\n step_naomit	Remove observations with missing values  欠損値補完 : imputation\n step_knnimpute	Imputation via K-Nearest Neighbors step_lowerimpute	Impute Numeric Data Below the Threshold of Measurement step_meanimpute	Impute Numeric Data Using the Mean step_medianimpute	Impute Numeric Data Using the Median step_modeimpute	Impute Nominal Data Using the Most Common Value step_impute_linear	Imputation of numeric variables via a linear model. step_bagimpute	Imputation via Bagged Trees step_rollimpute	Impute Numeric Data Using a Rolling Window Statistic  抽出？ : Extraction\n step_ica	ICA Signal Extraction step_kpca	Kernel PCA Signal Extraction step_kpca_poly	Polynomial Kernel PCA Signal Extraction step_kpca_rbf	Radial Basis Function Kernel PCA Signal Extraction step_nnmf	NNMF Signal Extraction step_pca	PCA Signal Extraction step_pls	Partial Least Squares Feature Extraction  その他\n  step_bs	B-Spline Basis Functions\n  step_ns	Natural Spline Basis Functions\n  step_poly	Orthogonal Polynomial Basis Functions\n  step_window	Moving Window Functions\n  step_classdist	Distances to Class Centroids\n  step_depth	Data Depths\n  step_isomap	Isomap Embedding\n  step_profile	Create a Profiling Version of a Data Set\n  サンプリング・ステップ # サンプリングを含むレシピを作成 (prep()しない) sampling_recipe \u0026lt;- recipes::recipe(Y ~ ., data = train_df) %\u0026gt;% # 目的変数 Y の値に基づいてサンプリングする # 訓練データに対してはサンプリングするが # テストデータや新規データに対してはサンプリングしない場合は skip = TRUE を指定する。 # この時点でサンプリングのための乱数 seed は固定される themis::step_downsample(Y, under_ratio = 0.5, skip = TRUE) %\u0026gt;% # prep() を実行すると skip = TRUE を指定したステップも実行される # retain = TRUE で、最初に渡した train_df にレシピが適用されたデータを # レシピオブジェクトの中に保持する recipes::prep(retain = TRUE) # サンプリングが適用されたデータの作成方法 # 1. レシピオブジェクトの中に存在する # レシピ適用済みデータを取り出す sampled_train_df \u0026lt;- sampling_recipe$template # 2. レシピオブジェクトの中に存在する # レシピ適用済みデータを bake(newdata=NULL) で取り出す sampled_train_df \u0026lt;- sampling_recipe %\u0026gt;% bake(newdata=NULL) # 注意：bake() に新データを渡して新データにレシピを適用したときには、サンプリングステップ は実行されない。(正確は `skip = TRUE` が指定されたステップは実行されない)  NOT_sampled_test_df \u0026lt;- sampling_recipe %\u0026gt;% bake(newdata=test_df) # newdata で渡したデータにはサンプリングは適用されない サンプリングを含んだクロスバリデーション クロスバリデーションをするときには、Validation データにはサンプリングを実施しないが、学習データに対してはサンプリングを実施したい。\n# CVのために（サンプリングをされていない）訓練データを分割する df_cv \u0026lt;- rsample::vfold_cv(train_df, v = 5, repeats = 1) # サンプリングを含むレシピを作成 () # 注意するのはここでは prep() しないということ sampling_recipe \u0026lt;- recipes::recipe(Y ~ ., data = train_df) %\u0026gt;% # テストデータや新規データに対してはサンプリングしない場合は skip = TRUE を指定する。 themis::step_downsample(Y, under_ratio = 0.5, skip = TRUE) # このレシピを使ってクロスバリデーションをする場合には、ここでは prep() しない # おそらく rsample::vfold_cv() の中で prep() される # CV を実行する cv_result_df \u0026lt;- tune::tune_grid( object = rnd_forest_model, # parsnip で作成したモデルオブジェクト preprocessor = sampling_recipe, resamples = df_cv, grid = params_grid_df, # 探索したいパラメータの値が格納されたデータフレーム metrics = yardstick::metric_set(yardstick::pr_auc, yardstick::roc_auc), control = tune::control_grid(verbose = TRUE) ) skip=TRUE について skip=TRUE が指定されたステップは、 recipes::prep(retain = TRUE) を実行したときには適用されるが、recipes::bake(newdata = new_df) を実行したときには適用されない。\n次のように考えてもよいかもしれない\n recipes::prep() は訓練データ作成用にレシピを実行する。 recipes::bake() は新データ・テストデータ・検証データ作成用にレシピを実行する  skip = TRUE を指定された処理は訓練データを作成するときには使用されるが、新データやテストデータや検証データに対しては使用されない。不均衡データ対策のためなど、目的変数を使ったサンプリングを実施するステップに対しては skip = TRUE を指定する。\nstep_*() の中での変数の指定の仕方 step_log(all_predictors(), all_outcomes())\ndplyrの　starts_with() contains() 等も使える\nprep() パラメータを持つステップの訓練・更新 rec_trained \u0026lt;- prep(mod_rec, retain = TRUE, verbose = TRUE)  retain : 処理済みの訓練データをオブジェクト内に保持する (template スロット)。後からステップを追加したいが、既存のステップの再トレーニングを避けたい場合に良いアイデア。skip = FALSE オプションを使用しているステップがある場合は、retain = TRUE を使用することをお勧めします。 training : データ処理のパラメタ（標準化のための平均と分散とか）の訓練のために使うデータフレームを別に指定する場合 fresh : TRUE なら、新たに訓練データを与えてパラメータを更新する。同時に、training に新たな訓練データを与えること。fresh=TRUE は前に prep() した step_*() の方も更新する。fresh=FALSE なら まだ prep() されていない step_*() を更新する。  レシピを新しいデータに適用する bake() 以前は bake() は新データにレシピを適用する。 juice() はレシピ内に保持されたデータに対してレシピを適用するという意味だったが。今は juice() の代わりに bake(newdata=NULL) を使うことが推奨されている。\nモデリング : parsnip パッケージ まずは目的に合わせてアルゴリズムを指定して、モデルオブジェクトを作成する。\nparsnip は異なるパッケージのアルゴリズムを同じインターフェースで扱えるようにする。\nモデルオブジェクトの作成 以下のアルゴリズムが使用できる\n parsnip::linear_reg() parsnip::decision_tree() parsnip::rand_forest()  それぞれのアルゴリズムで具体的にどのパッケージを使うかは parsnip::set_engine() で指定する。この関数はさらに使用するパッケージの機械学習モデルに追加で渡す引数を指定することもできる。\nrf_model \u0026lt;- parsnip::linear_reg() %\u0026gt;% parsnip::set_engine(\u0026quot;ranger\u0026quot;, , num.threads = 10, seed = 42, importance = \u0026quot;permutation) Thanks @ailich. Yea, that may be a confusion point. I would condider to add the explanation on the Chapter on the LogicalVector.\nI really appreciate this kinds of advice to make the book sensible for person who are not familiar to C++.\n線形回帰 : parsnip::linear_reg() linear_regression_model \u0026lt;- parsnip::linear_reg() %\u0026gt;% parsnip::set_engine(\u0026#34;lm\u0026#34;) 決定木 : parsnip::decision_tree() # 分類木 decision_tree_model \u0026lt;- parsnip::decision_tree( mode = \u0026#34;classification\u0026#34;, cost_complexity = cost_complexity, tree_depth = tree_depth, min_n = min_n) 決定木とランダムフォレストの実行の仕方 (rpart, ranger) は以下のサイトが参考になる\nhttps://www.marketechlabo.com/r-decision-tree/\nランダムフォレスト : parsnip::rand_forest() parsnip::rand_forest(mode = \u0026#34;unknown\u0026#34;, mtry = NULL, trees = NULL, min_n = NULL) ## S3 method for class \u0026#39;rand_forest\u0026#39; update( object, parameters = NULL, mtry = NULL, trees = NULL, min_n = NULL, fresh = FALSE, ... )  mode : Possible values for this model are \u0026ldquo;unknown\u0026rdquo;, \u0026ldquo;regression\u0026rdquo;, or \u0026ldquo;classification\u0026rdquo;. mtry: The number of predictors that will be randomly sampled at each split when creating the tree models. trees: The number of trees contained in the ensemble. min_n: The minimum number of data points in a node that are required for the node to be split further.  エンジン\nparsnip::set_engine(\u0026quot;ranger\u0026quot;, num.threads = 10, seed = 42, importance = \u0026quot;permutation\u0026quot;) set_engine()\n R: \u0026ldquo;ranger\u0026rdquo; (the default) or \u0026ldquo;randomForest\u0026rdquo; Spark: \u0026ldquo;spark\u0026rdquo;  変数重要度\nranger で変数重要度を計算させる場合は parsnip::set_engine() に以下のいずれかを指定する\n importance = 'impurity' importance = 'permutation'  モデル学習 モデル学習の実行 : fit() lm_fit \u0026lt;- # 学習済みモデルオブジェクト lm_mod %\u0026gt;% fit(width ~ initial_volume * food_regime, # 目的変数と説明変数をフォーミュラで指定 data = data_mart_df ) 学習済みモデルオブジェクトの内容を確認する tidy(lm_fit) # 線形回帰の学習済みオブジェクトを tidy() で処理した出力を dotwhisker::dwplot() に入力して結果を見る tidy(lm_fit) %\u0026gt;% dotwhisker::dwplot(dot_args = list(size = 2, color = \u0026#34;black\u0026#34;), whisker_args = list(color = \u0026#34;black\u0026#34;), vline = geom_vline(xintercept = 0, colour = \u0026#34;grey50\u0026#34;, linetype = 2)) 予測 : predict mean_pred \u0026lt;- predict( object = lm_fit, new_data = new_points, type = \u0026#34;conf_int\u0026#34; )   object : 学習済みモデルオブジェクト model_fit クラス\n  new_data : 予測を適用するデータ\n  type: 予測で出力する値のタイプ \u0026quot;numeric\u0026quot;, \u0026quot;class\u0026quot;, \u0026quot;prob\u0026quot;, \u0026quot;conf_int\u0026quot;, \u0026quot;pred_int\u0026quot;, \u0026quot;quantile\u0026quot;, or \u0026quot;raw\u0026quot;、指定しない場合はモデルの mode に合わせて適切に選ばれる\n  opts : type = \u0026quot;raw \u0026quot; のときに使用される引数の値を指定したリスト\n  ... : その他の引数\n level : type が \u0026quot;conf_int\u0026quot; または \u0026quot;pred_int\u0026quot; の時、ここで level=0.95 のように区間の値を指定する。 std_error : type が　\u0026quot;conf_int\u0026quot; and \u0026quot;pred_int\u0026quot; のときに、誤差の値も出力するか指定する。 TRUE なら出力する。 FALSE なら出力しない。デフォルトは FALSE  add the standard error of fit or prediction (on the scale of the linear predictors) for types of \u0026ldquo;conf_int\u0026rdquo; and \u0026ldquo;pred_int\u0026rdquo;. Default value is FALSE.\n quantile : the quantile(s) for quantile regression (not implemented yet) time : the time(s) for hazard probability estimates (not implemented yet)    ハイパーパラメータ・チューニング : tune() https://note.com/tqwst408/n/n2483a75d82a0\nfinetune パッケージを使うと精度が低いと考えられるハイパラの値については繰り返しを削減して計算効率を上げることができる。\nクロスバリデーションのデータの分割 # Partitioning data for CV df_cv \u0026lt;- rsample::vfold_cv(sampled_train_df, v = 5) 機械学習アルゴリズムとチューニングしたいハイパーパラメータを指定する parsnip パッケージでモデルオブジェクトを作成する際に、交差検証で探索したいハイパーパラメータを選ぶ。そのために、探索したいパラメータの値として tune::tune() を指定する。\n# モデルオブジェクトの作成と # チューニングするハイパーパラメータの指定 rnd_forest_model \u0026lt;- parsnip::rand_forest( mode = \u0026#34;classification\u0026#34;, trees = tune::tune(), min_n = tune::tune(), mtry = tune::tune() ) %\u0026gt;% parsnip::set_engine(\u0026#34;ranger\u0026#34;, num.threads = 10, seed = 42) CVの並列化 あるパラメタ値について、CVの各Foldの計算を並列化するために foreach パッケージを使用している。そのためには foreach のバックエンドの設定をする必要がある。\n# コア数を調べる n.cores \u0026lt;- parallel::detectCores() # クラスタを作成 my.cluster \u0026lt;- parallel::makeCluster( n.cores, # 並列数は基本的にFold数と同じにする。Fold数よりも多くても意味がない。 type = \u0026#34;PSOCK\u0026#34; # Mac/Linux の場合は \u0026#34;FORK\u0026#34; にするそちらのほうが効率が良い ) # 作成したクラスタを確認 print(my.cluster) # 作成したクラスタを foreach に登録する doParallel::registerDoParallel(cl = my.cluster) # 登録されたか確認する foreach::getDoParRegistered() # クラスタを停止するとき parallel::stopCluster(my.cluster) グリッドサーチ: tune::tune_grid() 探索したいパラメータの範囲を指定した後で、その範囲の中からランダム・規則的に100個の点を抽出する。\n# 探索するパラメータの範囲を指定した後で、 # その範囲の中からランダムに100個の点を抽出する params_grid_df \u0026lt;- list( min_n = dials::min_n(range = c(100, 1000)), mtry = dials::mtry(range = c(3, 5)), trees = dials::trees(range = c(100, 1000)) ) %\u0026gt;% tune::parameters() %\u0026gt;% # ランダムグリッドの場合 dials::grid_random(size = 100) # レギュラーグリッドの場合 #dials::grid_regular(size = 100)  # クロスバリデーションの実行 cv_result_df \u0026lt;- tune::tune_grid( object = rnd_forest_model, preprocessor = y ~ ., resamples = df_cv, grid = params_grid_df, metrics = yardstick::metric_set(yardstick::pr_auc, yardstick::roc_auc), control = tune::control_grid(verbose = TRUE) )  object : parsnip パッケージで作成したモデルオブジェクト、あるいは 内部でモデルを保持した workflows::workflow() で作成したワークフローオブジェクト preprocessor : モデルフォーミュラ、あるいは、recipes::recipe() で作成したレシピオブジェクト resamples : rset オブジェクト、rsample::vfold_cv() の出力結果 param_info : 探索したいハイパーパラメータの値の範囲、dials::parameters() オブジェクト、あるいは NULL grid : 1. param_info=NULLの時、探索したい具体的なパラメータの値が記録されたデータフレーム（カラム名はハイパーパラメータ名と一致する）、 2. 正の整数値、 param_info に値が渡されたとき （param_info の範囲から grid で指定された数のハイパーパラメータ点が探索される） metrics : 計算したい精度指標を yardstick::metric_set() を使って指定する。 NULL なら自動で選ばれる control : チューニングをいじるために使われるオブジェクト。control = tune::control_grid(verbose = TRUE) でCVの経過を出力する（Sys.setlocale(locale=\u0026quot;English\u0026quot;) をセットしないと一部文字化けする）。  ベイジアン最適化 : tune::tune_bayes() # 探索するパラメータの範囲を指定する # その範囲の中からランダムに100個の点する params_grid_df \u0026lt;- list( min_n = dials::min_n(range = c(100, 1000)), mtry = dials::mtry(range = c(3, 5)), trees = dials::trees(range = c(100, 1000)) ) %\u0026gt;% tune::parameters() # クロスバリデーションの実行 cv_result_df \u0026lt;- tune::tune_grid( object = rnd_forest_model, preprocessor = y ~ ., resamples = df_cv, grid = params_grid_df, metrics = yardstick::metric_set(yardstick::pr_auc, yardstick::roc_auc), control = tune::control_grid(verbose = TRUE) ) tune_bayes( object, ..., preprocessor, resamples, iter = 10, param_info = NULL, metrics = NULL, objective = exp_improve(), initial = 5, control = control_bayes() )  object : parsnip で作成したモデルオブジェクトか workflows::workflow(). ... : オプション、内部で使われる GPfit::GP_fit() に渡される引数 (主には corr 引数) preprocessor : モデルフォーミュラか recipes::recipe() で作成したレシピ resamples	: rset() オブジェクト rsample::vfold_cv() の返値とか iter : 探索の繰り返しの最大回数 param_info : 探索したいパラメタの範囲を指定する dials::parameters() オブジェクト。 NULL の場合は他の引数に基づいてパラメターセットが生成される。 metrics : 精度評価指標 yardstick::metric_set() オブジェクト。複数の指標を与えた場合には最初の指標が最適化される。 objective	: どのメトリックを最適化すべきかを示す文字列、または 獲得関数(acquisition function)オブジェクト（これなら複数の精度指標を組み合わせた最適化ができる？？）。 initial : 初期値（tune_grid()の結果と同様の形式、というかユーザーが事前計算したtune_grid()の結果を tune_bayes() に渡す）。あるいは正の整数（この場合は内部で自動でtune_grid()をしてから計算する）。最初に渡す結果の数はパラメタの数よりも多いほうが良い。 control : control_bayes() で作成した control オブジェクト  tune::fit_resamples() CVのなかの1つの計算は tune::fit_resamples() が使われている？？\n## S3 method for class \u0026#39;model_spec\u0026#39; fit_resamples( object, preprocessor, resamples, ..., metrics = NULL, control = control_resamples() ) ## S3 method for class \u0026#39;workflow\u0026#39; fit_resamples( object, resamples, ..., metrics = NULL, control = control_resamples() ) クロスバリデーションの結果の確認 tune::collect_metrics(cv_result_df) %\u0026gt;% filter(.metric == \u0026#34;pr_auc\u0026#34;) %\u0026gt;% arrange(desc(mean)) ベストのパラメターで再学習\nbest_param \u0026lt;- cv_result_df %\u0026gt;% select_best(metric = \u0026#34;pr_auc\u0026#34;) %\u0026gt;% select(-.config) model_best \u0026lt;- update(rnd_forest_model, best_param) fitted_model \u0026lt;- fit(model_best, formula = flag_detected_by_viirs ~ ., data = sampled_train_df) 精度検証 : yardstick  yardstick::roc_auc() yardstick::pr_auc() yardstick::roc_curve() yardstick::pr_curve() yardstick::gain_curve() yardstick::lift_curve()  "},{idx:56,href:"/notebook/r/tidyr/",title:"tidyr",content:"tidyr データフレーム内の情報を変化させずに、セルの並びを改変する\n複数列を1列にまとめる pivot_longer( data, cols, names_to = \u0026#34;name\u0026#34;, names_prefix = NULL, names_sep = NULL, names_pattern = NULL, names_ptypes = list(), names_transform = list(), names_repair = \u0026#34;check_unique\u0026#34;, values_to = \u0026#34;value\u0026#34;, values_drop_na = FALSE, values_ptypes = list(), values_transform = list(), ... ) cols: １つの列にまとめたい列を複数指定する。 names_to: 指定した列をまとめて作成される列の名前\n1列を複数列に分ける pivot_wider() 行を複製する "},{idx:57,href:"/notebook/r/wk/",title:"wk",content:"wk 地理データで登場する Well Known Text (WKT) や Well Known Binary (WKB) 形式のデータを扱うためのパッケージ\nこのパッケージをインストールすると bigrquery パッケージで BigQuery から GEOGRAPHY 型のカラムを含むテーブルをそのままダウンロードすることができるようになる。 wkutils パッケージも一緒にインストールすると良い\n https://cran.r-project.org/package=wk https://paleolimbot.github.io/wk/  GEOMETRY カラムを含む BigQuery テーブルを取得して、それを sf オブジェクトに変換する\n# テーブルの取得 geometry_df \u0026lt;- DBI::dbGetQuery(con, query) # GEOMETRY カラム (`geometry_wkt`) を `sfc` オブジェクトに変換 geometry_sfc \u0026lt;- sf::st_as_sfc(geometry_df$geometry_wkt, crs=4326) # データフレームから GEOMETRY カラムを削除 data_df \u0026lt;- geometry_df %\u0026gt;% select(-geometry_wkt) # データフレームと `sfc` オブジェクトを合体して `sf` オブジェクトを作成する geometry_sf \u0026lt;- sf::st_set_geometry(data_df, geometry_sfc) sf::st_crs(geometry_sf) \u0026lt;- 4326 wkutil https://paleolimbot.github.io/wkutils/index.html\n"},{idx:58,href:"/notebook/miscellaneous/",title:"miscellaneous",content:"miscellaneous tmux : ターミナル多重化\n"},{idx:59,href:"/notebook/python/poloars/",title:"polars",content:"polars Rustベースのデータフレーム処理ライブラリ\n"},{idx:60,href:"/notebook/python/bigquery/",title:"BigQuery",content:"BigQuery BigQuery を Python から使う\nhttps://cloud.google.com/bigquery/docs/pandas-gbq-migration\npandas.read_gbq() 小さいクエリの結果をデータフレームとして受け取るのに便利\nライブラリ pandas_gbq をインストールしておく。\nimport pandas query = \u0026quot;\u0026quot;\u0026quot; SELECT CONCAT( 'https://stackoverflow.com/questions/', CAST(id as STRING)) as url, view_count FROM `bigquery-public-data.stackoverflow.posts_questions` WHERE tags like '%google-bigquery%' ORDER BY view_count DESC LIMIT 10\u0026quot;\u0026quot;\u0026quot; df = pandas.read_gbq(query, project_id='myproject') # 大きいデータをダウンロードするときは use_bqstorage_api=True df = pandas.read_gbq(query, project_id='myproject', dialect='standard', use_bqstorage_api=True) google.cloud.bigquery 環境変数 GOOGLE_APPLICATION_CREDENTIALS 事前に環境変数 GOOGLE_APPLICATION_CREDENTIALS にクレデンシャル情報を保存したjsonファイルへのパスを格納しておく。\nクレデンシャル情報を保存したjsonファイルは pandas.read_gbq() を実行したときに以下に保存される。\n Windows  %APPDATA%\\pandas_gbq\\bigquery_credentials.dat   Linux/Mac/Unix  ~/.config/pandas_gbq/bigquery_credentials.dat    RStudio の retuculate で Python を使うときWindowsで定義した環境変数 GOOGLE_APPLICATION_CREDENTIALS を認識してくれない。そのため retuculate で実行している Python の中で環境変数を定義する。\nimport os # Set environment variables os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'C:/Users/tsuda/AppData/Roaming/pandas_gbq/bigquery_credentials.dat' from google.cloud import bigquery # Construct a BigQuery client object. client = bigquery.Client(project='world-fishing-827') query_job = client.query( \u0026quot;\u0026quot;\u0026quot; SELECT CONCAT( 'https://stackoverflow.com/questions/', CAST(id as STRING)) as url, view_count FROM `bigquery-public-data.stackoverflow.posts_questions` WHERE tags like '%google-bigquery%' ORDER BY view_count DESC LIMIT 10\u0026quot;\u0026quot;\u0026quot; ) results = query_job.result() # Waits for job to complete. # save as dataframe df = client.query(sql).to_dataframe() # データのサイズが大きい時は create_bqstorage_client=True df = client.query(sql).to_dataframe(create_bqstorage_client=True) データフレームをBigQueryへアップロードする pandas-gbq\nデータフレームをCSVに変換して送信する。ネスとしたデータには対応しない\ntable_id = \u0026#39;my_dataset.new_table\u0026#39; df.to_gbq(table_id) google-cloud-bigquery\npyarrow ライブラリに依存している。データフレームをparquetに変換して送信する。ネストしたデータにも対応。\nfrom google.cloud import bigquery import pandas df = pandas.DataFrame( { \u0026#39;my_string\u0026#39;: [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;], \u0026#39;my_int64\u0026#39;: [1, 2, 3], \u0026#39;my_float64\u0026#39;: [4.0, 5.0, 6.0], \u0026#39;my_timestamp\u0026#39;: [ pandas.Timestamp(\u0026#34;1998-09-04T16:03:14\u0026#34;), pandas.Timestamp(\u0026#34;2010-09-13T12:03:45\u0026#34;), pandas.Timestamp(\u0026#34;2015-10-02T16:00:00\u0026#34;) ], } ) client = bigquery.Client() table_id = \u0026#39;my_dataset.new_table\u0026#39; # Since string columns use the \u0026#34;object\u0026#34; dtype, pass in a (partial) schema # to ensure the correct BigQuery data type. # 部分的にカラムの値の型を指定している job_config = bigquery.LoadJobConfig(schema=[ bigquery.SchemaField(\u0026#34;my_string\u0026#34;, \u0026#34;STRING\u0026#34;), ]) # アップロードジョブを定義 job = client.load_table_from_dataframe( df, table_id, job_config=job_config ) # ジョブの実行 job.result() csv をアップロードする from google.cloud import bigquery client = bigquery.Client() filename = \u0026#39;/home/ec2-user/ml-25m/movies.csv\u0026#39; dataset_id = \u0026#39;cmbqdataset\u0026#39; table_id = \u0026#39;bqtable_via_python\u0026#39; dataset_ref = client.dataset(dataset_id) table_ref = dataset_ref.table(table_id) job_config = bigquery.LoadJobConfig() job_config.source_format = bigquery.SourceFormat.CSV job_config.skip_leading_rows = 1 job_config.autodetect = True with open(filename, \u0026#34;rb\u0026#34;) as source_file: job = client.load_table_from_file(source_file, table_ref, job_config=job_config) job.result() # Waits for table load to complete. print(\u0026#34;Loaded {}rows into {}:{}.\u0026#34;.format(job.output_rows, dataset_id, table_id)) Runquery def RunQueryPandas(query, credentials): client = bigquery.Client(credentials=credentials) query_job = client.query(query) results = query_job.result() return(results)\ndef RunQueryBq(query, destination_table):\ncommand = f'bq query -n 10 --replace --allow_large_results --use_legacy_sql=false \\ \u0026quot;{query}\u0026quot;' #--destination_table={destination_table} print(command) os.system(command)  def RunQueryGCP(query): client = bigquery.Client(project=\u0026lsquo;world-fishing-827\u0026rsquo;) query_job = client.query(query) results = query_job.result() return(results)\nCheck if the table exists def table_exists(dataset_table): dataset = dataset_table.split('.')[-2] table = dataset_table.split('.')[-1]\nq = '''SELECT size_bytes FROM {dataset}.__TABLES__ WHERE table_id='{table}' '''.format(dataset=dataset, table=table) print(q) df = pd.read_gbq(q, project_id='world-fishing-827',progress_bar_type=None) if(len(df))==0: return False else: return True  Create date partitioned table if not exist def make_date_parititoned_if_doesnt_exist(dataset_table): if not table_exists(dataset_table): dataset_table = convert_period_between_project_and_dataset_to_colon(dataset_table) command = \u0026ldquo;bq mk \u0026ndash;time_partitioning_type=DAY {}\u0026quot;.format(dataset_table) print(command) os.system(command)\ndef convert_period_between_project_and_dataset_to_colon(table): t = table.split(\u0026rdquo;.\u0026quot;) if len(t) == 2: return table if len(t) == 3: return f\u0026quot;{t[0]}:{t[1]}.{t[2]}\u0026quot;\n # try gcp API ```{python} # project = client.project # dataset_ref = bigquery.DatasetReference(project, 'my_dataset') # OUTPUT dataset_id_full = f'{project}.{dataset}' # データセットへのリファレンス dataset_ref = bigquery.Dataset(dataset_id_full) # 作成したいテーブルへのリファレンス table_ref = dataset_ref.table(table) # テーブルオブジェクト table_obj = bigquery.Table(table_ref) # 日付でパーティションに指定する table_obj.time_partitioning = bigquery.TimePartitioning( type_=bigquery.TimePartitioningType.DAY, field=\u0026quot;date\u0026quot; # name of column to use for partitioning ) # パーティションに使うカラムについてはスキーマを定義する必要がある schema = [bigquery.SchemaField(\u0026quot;date\u0026quot;, \u0026quot;DATE\u0026quot;)] # テーブルを作成 table_obj = client.create_table(table_obj, schema) tables.insert table_ref  # データセット内へ新しいテーブルへのリファレンス table_ref = dataset.table(table) # Configure the query job. job_config = bigquery.QueryJobConfig() # Set the destination table to the table reference created above. job_config.destination = table_ref # Run the query. query_job = client.query(query, job_config=job_config) query_job.result() # Waits for the query to finish "},{idx:61,href:"/notebook/python/environment/",title:"environment",content:"Python 環境設定 色々な環境構築の方法があるが、ここでは pyenv と pipenv を使った方法を採用する。\npyenv python本体のバージョン管理、インストール\nイントールと設定 ~/.zshrc などに以下の記述を追加\nexport PYENV_ROOT=\u0026quot;$HOME/.pyenv\u0026quot; export PATH=\u0026quot;$PYENV_ROOT/bin:$PATH\u0026quot; eval \u0026quot;$(pyenv init -)\u0026quot; Mac brew install pyenv Ubuntu 18.04 sudo apt update \u0026amp;\u0026amp; sudo apt install -y --no-install-recommends \\ build-essential \\ libffi-dev \\ libssl-dev \\ zlib1g-dev \\ libbz2-dev \\ libreadline-dev \\ libsqlite3-dev \\ git # Download pyenv git clone https://github.com/pyenv/pyenv.git ~/.pyenv Windows pyenv-win をインストールする。\nPowerShell\ngit clone https://github.com/pyenv-win/pyenv-win.git \u0026#34;$HOME\\.pyenv\u0026#34; 環境変数を設定\n[System.Environment]::SetEnvironmentVariable(\u0026#39;PYENV\u0026#39;,$env:USERPROFILE + \u0026#34;\\.pyenv\\pyenv-win\\\u0026#34;,\u0026#34;User\u0026#34;) [System.Environment]::SetEnvironmentVariable(\u0026#39;PYENV_ROOT\u0026#39;,$env:USERPROFILE + \u0026#34;\\.pyenv\\pyenv-win\\\u0026#34;,\u0026#34;User\u0026#34;) [System.Environment]::SetEnvironmentVariable(\u0026#39;PYENV_HOME\u0026#39;,$env:USERPROFILE + \u0026#34;\\.pyenv\\pyenv-win\\\u0026#34;,\u0026#34;User\u0026#34;) 環境変数PATHを更新\n[System.Environment]::SetEnvironmentVariable(\u0026#39;PATH\u0026#39;, $env:USERPROFILE + \u0026#34;\\.pyenv\\pyenv-win\\bin;\u0026#34; + $env:USERPROFILE + \u0026#34;\\.pyenv\\pyenv-win\\shims;\u0026#34; + [System.Environment]::GetEnvironmentVariable(\u0026#39;PATH\u0026#39;, \u0026#34;User\u0026#34;),\u0026#34;User\u0026#34;) pipenv に pyenv を認識させるため\nPYENV_ROOT : %USERPROFILE%.pyenv PYENV : %USERPROFILE%.pyenv\\pyenv-win\n環境変数PATHの先頭に追加 %PYENV%\\bin %PYENV%\\shims\npyenv を使ったPython本体のインストール pyenv を使ったPython本体のインストール方法\nインストールできるPythonの一覧\npyenv install -l バージョンを指定してインストール\npyenv install 3.8.2 デフォルトで使う Python バージョンを指定する\npyenv global 3.8.2 # 全体のデフォルト設定 # pyenv local 3.8.2 # 特定のフォルダの中だけ pyenv rehash # 2系の最新版のバージョン番号を取得 python2=$(pyenv install -l | grep -v '[a-zA-Z]' | grep -e '\\s2\\.?*' | tail -1) # 3.6系の最新版のバージョン番号を取得 python36=$(pyenv install -l | grep -v '[a-zA-Z]' | grep -e '\\s3\\.6?*' | tail -1) # 3.7系の最新版のバージョン番号を取得 python37=$(pyenv install -l | grep -v '[a-zA-Z]' | grep -e '\\s3\\.7?*' | tail -1) # 3.8系の最新版のバージョン番号を取得 python38=$(pyenv install -l | grep -v '[a-zA-Z]' | grep -e '\\s3\\.8?*' | tail -1) # pipenvを使ってインストール pyenv install $python2 pyenv install $python36 pyenv install $python37 pyenv install $python38 # python2 python3 のデフォルトのバージョンに指定 # ログアウトしても引き継がれる pyenv global $python2 $python37 pyenv global $python36 pyenv rehash Poetry Python本体とライブラリを含んだ環境管理は、最近はpipenvよりもPoetryが主流になってきているらしい。\nとりあえずは後回し\npipenv プロジェクトで使用する Python とそのパッケージのインストールと管理を行う。\n基本的には１つのフォルダを１つのプロジェクトとして、そのフォルダの中で使いたい python 環境（pythonインタープリタのバージョン、パッケージのバージョン）を pipenv を使って構築すると、構築された Python のバージョン、パッケージのバージョンの情報が Pipfile と Pipfile.lock ファイルに記録される。\nPipfile と Pipfile.lock ファイルはプロジェクトのトップレベルに生成される。後で他の人が Pipfile や Pipfile.lock を使って、同じPython環境を再構築できる。\n Pipfile ユーザーが自分で編集することもできる。 Pipfile.lock こちらは Pipenv により生成されるのでユーザーは自分で編集しない。pipenv lock で生成する。  Pipfile.lock には実際にインストールされた全ての依存パッケージとバージョンが記録される。なので完全に同じ環境が再現できる。\npipenv は、プロジェクトで使う Python インタープリタのインストール・アンインストール、バージョンの切替を行うために、pyenv に依存している。pipenv では最初にプロジェクトで使う Python インタープリタを指定するが、 pipenv は pyenv を使って指定されたPython インタープリタをインストールする。\n github Pipenv: 人間のためのPython開発ワークフロー Pipenvの進んだ使い方  環境とは 環境とは、特定のPython インタープリタとそのパッケージのバージョンが有効になった状態。\npipenv では、1プロジェクト、1フォルダ、1環境になっていると想定する。\nまずはプロジェクトフォルダを作成し、そのフォルダの中に移動する、そしてそのフォルダ内で環境を作成し、その環境に入る、そして環境内で様々なパッケージをインストールし、実際の開発を行う。\nしかし、ある環境に入った時、この環境はこのプロジェクトフォルダの中でだけ有効になるわけではない。一度環境に入ればそのターミナルで作業している限り、プロジェクトフォルダの外でもその環境は有効になっている。なぜなら「環境に入る」と書いたが、正確には pipenv は pipenv shell でそのPython環境が有効になった新たな shell を起動する。つまり、実行中のそのターミナルがその環境に乗っ取られるということ。この環境は、別のターミナルウィンドウで開いた shell には影響しない、あくまでその環境に入ったターミナルの中でだけ有効になる。ただし、プロジェクトフォルダの外で pipenv コマンドを使ってしまうと、そのフォルダに新たな環境を作成してしまうらしい。\nとりあえず、OSにインストールされているPythonとは別に、独立した特定のPythonバージョンを使った開発環境を作成したいということであれば、ダミーのフォルダを作成して、そこをプロジェクトフォルダとして好きなPython環境を構築し、毎回シェルにログインするときに自動的にそのプロジェクト環境が有効になるように .zshrc などにコマンドを記述しておけばよいだろう。\nインストール brew install pipenv ubuntu\nubuntu では pip を使って pipeenv をインストールする。\nsudo apt install python3-pip ~/.local/bin に pip3 がインストールされる\npip3 install pipenv Windows\nWindows では pip を使って pipeenv をインストールする。\nコマンドプロンプト\npip install pipenv 環境の作成 プロジェクト用のフォルダを作成し、移動する\nmkdir myproject cd myproject  このプロジェクトで使用する Python バージョンを指定する。 同時にこのプロジェクト用の仮想環境が作成される この時 pyenv がインストール＆設定されていれば自動でインストールされる。 これにより Pipfile が作成される  pipenv --python 3.7.6 # 下記のような指定も可能 # Python 3を使う場合 # pipenv --three # Python 2を使う場合 # pipenv --two Pipfile，Pipfile.lockから環境を再現する 既存の Pipfile Pipfile.lock を参照して、Pythonとパッケージをインストールすることもできる。\nプロジェクトフォルダを作成し、その中に、既存の Pipfile を配置する。そして次のコマンドを実行すると Pipfile に記述された環境が構築される。\n# Pipfile.lock を参照してパッケージをインストールする pipenv install # 通常のパッケージの他に開発用パッケージもインストールしたい場合 pipenv install --dev 同様に、Pipfile.lock が手元にある場合は、以下のコマンドを用いる\npipenv sync pipenv sync --dev # 開発用パッケージもインストールする場合 Pipenv では、開発者が Pipfile にインストールしたいバージョンの希望を書き、Pipfile.lock に実際にインストールしたバージョンが記録される（らしい）。\n Pipfile ユーザーが自分で編集することもできる。 Pipfile.lock こちらは Pipenv により生成されるのでユーザーは自分で編集しない。このファイルには実際にインストールされた全ての依存パッケージとバージョンが記録されている。なので完全に同じ環境が再現できる。  仮想環境の確認 # 現在のプロジェクトのために作成された仮想環境のパスを表示する pipenv --venv デフォルトでは ~/.local/share/virtualenvs 以下に各プロジェクトの環境が保存される\n仮想環境の削除 # 現在のプロジェクトの仮想環境を削除 pipenv --rm 仮想環境をプロジェクトフォルダ内に保存する デフォルトでは ~/.local/share/virtualenvs 以下に各プロジェクトの環境が保存されるが、これをプロジェクトフォルダ内に保存させるための設定。.bashrc などに以下の記述を追加する。\nexport PIPENV_VENV_IN_PROJECT=true 既存の仮想環境にに対する操作 仮想環境にの中に入る 環境が作成されたプロジェクトフォルダに移動して環境の中に入る。正確にはPython環境が有用になったシェルを起動している。実行中のターミナルのpython環境が、ここで入った環境に乗っ取られる。\npipenv shell この環境はこのプロジェクトフォルダの外に出ても有効ではある。ただし、プロジェクトフォルダの外で pipenv コマンドを使うとそのフォルダに新しい環境が作成されてしまうので注意する。\n仮想環境にライブラリをインストールする pipenv ではライブラリをインストールするときに通常のライブラリと開発用のライブラリを分けることができる。いくつかのライブラリは、プログラムの開発をするときには使用するが、完成したプログラムを実行するときには必要ない場合があるので、開発の時だけ使用するライブラリは開発用ライブラリとしてインストールするようにする。\n# 通常のライブラリのインストール pipenv install numpy # 開発用ライブラリのインストール pipenv install --dev autopep8 仮想環境にライブラリをインストールすると、その情報が Pipfile と Pipfile.lock に記録される。\n例えば、autopep8 はコードを成形するためのライブラリなので、プログラムの動作そのものには必要ない。\nライブラリのバージョンを指定してインストールする pipenv install 'requests\u0026gt;=3.7.2' geos インストール済パッケージを列挙 pipenv graph 古いパッケージを探す pipenv update --outdated 古いパッケージを更新 pipenv update pip パッケージのインストール先 pipでインストールされたパッケージのインストール先の確認\npip show [パッケージ名] [user@local] /usr/local/bin/pip3 show matplotlib Name: matplotlib Version: 3.1.2 Summary: Python plotting package Home-page: https://matplotlib.org Author: John D. Hunter, Michael Droettboom Author-email: matplotlib-users@python.org License: PSF Location: /usr/local/lib/python3.7/site-packages Requires: pyparsing, cycler, python-dateutil, kiwisolver, numpy Required-by: Pythonパッケージの検索先 パッケージの検索先の確認法 import sys import pprint # print() より見やすい pprint.pprint(sys.path) インストールされている Python\n/usr/bin/python3 /usr/local/bin/python3 /usr/local/opt/python/bin/python3.7 $HOME/.pyenv/versions/3.7.6/bin/python3.7 Mac 標準の python3\n/usr/bin/python3\nPython 3.7.3 (default, Dec 13 2019, 19:58:14) [Clang 11.0.0 (clang-1100.0.33.17)] on darwin ['', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python37.zip', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/lib-dynload', '/Users/tsuda/Library/Python/3.7/lib/python/site-packages', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/site-packages'] Homebrew でいれた Python3\n/usr/local/bin/python3\nPython 3.7.4 (default, Sep 7 2019, 18:27:02) [Clang 10.0.1 (clang-1001.0.46.4)] on darwin ['', '/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python37.zip', '/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7', '/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload', '/Users/tsuda/Library/Python/3.7/lib/python/site-packages', '/usr/local/lib/python3.7/site-packages', '/usr/local/lib/python3.7/site-packages/geos', '/usr/local/Cellar/numpy/1.16.4_1/libexec/nose/lib/python3.7/site-packages'] Homebrew でいれた Python3?\n/usr/local/opt/python/bin/python3.7\nPython 3.7.4 (default, Sep 7 2019, 18:27:02) [Clang 10.0.1 (clang-1001.0.46.4)] on darwin ['', '/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python37.zip', '/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7', '/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload', '/Users/tsuda/Library/Python/3.7/lib/python/site-packages', '/usr/local/lib/python3.7/site-packages', '/usr/local/lib/python3.7/site-packages/geos', '/usr/local/Cellar/numpy/1.16.4_1/libexec/nose/lib/python3.7/site-packages'] pyenv で入れた Python\n.pyenv/versions/3.7.6/bin/python3.7\nPython 3.7.6 (default, Mar 14 2020, 19:17:31) [Clang 11.0.0 (clang-1100.0.33.17)] on darwin ['', '/Users/tsuda/.pyenv/versions/3.7.6/lib/python37.zip', '/Users/tsuda/.pyenv/versions/3.7.6/lib/python3.7', '/Users/tsuda/.pyenv/versions/3.7.6/lib/python3.7/lib-dynload', '/Users/tsuda/.pyenv/versions/3.7.6/lib/python3.7/site-packages'] パッケージの検索先への追加 sys.path.append() を使う方法\nこれはこのときの実行限りで有効になる\n# スクリプトファイルの１階層上のディレクトリを追加 import os import sys sys.path.append(os.path.join(os.path.dirname(__file__), \u0026#39;..\u0026#39;)) PYTHONPATH に追加する方法\nPython2 と Python3 で PYTHONPATH を使い分けられない\nexport PYTHONPATH=\u0026#34;/path/to/add:$PYTHONPATH\u0026#34;` '/usr/local/lib/python3.7/site-packages', import os import sys sys.path.append('/usr/local/lib/python3.7/site-packages') "},{idx:62,href:"/notebook/python/jupyter/",title:"jupyter",content:"jypyter インストール Jupyter インストール プロジェクトフォルダに移動して、仮想環境に入ってから\ncd Project pipenv shell pipenv install jupyter jupyter を起動する jupyter notebook pipenv の仮想環境の中の jupyter を起動する場合。\npipenv プロジェクトフォルダの「外」でこのコマンドを打つと、新しい pipenv 環境が作成されてしまう\npipenv run jupyter notebook jupytext jupytext は jupyter 経由で .ipynb と .py .Rmd などを同期するツール\nインストール cd Project pipenv shell pipenv install jupytext 設定 jupyter の設定 jupyter の設定ファイル（~/.jupyter/jupyter_notebook_config.py）を生成する。\npipenv run jupyter notebook --generate-config 以下の記述を追加\n# jupytext を使用可能にする設定 c.NotebookApp.contents_manager_class = \u0026quot;jupytext.TextFileContentsManager\u0026quot; # jupytext で、 `.ipynb` と `.py` と `.Rmd` が連動（同期）するようにする設定 c.ContentsManager.default_jupytext_formats = \u0026quot;ipynb,py,Rmd\u0026quot; 既存の notebook を編集する場合 jupyter の Edit \u0026gt; Edit Notebook Metadata から、以下の記述を先頭に追加する。\n\u0026quot;jupytext\u0026quot;: {\u0026quot;formats\u0026quot;: \u0026quot;ipynb,py,Rmd,R\u0026quot;}, ここで .py, .ipynb 以外にも、md, Rmd, jl, R などのフォーマットが使える。\n既存の notebook のメタデータを上記のように編集し、上書き保存すると .ipynb に対応する .py が生成されている。\n.ipynb を編集すると .py に変更が反映され、.py を編集すると .ipynb に変更が反映されるらしい。\nコマンドライン jupytext はコマンドラインツールとしても使うことができる\n# ipynb を Rmd に変換 jupytext --to Rmd notebook.ipynb # Rmd を ipynb に変換 jupytext --to ipynb notebook.Rmd # convert notebook.ipynb to a .py file in the double percent format jupytext --to py:percent notebook.ipynb jupytext --to py:percent --opt comment_magics=false notebook.ipynb # same as above + do not comment magic commands jupytext --to markdown notebook.ipynb # convert notebook.ipynb to a .md file jupytext --output script.py notebook.ipynb # convert notebook.ipynb to a script.py file jupytext --to notebook notebook.py # convert notebook.py to an .ipynb file with no outputs jupytext --update --to notebook notebook.py # update the input cells in the .ipynb file and preserve outputs and metadata jupytext --to md --test notebook.ipynb # Test round trip conversion jupytext --to md --output - notebook.ipynb # display the markdown version on screen jupytext --from ipynb --to py:percent # read ipynb from stdin and write double percent script on stdout jupytext \u0026ndash;to py notebool.ipynb\n"},{idx:63,href:"/notebook/python/pandas/",title:"Pandas",content:"Pandas DataFrame 提供するライブラリ\nPandas User Guide\n型  Series: 列の型（要素の名前付きベクター） DataFrame: Seriesを列とするテーブル  データフレームのメタデータの取得 df.columns # 列名(=文字列型の列インデックス) df.index # 行インデックス df.values # データフレームの値ををNumPy配列（ndarray）として取り出す df.dtypes # 各列の型を返す メソッドを繋げる DataFrameを何回も再帰代入するより、メソッドを繋いだほうが書きやすいし読みやすい。 カッコかバックスラッシュを使えばドット前に改行やスペースを入れて整形可能。\n(iris.query(\u0026#39;species != \u0026#34;setosa\u0026#34;\u0026#39;) .filter(regex=\u0026#39;^(?!sepal)\u0026#39;) .assign(petal_area=lambda x: x[\u0026#39;petal_length\u0026#39;] * x[\u0026#39;petal_width\u0026#39;] * 0.5) .groupby(\u0026#39;species\u0026#39;) .aggregate(np.mean) .sort_values([\u0026#39;petal_length\u0026#39;], ascending=False) ) ファイルの読み書き 読み込み import pandas as pd # 読み込み df = pd.read_excel(\u0026#34;test.xlsx\u0026#34;, sheet_name=\u0026#34;mysheet\u0026#34;, skiprows = 1) # 他にもいろいろある # pd.read_csv() # pd.read_parquet() # pd.read_feather() 書き出し # 書き出し df.to_excel(filepath, index=False) # df.to_csv() # df.to_parquet() # df.to_feather() 列の選択 # df[]の返値はSeries # 列名での列の抽出 col = df.loc[\u0026#34;col1\u0026#34;] # 列番号での列の抽出 col = df.iloc[:, 1] # df[[]]での返値はデータフレーム df2 = df[[\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;]] # 複数列を抽出（特定の列番号リストで指定） df2 = df.iloc[:, [0, 2]] # A列とC列を抽出 # 複数列を抽出（列の範囲を指定） df2 = df.iloc[:, 0:5] # A列～D列を抽出 0:5は[0,1,2,3,4]と同義 # 特定の文字列を含む列を抽出 df.filter(like=\u0026#39;得点\u0026#39;, axis=1) # 列名を正規表現で抽出 df.filter(regex=\u0026#39;^点数\u0026#39;, axis=1) # 数値型の列だけを抽出 df.select_dtypes(include=\u0026#39;number\u0026#39;) # 文字列型の列だけを抽出 df.select_dtypes(include=\u0026#39;object\u0026#39;) 行の選択 行インデックスや行番号での指定 DataFrameの「行インデックス」と「行番号」は異なる。行番号は先頭行を0番とした連番だが、行インデックスには「文字列や数値・日付など」を自由に付けらえる。なので3番目の行の行インデックスが0であるかもしれない。\n# 行インデックスで行を取得 df.loc[1000] # インデックスが1000の行 df.loc[\u0026#34;A\u0026#34;] # インデックスがAの行 # 行番号で行を取得 df.iloc[1000] # 0から始まる行番号が1000番目の行 # 範囲指定もできる df.loc[1:10] # 行インデックスが1～9 df.iloc[1:10] # 行番号が1～9 # 行番号と列番号の範囲指定で部分データフレーム df2 = df.iloc[1:5, 6:10] # 行番号1～4, 列番号6～9の範囲を取得 # 列番号の範囲指定 df2 = df.iloc[:, 6:10] # 行番号の範囲指定 df2 = df.iloc[1:5, :] # df.iloc[1:5] と同義 先頭行、末尾行の取得 df.head(10) df.tail(10) 特定の列の値で行を抽出 数値型の列の例\n# 数値がある値より大きい ddf_high_score f2 = df[df[\u0026#39;点数\u0026#39;] \u0026gt; 80] # 数値がある値の範囲 df_age_range = df[(df[\u0026#39;年齢\u0026#39;] \u0026gt;= 30) \u0026amp; (df[\u0026#39;年齢\u0026#39;] \u0026lt; 40)] # 数値が特定のリストに含まれる df_selected_scores = df[df[\u0026#39;点数\u0026#39;].isin([70, 90])] # ある列に欠損値を含まない行 df_notna = df[df[\u0026#39;年齢\u0026#39;].notna()] # 特定条件に合致する行のインデックスのみを取得する場合 index_high_score = df[df[\u0026#39;点数\u0026#39;] \u0026gt; 80].index 文字列型の列の例\n# 特定の文字列と一致する行 df_sales = df[df[\u0026#39;部署\u0026#39;] == \u0026#39;営業\u0026#39;] # 複数の文字列のどれかと一致する行 df_selected = df[df[\u0026#39;部署\u0026#39;].isin([\u0026#39;営業\u0026#39;, \u0026#39;人事\u0026#39;])] # 特定の文字列を含んでいる行 df_tanaka = df[df[\u0026#39;氏名\u0026#39;].str.contains(\u0026#39;田中\u0026#39;, na=False)] # 特定の文字列を含んでいる行（アルファベットで大文字小文字を区別しない） df_ignore_case = df[df[\u0026#39;name\u0026#39;].str.contains(\u0026#39;TARO\u0026#39;, case=False, na=False)] # 特定の文字列を含まない行 df_not_tanaka = df[~df[\u0026#39;氏名\u0026#39;].str.contains(\u0026#39;田中\u0026#39;, na=False)] # 特定の文字列で始まる行 df_start_tanaka = df[df[\u0026#39;氏名\u0026#39;].str.startswith(\u0026#39;田中\u0026#39;, na=False)] # 特定の文字列で終わる行 df_rou = df[df[\u0026#39;氏名\u0026#39;].str.endswith(\u0026#39;郎\u0026#39;, na=False)] 重複行の削除 df_nodup = df.drop_duplicates() 単一要素の取得 # fast scalar (single value) lookup df.at[0,\u0026#39;col\u0026#39;] # 行番号（行インデックス？）と列名での単一要素の取り出し df.iat[0,1] # 行番号と列番号で単一要素の取り出し 列の加工 加工列の作成 既存の列を加工して新しい列を追加する。\n# assign()とlambdaの組み合わせ # xはdf全体を意味する import pandas as pd df = pd.DataFrame({ \u0026#39;price\u0026#39;: [100, 200, 300], \u0026#39;quantity\u0026#39;: [1, 3, 2] }) # よくあるやり方 df[\u0026#39;total\u0026#39;] = df[\u0026#39;price\u0026#39;] + df[\u0026#39;quantity\u0026#39;] # assign() \u0026amp; lambdaを使った方法 # xはdf全体を意味する df = ( df .assign( total=lambda x: x[\u0026#39;price\u0026#39;] * x[\u0026#39;quantity\u0026#39;], price_with_tax=lambda x: x[\u0026#39;price\u0026#39;] * 1.10 ) ) 列名の変更.rename() df_renamed = df.rename(columns={ \u0026#39;size\u0026#39;: \u0026#39;件数\u0026#39;, \u0026#39;③成分名\u0026#39;: \u0026#39;成分\u0026#39;, \u0026#39;⑫製造販売業者の\\n「出荷対応」の状況\u0026#39;: \u0026#39;出荷状況\u0026#39; }) 行をソートする df_sorted = df.sort_values( by=[\u0026#39;列A\u0026#39;, \u0026#39;列B\u0026#39;], # ソート基準の列 ascending=[False, False] # どちらも降順 ) 集計 groupby() import pandas as pd df = pd.DataFrame({ \u0026#39;category\u0026#39;: [\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;A\u0026#39;], \u0026#39;price\u0026#39;: [100, 200, 150, 250, 120], \u0026#39;quantity\u0026#39;: [1, 2, 1, 1, 3] }) # 2つの列でグループ化し、件数をカウント grouped = df.groupby([\u0026#39;列A\u0026#39;, \u0026#39;列B\u0026#39;], as_index=False).size().reset_index(name=\u0026#39;件数\u0026#39;) # 基本はこの形で覚える result = ( df # category列の値でグループ化 .groupby(\u0026#39;category\u0026#39;, as_index=False) # as_index=False だと結果に category 列を含める # as_index=True だと category の値が行インデックスになる # グループごとに集計 .agg( # price列の meanを計算して、price_mean列を作成 price_mean=(\u0026#39;price\u0026#39;, \u0026#39;mean\u0026#39;), price_max=(\u0026#39;price\u0026#39;, \u0026#39;max\u0026#39;), total_quantity=(\u0026#39;quantity\u0026#39;, \u0026#39;sum\u0026#39;) ) ) # この形だと、結果の行(\u0026#39;category\u0026#39;)と列（\u0026#39;price\u0026#39;, \u0026#39;quantity\u0026#39;）にインデックスが付与される result = ( df .groupby(\u0026#39;category\u0026#39;) .agg({ \u0026#39;price\u0026#39;: \u0026#39;sum\u0026#39;, \u0026#39;quantity\u0026#39;: \u0026#39;sum\u0026#39; }) ) # 複数の関数を適用することもできる # ただし結果が列名ではなく、マルチインデックス（入れ子になったインデックス）になるのでわかりにくい result = ( df .groupby(\u0026#39;category\u0026#39;) .agg({ \u0026#39;price\u0026#39;: [\u0026#39;mean\u0026#39;, \u0026#39;sum\u0026#39;, \u0026#39;max\u0026#39;], \u0026#39;quantity\u0026#39;: [\u0026#39;sum\u0026#39;, \u0026#39;count\u0026#39;] }) )    関数名 説明     'sum' 合計   'mean' 平均   'median' 中央値   'max' 最大値   'min' 最小値   'count' 件数（NaNを除く）   'size' 件数（NaN含む、countとの違い）   'std' 標準偏差   'var' 分散   'first' 最初の値   'last' 最後の値   'nunique' 重複を除いた件数   'any' 真偽値でいずれかがTrue   'all' 真偽値で全てTrue    データフレームの結合 pd.merge pd.merge(df1, df2, on=\u0026#39;キー列\u0026#39;, how=\u0026#39;left\u0026#39;) # 複数列 pd.merge(df1, df2, on=[\u0026#39;成分名\u0026#39;, \u0026#39;製造会社名\u0026#39;], how=\u0026#39;outer\u0026#39;) # キーとなる列名が異なる場合 pd.merge(df1, df2, left_on=\u0026#39;df1側の列\u0026#39;, right_on=\u0026#39;df2側の列\u0026#39;, how=\u0026#39;inner\u0026#39;) how	\u0026lsquo;inner\u0026rsquo;, \u0026lsquo;left\u0026rsquo;, \u0026lsquo;right\u0026rsquo;, \u0026lsquo;outer\u0026rsquo;\nデータの変形 https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html\n縦持ち変換.melt() 横持ち変換.pivot() 縦持ち変換 df.melt(id_vars=None, value_vars=None, var_name=None, value_name='value', ...)\nRでいう tidyr::pivot_longer()。\n横持ち変換 df.pivot(index=None, columns=None, values=None)\nRでいう tidyr::pivot_wider()。\ndf.pivot_table() インデックス # 指定した列を**インデックス（行ラベル）**に変換します。 # # inplace=Trueを指定すると元のデータフレームに対して直接変更されます。 df.set_index(\u0026#39;列名\u0026#39;, inplace=False) # 現在のインデックスを通常の列として戻し、デフォルトの数値インデックスに戻します。 # drop=Trueを指定すると、インデックス列をデータフレームに戻さず削除します。 # inplace=Trueを使えば元のDataFrameを直接変更します。 df.reset_index(inplace=False, drop=False) Pandas.Series 名前付きベクトルのようなもの、データフレームの列や行をSeriesとして取り出すこともできる。\ns = pd.Series([1, 2, 3, 4], index=[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;]) print(s[\u0026#39;a\u0026#39;]) # 出力: 1 print(s[0]) # 出力: 1 # 値で抽出 print(s[s \u0026gt; 2]) # 計算集計 print(s.sum()) # 出力: 10 print(s.mean()) # 出力: 2.5 BigQuery との連携 import pandas_gbq df = pandas_gbq.read_gbq(\u0026#34;SELECT my_col FROM `my_project.my_dataset.my_table`\u0026#34;) 初回に実行した時にブラウザが開いてアクセスコードの入力が求められる\nユーザーごとのアクセス情報は以下に保存される。\n Windows  %APPDATA%\\pandas_gbq\\bigquery_credentials.dat   Linux/Mac/Unix  ~/.config/pandas_gbq/bigquery_credentials.dat    これらのパスをLinux/Mac/Windowsの環境変数として保存しておく GOOGLE_APPLICATION_CREDENTIALS\n"},{idx:64,href:"/notebook/python/vscode/",title:"Python in VScode",content:"Python in VScode VScode で Python を使う時の Tips\n参考：[VS Code でPython，Jupyter を動かす](https://qii ta.com/surei/items/9f25d7efa7c67d55d98f)\n拡張機能  Python 必須  はじめに コマンドパレットで select interpreter と入力\nVScodeと一緒に使いたい python インタープリタを選択する\n"},{idx:65,href:"/notebook/python/anaconda/",title:"anaconda",content:"Anaconda minoconda\nhttps://conda.io/projects/conda/en/latest/user-guide/concepts/environments.html\n"},{idx:66,href:"/notebook/r/dbi/",title:"DBI",content:"DBI DBIパッケージはRからデータベース（DB）とやりとりするためのインターフェースを提供している。これにより、ドライバーを切り替えるだけで、共通のインターフェースを用いて様々な種類のDBサーバーとやりとりすることができる。\nDBIパッケージはRとデータベース（DB）がやりとりするための基本的な関数を提供している。それぞれの関数の引数などは各DBのドライバーを提供している別のパッケージ（ RPostgreSQL や bigquery など）によって動作が細かく指定できるように拡張されているので、そちらのマニュアルを参照すること。\nオブジェクト DBIパッケージでは主に下の３種類のオブジェクトが登場する。\nDBIDriver: ドライバー・オブジェクト drv\n dbDriver() RSQLite::RSQLite(), RPostgreSQL::RPostgreSQL(), RMySQL::RMySQL(), bigrquery::bigquery() などの関数が返すオブジェクト  DBIConnection: DBコネクション・オブジェクト con\ndbConnect()が返すオブジェクト\nDBIResult: クエリ結果のオブジェクト res\ndbSendquery()が返すオブジェクト\nドライバ・コネクション・クエリ結果の情報：dbGetInfo() dbGetInfo()\n接続したいDBへのドライバーをを用意する DBIパッケージに対応した、各DBへのドライバを、提供するパッケージをインストールする。\n RPostgreSQL RMySQL RSQLite bigquery  ドライバを用意する。下の２つの方法は等価。\ndrv \u0026lt;- PostgreSQL() drv \u0026lt;- dbDriver(\u0026quot;PostgreSQL\u0026quot;) ドライバを閉じる：dbUnloadDriver\ndbUnloadDriver(drv) DBサーバーへ接続する：dbConnect ドライバーは例えば以下がある。各パッケージをインストールする。\n RSQLite::RSQLite() RPostgreSQL::RPostgreSQL() RMySQL::RMySQL()  例）PostgreSQLへの接続\ndrv \u0026lt;- PostgreSQL() con \u0026lt;- dbConnect(drv, host=\u0026quot;localhost\u0026quot;, user= \u0026quot;edd\u0026quot;, password=\u0026quot;.....\u0026quot;, dbname=\u0026quot;...\u0026quot;) パスワードをRのソースに直接記述するのはセキュリティ上よろしくない。ファイルに書いておいてそれを読み出すようにする。そうすればRのソースを共有する場合にも安心である。\n例えば \u0026ldquo;.pgpass\u0026rdquo; というファイルにパスワードを保存してきそれを読み出す場合\npassword \u0026lt;- scan(\u0026quot;.pgpass\u0026quot;, what=\u0026quot;\u0026quot;) 接続の設定\nPostgreSQL() は接続の設定を変えられる。\nPostgreSQL(max.con = 16, fetch.default.rec = 500, force.reload = FALSE)  max.con：最大コネクション数 fetch.default.rec：データを取得するときに一度に送信するレコード数。fetch()はこの値を利用する。 force.reload：クライアントのコードをリロードするか。イミフ  コネクションの情報を表示する summary(con) コネクションを解除する dbDisconnect(con) ## Closes the connection データフレームの内容からテーブルを作成する：dbWriteTable dbWriteTable( con, name = \u0026quot;table_name\u0026quot;, #テーブル名 value = df, # データフレーム overwrite=TRUE, # テーブルを上書きする append=FALSE, # 行を追加する row.names=FALSE # 行番号 ) テーブルのリスト：dbListTables dbListTables(con) テーブルのカラム名：dbListFields dbListFields(con, \u0026quot;iris\u0026quot;) DBのデータを取得する テーブルを指定して読み込む : dbReadTable iris1 \u0026lt;- dbReadTable(con, \u0026quot;iris\u0026quot;) クエリの結果を読み込む：dbGetQuery data \u0026lt;- dbGetQuery(con, \u0026quot;SELECT * FROM iris ORDER BY weighted DESC LIMIT 5\u0026quot;) クエリの送信とデータの取得を分離する：dbSendQuery \u0026amp; fetch 上と同様クエリの結果を取得するが、クエリの送信とデータの取得を分離する。\nクエリを送信する\nrs \u0026lt;- dbSendQuery(con, \u0026quot;SELECT * FROM iris\u0026quot;) 最初の10レコードだけ取得する\niris3.first10 \u0026lt;- fetch(rs, 10) 残りを全て取得する\niris3.rest \u0026lt;- fetch(rs, -1) fetch はカーソルを移動させながらデータを取得する。なので上記の場合には iris3.first10 と iris3.rest 合体させると全レコードになる。\nrbind(iris3.first10, iris3.rest) 【重要】ローカルとリモート確保されたリソースを開放する\ndbSendQueryの結果はリモートのリソースを消費するので必要がなくなったら dbClearResult(rs) すること。\ndbClearResult(rs) ファイルからクエリを読み込んで実行する fileName\u0026lt;-\u0026quot;test.sql\u0026quot; q\u0026lt;-readChar(fileName, file.info(fileName)$size) res \u0026lt;- dbSendQuery(con, q) クエリ結果のリソースを開放する：dbClearResult 前のクエリの結果を全て取得していないうちに、同じコネクションで、次のクエリを実行しようとしてもできない。\nrs \u0026lt;- dbSendQuery(con, \u0026quot;SELECT * FROM iris\u0026quot;) #前のクエリ rs \u0026lt;- dbSendQuery(con, \u0026quot;SELECT * FROM hoge\u0026quot;) #次のクエリ（エラー） 実行する場合には、前のクエリの結果をクリアする必要がある。\ndbClearResult(con, rs) rs \u0026lt;- dbSendQuery(con, \u0026quot;SELECT * FROM hoge\u0026quot;) #OK dbSendQuery() するとサーバーでクエリが実行され、サーバー上に結果が保存されるらしい、そのままにしておくとメモリなどのリソースを消費するので、必要なくなった結果は適宜開放する。\nテーブルを削除する：dbRemoveTable dbRemoveTable(conn,\u0026quot;table1\u0026quot;) カラム情報を表示する：dbColumnInfo(res, \u0026hellip;) dbColumnInfo(rs) ## name Sclass type len precision scale nullOK ## 1 Sepal.Length double FLOAT8 8 -1 -1 TRUE ## 2 Sepal.Width double FLOAT8 8 -1 -1 TRUE ## 3 Petal.Length double FLOAT8 8 -1 -1 TRUE ## 4 Petal.Width double FLOAT8 8 -1 -1 TRUE ## 5 Species character TEXT -1 -1 -1 TRUE 結果の元クエリを表示：dbGetStatement dbGetStatement(rs) ## [1] \u0026quot;SELECT * FROM iris\u0026quot; ローカルにあるクエリ結果のレコード数：dbGetRowCount(rs) dbGetRowCount(rs) ## [1] 10 # ... just get the first 10 records 結果のうち、ローカルに送られてきたレコード数？\nテーブルの存在を確認：dbExistsTable dbExistsTable(con, c(\u0026quot;tmp\u0026quot;,\u0026quot;test_tbl\u0026quot;)) クエリ結果オブジェクトのリスト：dbListResults 現在のコネクションでアクティブな DBIResult のリストを返す。\ndbClearResults(dbListResults(con)[[1]]) 現在開いているコネクション・オブジェクトのリスト：dbListConnections オブジェクトの型を調べる：dbDataType DBのでの例外（エラー情報）を取得する：dbGetException データ変更クエリにより影響を受ける行数：dbGetRowsAffected クエリ結果に対する処理が完了しているか？：dbHasCompleted DBオブジェクトの状態が正常かチェックする：dbIsValid "},{idx:67,href:"/notebook/",title:"Introduction",content:"for my personal use "}];window.bookSearch={pages:a,idx:lunr(function(){this.ref("idx"),this.field("title"),this.field("content"),a.forEach(this.add,this)})}})()