(function(){const pages=[{"idx":0,"href":"/notebook/docs/","title":"About","content":"About  Masaki E. Tsuda 津田 真樹（つだ　まさき） Twitter : teuder  "},{"idx":1,"href":"/notebook/python/","title":"Python","content":"Python "},{"idx":2,"href":"/notebook/r/","title":"R","content":"R コーディングスタイル The tidyverse style guide\nソースコードを確認 lookup パッケージ\ndevtools::install_github(\u0026quot;jimhester/lookup\u0026quot;) lookup::lookup(dplyr::summarise) バージョン管理  RSwitch R本体のバージョン管理 https://twitter.com/hrbrmstr renv, packrat : パッケージのバージョン管理  並列計算 furrr : purrr と同様の使い方で並列に計算できる\n可視化 Data to Viz\nインストール Ubuntu  UBUNTU PACKAGES FOR R Ubuntuに最新版のRをインストールする  環境設定 環境変数 Sys.getenv() Sys.setenv(\u0026quot;LANGUAGE\u0026quot;=\u0026quot;ja_Jp.UTF-8\u0026quot;) 環境設定ファイル Rの設定ファイルには以下がある。\n .Renviron .Rprofile .R/Makevars  ユーザーごとのこれらの設定ファイルは、ユーザーのRホームディレクトリ R_USER に配置しておくとよい。\n# ユーザーのRホームディレクトリの確認方法 \u0026gt; Sys.getenv(\u0026quot;R_USER\u0026quot;) [1] \u0026quot;C:/Users/hoge/Documents\u0026quot; .Renviron 環境変数を指定するシェルスクリプト\nR_USER=${HOME}/.R R_LIBS_USER=${R_USER}/library R_ENVIRON_USER=${R_USER}/.Renviron R_PROFILE_USER=${R_USER}/.Rprofile R_HISTFILE=${R_USER}/.Rhistory R_HISTSIZE=65535 LANG=C LC_CTYPE=en_US.UTF-8 .Rprofile Rの起動時や終了時に実行したいRのスクリプト\nhttps://cran.r-project.org/doc/manuals/R-intro.html#Customizing-the-environment\n~/.R/Makevars https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Using-Makevars\nCC=/opt/local/bin/gcc-mp-4.7 CXX=/opt/local/bin/g++-mp-4.7 CPLUS_INCLUDE_PATH=/opt/local/include:$CPLUS_INCLUDE_PATH LD_LIBRARY_PATH=/opt/local/lib:$LD_LIBRARY_PATH CXXFLAGS= -g0 -O3 -Wall MAKE=make -j4 ロケール、文字コード ロケールを指定する文字列 ロケールとは表示する言語・文字コードや数字や時刻の表示形式を変更するための設定のこと\n R manual: Locale WindowsでRのロケールを設定するときのメモ  次の３つの情報を指定する文字列\n 言語 ja 領域 jp 文字コード utf-8  ロケールの指定形式はOSにより異なる\n Windows  Japanese_Japan.932 日本語、日本国、Windows拡張Shift-JIS（コードページ932） English_United States.1252 省略した入力として Japanese English でも良い（エンコーディングはデフォルトで言語だけ変えたいとき）   Mac, Linux  ja_JP.UTF-8 en_US.UTF-8    環境変数 メッセージを変えたいだけなら .Renviron ファイルの中で LANGUAGE 環境変数を設定する\n.Renviron\nLANGUAGE=English #LANGUAGE=Japanese 表示されるメッセージの言語は、最初に環境変数 LANGUAGE の設定が読まれる。次に LC_ALL, LC_MESSAGES and LANG\nRでロケールに関連する環境変数\n LC_MESSAGES : R で表示されるメッセージの設定 LC_COLLATE : 並び換えや正規表現に用いる文字の照合順序に影響します。 LC_CTYPE : 文字の判定・操作・文字数のカウントなどに影響します。 LC_MONETARY : 金額に関連する数値、通貨記号の表示に影響します。 LC_NUMERIC : 金額に関係しない数値の表示（小数の区切り文字など）に影響します。 LC_TIME : 日付と時刻の表示に影響します。 LC_ALL : これが設定されていると他のロケール設定（LC_*）よりも優先して、こちらの設定が使われる   Sys.setlocale(\u0026quot;LC_MESSAGES\u0026quot;, \u0026quot;English\u0026quot;) 現在のロケールを確認する（windows）\n\u0026gt; Sys.getlocale() [1] \u0026#34;LC_COLLATE=Japanese_Japan.932;LC_CTYPE=English_United States.1252;LC_MONETARY=Japanese_Japan.932;LC_NUMERIC=C;LC_TIME=Japanese_Japan.932\u0026#34; Ubuntu でロケールを表示してみた。　ロケールに関連する環境変数はもっと多い\n$ locale LANG=ja_JP.UTF-8 LANGUAGE= LC_CTYPE=\u0026quot;ja_JP.UTF-8\u0026quot; LC_NUMERIC=\u0026quot;ja_JP.UTF-8\u0026quot; LC_TIME=\u0026quot;ja_JP.UTF-8\u0026quot; LC_COLLATE=\u0026quot;ja_JP.UTF-8\u0026quot; LC_MONETARY=\u0026quot;ja_JP.UTF-8\u0026quot; LC_MESSAGES=\u0026quot;ja_JP.UTF-8\u0026quot; LC_PAPER=\u0026quot;ja_JP.UTF-8\u0026quot; LC_NAME=\u0026quot;ja_JP.UTF-8\u0026quot; LC_ADDRESS=\u0026quot;ja_JP.UTF-8\u0026quot; LC_TELEPHONE=\u0026quot;ja_JP.UTF-8\u0026quot; LC_MEASUREMENT=\u0026quot;ja_JP.UTF-8\u0026quot; LC_IDENTIFICATION=\u0026quot;ja_JP.UTF-8\u0026quot; LC_ALL=ja_JP.UTF-8 フォント Windows 環境でフォントを使うときの注意は extrafontパッケージのREADME に詳しく書いてある。\nこちらも悪くない 【ggplot2】 好きなフォントを適用できるようにする\nWindowsでフォントをインストールする\nその後、Rからアクセスできる場所にフォントをインポートする。これはフォントをインストールしたときに1度だけ行う。\nextrafont::font_import()\nR起動するたびに次のコードを実行してRにフォントをロードする\nextrafont::loadfonts(\u0026quot;win\u0026quot;, quiet = TRUE) extrafont::loadfonts(\u0026quot;pdf\u0026quot;, quiet = TRUE) extrafont::loadfonts(\u0026quot;postscript\u0026quot;, quiet = TRUE) しかし、ここまでやっても \u0026lsquo;Roboto Bold\u0026rsquo; などは、windowsの pdf では使えないようだ。 → showtext パッケージ を使えば利用可能になるのだろうか？\npdfデバイスで使えるフォントの一覧\nnames(pdfFonts()) # Vector of font family names fonts() # Show entire table fonttable() きになるパッケージ 可視化  gganimate : gifアニメ export : ggplotオブジェクトをパワポに変換する ggridge 複数のヒストグラム比べるやつ  オブジェクトを調べる class() typeof() mode()\nstr() attributes()\n"},{"idx":3,"href":"/notebook/gis/bigquerygis/","title":"BigQuery GIS","content":"BigQuery GIS BigQuery標準SQLの地理関数\n"},{"idx":4,"href":"/notebook/gis/sar/","title":"SAR","content":"SAR 合成開口レーダ（SAR）のキホン～事例、分かること、センサ、衛星、波長～\n合成開口レーザー（SAR:Synthetic Aperture Radar）は、マイクロ波を発射し、地表で跳ね返ってきたマイクロ波をとらえるセンサ\nざらざらした表面ほど多く電波が返ってきて白くみえ、水面などつるつるした表面では電波が反射してしまうため黒く見えます。\n観測周波数（バンド）、周波数の違いによって、何に反射して跳ね返ってくるかが異なる。\nよく用いられるのは Lバンド（1〜2GHz）、 Cバンド(4〜8GHz)、Xバンド(8〜12GHz)の3つで順番に波長が短くなっていきます。\n偏波、偏光と同じで、電磁波も波の傾きがある。物体によって偏波に対する反射特性が異なるので、物体の識別に使える\n衛星に搭載されるセンサーにより、発信する電波の傾き、受信する電波の傾きが異なる\n HH : 水平偏波で電波を出して水平で受けること HV : 水平偏波で電波を出して垂直で受けること VH : 垂直偏波で電波を出して水平で受けること VV : 垂直偏波で電波を出して垂直で受けること  "},{"idx":5,"href":"/notebook/gis/terminology/","title":"用語","content":"用語  Panchromatic PAN 白黒の画像 Multispectral カラー画像 可視光 Visible VIS 近赤外線 Near InfraRed NIR 可視光と近赤外線をまとめて VNIR バンド センサーが感受する電磁波の波長帯域、バンドによって見えるものが違う  センサー  合成開口レーダー SAR VIIRS  "},{"idx":6,"href":"/notebook/gis/viirs/","title":"VIIRS","content":"VIIRS Visible Infrared Imaging Radiometer Suite\n赤外線〜可視光を含む波長の光を受信する22個のセンサーからなる「装置の名称」。様々な波長で観測することにより地表や海洋の色や夜間の光を観測する。雲、エアロゾル、氷、海の色、植生、夜間光などの分析に利用できる。\nSuomi-NPP（SNPP）, NOAA-20 (旧称 JPSS-1) の２つの衛星に搭載されている。どちらの衛星もNASAとNOAAの共同プロジェクト Joint Polar Satellite System (JPSS) により打ち上げられ運用されている。（NOAA-20 と JPSS-1は同じ衛星、JPSS-1 から NOAA-20 に名前が変わった。SNPP は JPSS-1のための実験衛星という位置付けらしいが、現在も運用中。）\nSNPPのデータは2012年から利用可能で、VIIRSを搭載した衛星は今後しばらく（2030以降まで）は運用を続けられる予定。\n衛星 Suomi-NPP  https://www.restec.or.jp/satellite/suomi-npp https://directory.eoportal.org/web/eoportal/satellite-missions/s/suomi-npp  回帰日数（repeat cycle）は 16 日と書いてあるけど、CLASSのデータの範囲を見ると、2017-06-01 と 2017-06-12 はほぼ同じ地点を通過するので、周期は11日ではないのか？？\n準回帰軌道である。\n 太陽同期軌道(Sun-synchronous orbit)でもあるので、太陽同期準回帰軌道と言える。  Sun-synchronous near-circular polar orbit\nS-NPP衛星軌道情報\nNOAA-20 https://www.restec.or.jp/satellite/jpss-series\nVIIRS データ情報 VIIRSポータルサイト\n SNPP VIIRS NASA VIIRS LAND  最初にこれを読む\nBeginner Guide to VIIRS data\nこのドキュメントが、わかりやすくVIIRSのデータの詳細を理解するのに役だちそう\n VIIRS_SDR_Users_Guide v1.3  データプロダクト SNPPのデータは大きく３つに分けられる、一般ユーザーはSDRとEDRを使う\n RDR (Raw Data Records) : センサーから取得された生データ（一般ユーザーには必要ない） SDR (Sensor Data Records) : センサー値を適切に処理して得られた物理量や品質フラグなどのデータ（一般ユーザーにとっての生データ） EDR (Environmental Data Records) : センサーから得られた値をさらに加工して得られたデータ（雲マスク、植生など）  JPSSのサイトに、利用できるデータプロダクトの一覧がある https://www.star.nesdis.noaa.gov/jpss/JPSS_products.php\nデータのドキュメント JPSSの公式サイトにあるSNPPのドキュメント集\nJPSSのドキュメントがDataRefugeにある。 DataRefugeは環境や気候関係のデータのアーカイブを進めている？トランプ大統領が気候変動に関するデータの公開を制限しようとした時に対策として生まれた運動？？\nJPSSのデータフォーマットのドキュメントは Common Data Format Control Book - External (CDFCB-X) と呼ばれているらしい。VIIRSドキュメントのコードは 474 、VIIRSのデータフォーマットのドキュメントコードは 474-00001\n CDFCB-X Volume I: Overview  474-00001-01_JPSS-CDFCB-X-Vol-I_0124D 474-00001-01_jpss-cdfcb-x-vol-i_0200c.pdf こっちのが新しいらしい   CDFCB-X Volume II: Raw Data Record Formats CDFCB-X Volume III: Sensor Data Record/Temperature Data Record Formats CDFCB-X Volume IV: Environmental Data Record/Intermediate Product/Application Related Product Formats  CDFCB-X Volume IV Part 1: Overview, IPs, ARPs, and Common Geolocation Data CDFCB-X Volume IV Part 2: Imagery, Atmospheric, and Cloud EDRs CDFCB-X Volume IV Part 3: Land and Ocean/Water EDRs CDFCB-X Volume IV Part 4: Earth Radiation Budget and Space EDRs   CDFCB-X Volume V: Metadata CDFCB-X Volume VI: Ancillary Data, Auxiliary Data, Messages, and Reports CDFCB-X Volume VII: 廃止 CDFCB-X Volume VIII: Look Up Tables and Processing Coefficients  HDF5に格納されているデータの定義は以下のドキュメントにまとまっている\nJoint Polar Satellite System (JPSS) Algorithm Specification Volume II: Data Dictionary for VIIRS RDR/SDR\nHDF5ファイルに格納されているデータの定義については以下のドキュメントに概要がある\n VIIRS SDR Data format Joint Polar Satellite System (JPSS) Algorithm Specification Volume II: Data Dictionary for VIIRS RDR/SDR.  SDR I-band (SVI01 - SVI05) SVI01, SVI02, SVI03, SVI04, SVI05\n位置情報\n– GIMGO: projected onto smooth ellipsoid (WGS84 ellipsoid) – GITCO: parallax-corrected for terrain\nM-band (SVM01 - SVM16) SVM01, SVM02, SVM03, SVM04, SVM05, SVM06, SVM07, SVM08, SVM09, SVM10, SVM11, SVM12, SVM13, SVM14, SVM15, SVM16\n位置情報\n– GMODO: projected onto smooth ellipsoid – GMTCO: parallax-corrected for terrain\nDay Night Band (SVDNB) 全てのM-band (M1~M16)の波長のセンサーの信号を統合して（恐らく）、最も感度と強さのレンジを高めたプロダクト\nファイル記号 : SVDNB\n位置情報\n– GDNBO: projected onto smooth ellipsoid (as of May 2013 there is a discussion of whether or not to produce a terrain-corrected geolocation)\nEDR EDR User Guide\n2020-06-24 にこれまで作成されていた多くのEDRが作成されなくなり、あたらしい形式のものが提供されるようになった。\n以前のEDRプロダクトと新しいプロダクトの対応表はこちら。\nこれら新しい VIIRS の EDR は JPSS_RR product と呼ばれている？？\n現在、多くの EDR は CLASS の JPSS_GRAN から取得できる。\nドキュメント https://www.nodc.noaa.gov/archivesearch/rest/find/document?searchText=%22gov.noaa.class:JPSS_GRAN%22\u0026amp;max=30\u0026amp;f=searchpage\nI-band EDR I-band のSDR (SVI01, SVI02, SVI03, SVI04, SVI05) はそれぞれ対応するEDR (VI1BO, VI2BO, VI3BO, VI4BO, VI5BO) がある\nVI1BO, VI2BO, VI3BO, VI4BO, VI5BO\nM-band EDR M-band は６つのバンドについて対応するEDRがある\n SVM01 (SDR) \u0026ndash;\u0026gt; VM01O (EDR) reflectance SVM04 (SDR) \u0026ndash;\u0026gt; VM02O (EDR) reflectance SVM09 (SDR) \u0026ndash;\u0026gt; VM03O (EDR) reflectance SVM14 (SDR) \u0026ndash;\u0026gt; VM04O (EDR) brightness temperature SVM15 (SDR) \u0026ndash;\u0026gt; VM05O (EDR) brightness temperature SVM16 (SDR) \u0026ndash;\u0026gt; VM06O (EDR) brightness temperature  M-band EDR のための入力SDRは変更される可能性がある。EDRのメタデータ band_id を見ると、どのSDRが使われたのかわかる\nI-band M-band EDRs は SDR を the Ground-Track Mercator projection に再投影し直したもの\nDNB EDR DNBのEDRは Near-Constant Contrast (NCC) product と呼ばれる\nファイル記号 : VNCCO\n局所的な明るさを標準化して、夜と昼にかかわらず、光の反射率（reflectance）の分布を可視化する （理想的には、夜と昼にかかわらず同じような画像を出力するはずだが、技術的には難しい）\nDNB の EDRは、SDRの輝度を反射率に変換して、ground-track Mercator projection に再投影したもの、\nつまり、DNB の EDRは反射率 (reflectance) しか持っていない、DNB の SDR は輝度しか持っていない\n輝度（radiance）\n位置情報\n– GIGTO: I-band EDR geolocation – GMGTO: M-band EDR geolocation – GNCCO: Day/Night Band EDR (NCC) geolocation\nThe cloud mask is 1km resolution and contains probability of cloud for each pixel (I used discretized cloud mack, though.) I wish I had VIIRS data in relational database. To be honest, I\u0026rsquo;m tired of dealing with the raw data.\nCloud Mask  JRR-CloudMask   アルゴリズム改良版: Enterprize Cloud Mask, ECM\n  期間 (xxxx-xx-xx ~ 現在)\n  取得 https://www.avl.class.noaa.gov/saa/products/search?sub_id=0\u0026amp;datatype_family=JPSS_GRAN\u0026amp;submit.x=34\u0026amp;submit.y=0\n  データ形式: NetCDF (Version: 4)\n  リンク\n NOAA JPSS の CLOUD チーム（アルゴリズムの開発元） VIIRS EDRについてのポータル CLOUD MASKの可視化    ドキュメント\n  External User Manual (EUM): NetCDFファイル内部に格納されているデータについて記載\n https://www.star.nesdis.noaa.gov/jpss/documents/UserGuides/JPSS_RR_EUM.pdf https://www.ospo.noaa.gov/Products/Suites/files/JPSS_RR_EUM_June2016.pdf    Algorithm Theoretical Basis Document (ATBT) データの意味についてはこちら\n すべてのVIIRS EDRのATBTへのリンク Enterprize Cloud Mask の ATBT    プレゼン資料\n [How to Use the NOAA Enterprise Cloud Mask (ECM)]https://www.star.nesdis.noaa.gov/jpss/documents/meetings/2015/SJASTM/Session7e-Clouds+Aerosol.pdf) Enterprise Cloud Mask (ECM): Data format basics -　メモ    雲マスクのアルゴリズムは ECM (Enterprize Cloud Mask) と呼ばれている。VIIRSの他のセンサーに対しても適用できるように作られている（VIIRS, MODIS, AVHRR, GOES, ABI, SEVIRI, AHI, COMS, など）。使用するバンドはセンサーにより異なる。\n    参考リンク\n [ABI CLOUD MASK (ACM)、ECMとは異なるクラウドマスク（ECMのもとになっている？？）] (https://www.star.nesdis.noaa.gov/goesr/docs/ATBD/Cloud_Mask.pdf) こちらには CloudMaskQualFlag の説明が載っているが、ECMと同じなのかは不明     VICMO  EDRに格上げされた。VIIRS CLOUD MASK (VCM) (2017-03-08 ~ 2020-06-24) https://www.bou.class.noaa.gov/saa/products/search?sub_id=0\u0026amp;datatype_family=VIIRS_EDR\u0026amp;submit.x=24\u0026amp;submit.y=3 データ形式: HDF5 JPSS Algorithm Specification, Volume II: Data Dictionary for the Cloud Mask 474-00448-02-11_JPSS-DD-Vol-II-Part-11_0200E   IICMO  中間プロダクト Cloud Mask Intermediate Product (2012-05-02 ~ 2017-03-08) https://www.bou.class.noaa.gov/saa/products/search?sub_id=0\u0026amp;datatype_family=VIIRS_IPNG\u0026amp;submit.x=29\u0026amp;submit.y=8 データ形式: HDF5 IICMOのドキュメント    NetCDF4ファイルをRで読み込んでメタデータを表示する\npath_ncdf \u0026lt;- \u0026#34;JRR-CloudMask_v1r1_npp_s201804161754397_e201804161756039_c201804162011060.nc\u0026#34; nc \u0026lt;- ncdf4::nc_open(path_ncdf) print(nc) NetCDF4ファイルに格納された値の取得方法\n# 変数の値を取得 CloudMask \u0026lt;- ncdf4::ncvar_get(nc, \u0026#34;CloudMask\u0026#34;) # グローバル属性の取得 start_orbit_number \u0026lt;- ncdf4::ncatt_get(nc, 0, attname = \u0026#34;start_orbit_number\u0026#34;)$value メタデータの表示例\n主要な変数\n Latitude 緯度 Longitude 経度 CloudMask クラウドマスクの値 0,1,2,3, _FillValue: -128 CloudMaskPacked ビットごとにATBTの  chunkingは各要素のデータサイズ？単位はbite?\nFile JRR-CloudMask_v1r1_npp_s201804161754397_e201804161756039_c201804162011060.nc (NC_FORMAT_NETCDF4): 30 variables (excluding dimension variables): float Latitude[Columns,Rows] (Chunking: [200,256]) (Compression: shuffle,level 2) long_name: Latitude units: degrees_north comments: Pixel latitude in field Latitude (degree) _FillValue: -999 valid_range: -90 valid_range: 90 float Longitude[Columns,Rows] (Chunking: [200,256]) (Compression: shuffle,level 2) long_name: Longitude units: degrees_east comments: Pixel longitude in field Longitude (degree) _FillValue: -999 valid_range: -180 valid_range: 180 int StartRow[] (Contiguous storage) long_name: Start row index int StartColumn[] (Contiguous storage) long_name: Start column index byte CloudMask[Columns,Rows] (Chunking: [200,256]) (Compression: shuffle,level 2) long_name: Cloud Mask coordinates: Longitude Latitude units: 1 _FillValue: -128 valid_range: 0 valid_range: 3 byte CloudMaskBinary[Columns,Rows] (Chunking: [200,256]) (Compression: shuffle,level 2) long_name: Cloud Mask Binary coordinates: Longitude Latitude units: 1 _FillValue: -128 valid_range: 0 valid_range: 1 byte CloudMaskPacked[CldMaskPkedCnst,Columns,Rows] (Chunking: [7,200,256]) (Compression: shuffle,level 2) long_name: Diagnostic Cloud Mask coordinates: Longitude Latitude units: 1 _FillValue: -128 valid_range: -128 valid_range: 127 byte CloudMaskFlag[FlagConst,Columns,Rows] (Chunking: [33,200,256]) (Compression: shuffle,level 2) long_name: Cloud Mask Test coordinates: Longitude Latitude units: 1 _FillValue: -128 valid_range: -128 valid_range: 127 byte Smoke_Mask[Columns,Rows] (Chunking: [200,256]) (Compression: shuffle,level 2) long_name: Smoke Mask coordinates: Longitude Latitude units: 1 _FillValue: -128 valid_range: 0 valid_range: 3 byte Fire_Mask[Columns,Rows] (Chunking: [200,256]) (Compression: shuffle,level 2) long_name: Fire Mask coordinates: Longitude Latitude units: 1 _FillValue: -128 valid_range: 0 valid_range: 3 byte Dust_Mask[Columns,Rows] (Chunking: [200,256]) (Compression: shuffle,level 2) long_name: Dust Mask coordinates: Longitude Latitude units: 1 _FillValue: -128 valid_range: 0 valid_range: 3 byte CloudMaskQualFlag[Columns,Rows] (Chunking: [200,256]) (Compression: shuffle,level 2) long_name: Cloud Mask Quality Flag coordinates: Longitude Latitude units: 1 _FillValue: -128 valid_range: 0 valid_range: 6 float CloudProbability[Columns,Rows] (Chunking: [200,256]) (Compression: shuffle,level 2) long_name: Cloud Probability coordinates: Longitude Latitude units: 1 _FillValue: -999 valid_range: 0 valid_range: 1 float ClearProbClear[] (Contiguous storage) long_name: Percent of Clear and Probably Clear units: % valid_range: 0 valid_range: 100 int NumOfQualityFlag[] (Contiguous storage) long_name: Number of quality flag units: 1 float Cloudy[] (Contiguous storage) long_name: Percent of Pixels that passed a test for cloud and failed a test for cloud edge units: % valid_range: 0 valid_range: 100 float ProbCloudy[] (Contiguous storage) long_name: Percent of Pixels that passed a test for cloud and passed a test for cloud edge units: % valid_range: 0 valid_range: 100 float ProbClear[] (Contiguous storage) long_name: Percent of Pixels that passed no test for cloud but passed tests for spatial heterogenity units: % valid_range: 0 valid_range: 100 float Clear[] (Contiguous storage) long_name: Percent of Pixels that passed no test for cloud and failed a test for spatial heterogenity units: % valid_range: 0 valid_range: 100 int TotalPixel[] (Contiguous storage) long_name: Total Number of pixels units: 1 float TerminatorPixPercent[] (Contiguous storage) long_name: Percent of terminator pixels units: % valid_range: 0 valid_range: 100 int TotalCloudMaskPixel[] (Contiguous storage) long_name: Total Number of cloud Mask pixels units: 1 float MinClrSkyOBS_RTM[Meta10] (Contiguous storage) long_name: Minimum observation - RTM for Clear Sky IR Channel 07 to 16 coordinates: units: Kelvin _FillValue: 0 float MaxClrSkyOBS_RTM[Meta10] (Contiguous storage) long_name: Maximum observation - RTM for Clear Sky IR Channel 07 to 16 coordinates: units: Kelvin _FillValue: 0 float MeanClrSkyOBS_RTM[Meta10] (Contiguous storage) long_name: Mean observation - RTM for Clear Sky IR Channel 07 to 16 coordinates: units: Kelvin _FillValue: 0 float StdDevClrSkyOBS_RTM[Meta10] (Contiguous storage) long_name: Std Dev observation - RTM for Clear Sky IR Channel 07 to 16 coordinates: units: Kelvin _FillValue: 0 float MinAllSkyOBS_RTM[Meta10] (Contiguous storage) long_name: Minimum observation - RTM for All Sky IR Channel 07 to 16 coordinates: units: Kelvin _FillValue: 0 float MaxAllSkyOBS_RTM[Meta10] (Contiguous storage) long_name: Maximum observation - RTM for All Sky IR Channel 07 to 16 coordinates: units: Kelvin _FillValue: 0 float MeanAllSkyOBS_RTM[Meta10] (Contiguous storage) long_name: Mean observation RTM for All Sky IR Channel 07 to 16 coordinates: units: Kelvin _FillValue: 0 float StdDevAllSkyOBS_RTM[Meta10] (Contiguous storage) long_name: Std Dev observation - RTM for All Sky IR Channel 07 to 16 coordinates: units: Kelvin _FillValue: 0 5 dimensions: Columns Size:3200 [1] \u0026quot;vobjtovarid4: **** WARNING **** I was asked to get a varid for dimension named Columns BUT this dimension HAS NO DIMVAR! Code will probably fail at this point\u0026quot; Rows Size:768 [1] \u0026quot;vobjtovarid4: **** WARNING **** I was asked to get a varid for dimension named Rows BUT this dimension HAS NO DIMVAR! Code will probably fail at this point\u0026quot; CldMaskPkedCnst Size:7 [1] \u0026quot;vobjtovarid4: **** WARNING **** I was asked to get a varid for dimension named CldMaskPkedCnst BUT this dimension HAS NO DIMVAR! Code will probably fail at this point\u0026quot; FlagConst Size:33 [1] \u0026quot;vobjtovarid4: **** WARNING **** I was asked to get a varid for dimension named FlagConst BUT this dimension HAS NO DIMVAR! Code will probably fail at this point\u0026quot; Meta10 Size:10 [1] \u0026quot;vobjtovarid4: **** WARNING **** I was asked to get a varid for dimension named Meta10 BUT this dimension HAS NO DIMVAR! Code will probably fail at this point\u0026quot; 34 global attributes: Conventions: CF-1.5 Metadata_Conventions: CF-1.5, Unidata Dataset Discovery v1.0 standard_name_vocabulary: CF Standard Name Table (version 17, 24 March 2011) project: S-NPP Data Exploitation institution: DOC/NOAA/NESDIS/NDE-\u0026gt;S-NPP Data Exploitation, NESDIS, NOAA, U.S. Department of Commerce naming_authority: gov.noaa.nesdis.nde satellite_name: NPP instrument_name: VIIRS title: JPSS Risk Reduction Unique Cloud Mask summary: Cloud Mask history: VIIRS Cloud Mask Version 1.0 processing_level: NOAA Level 2 references: id: c1b243ad-cae9-46a1-a416-f972c10ea5e2 Metadata_Link: JRR-CloudMask_v1r1_npp_s201804161754397_e201804161756039_c201804162011060.nc start_orbit_number: 33516 end_orbit_number: 33516 day_night_data_flag: night ascend_descend_data_flag: 1 time_coverage_start: 2018-04-16T17:54:39Z time_coverage_end: 2018-04-16T17:56:03Z date_created: 2018-04-16T20:11:06Z cdm_data_type: swath geospatial_first_scanline_first_fov_lat: 50.7433967590332 geospatial_first_scanline_last_fov_lat: 44.5372085571289 geospatial_last_scanline_first_fov_lat: 45.7234344482422 geospatial_last_scanline_last_fov_lat: 40.0141525268555 geospatial_first_scanline_first_fov_lon: 101.341705322266 geospatial_first_scanline_last_fov_lon: 141.848983764648 geospatial_last_scanline_first_fov_lon: 101.343467712402 geospatial_last_scanline_last_fov_lon: 138.53092956543 geospatial_lat_units: degrees_north geospatial_lon_units: degrees_east geospatial_bounds: POLYGON((101.342 50.7434,141.849 44.5372,138.531 40.0142,101.343 45.7234,101.342 50.7434)) 生データ取得 ウィスコンシン 直近１週間分のデータ\nftp://ftp.ssec.wisc.edu/pub/eosdb/npp/viirs/\nNOAA CLASS ウェブサイトからSDR, EDRの生データを取得できるが、APIで自動取得することができない\u0026hellip;\n 直近90日分のデータは　NOAAのFTPサイトから入手できる VIIRS SDR VIIRS EDR 古いバージョン VIIRS EDR 新しいバージョン  NASA LAADS NOAA CLASS 同様のSDR/EDRが入手できるが、どうも何らかの処理が加えられているように見える。ただスクレイピングによりデータ取得を自動化できる。\n NASA LAADS VIIRS  VIIRS:Suomi-NPP VIIRS CLOUD MASK    データプロダクト・サービス  Cololad School of Mines: Earth Observation Group : VIIRS  VIIRSを利用した、夜間の焱、夜間光（月次、年次）、夜間の漁船光の検出のデータを提供している 元々はNOAAのグループだったが Cololad School of Mines に移籍した。NOAA時代のサイト https://eogdata.mines.edu/wwwdata/viirs_products/vbd/v23/global-saa/current/ https://eogdata.mines.edu/wwwdata/viirs_products/vbd/v23/global-saa/nrt/    https://payneinstitute.mines.edu/eog/viirs/\nVIIRS を利用した論文 夜間の漁船光の検出  Automatic Boat Identification System for VIIRS Low Light Imaging Data Cross-Matching VIIRS Boat Detections with Vessel Monitoring System Tracks in Indonesia  一般記事 衛星が撮影した夜の地球「夜間光」がお金に変わる!? 概要と利用事例\n"},{"idx":7,"href":"/notebook/datascience/","title":"Data Science","content":"Data Science データ作成 drawdata.xyz\n手でグラフをお絵描きして対応するデータをダウンロードできる\n精度指標 分類モデル 混合行列\nhttps://en.wikipedia.org/wiki/Confusion_matrix\nページの右に色々カテゴリ変数のための精度指標のリストがある\nhttps://en.wikipedia.org/wiki/Receiver_operating_characteristic\n"},{"idx":8,"href":"/notebook/gis/","title":"GIS \u0026 Remote Sensing","content":"GIS \u0026amp; Remote Sensing 情報収集 ポータルサイト  JAXA 衛星利用推進サイト 宙畑 リモートセンシング技術センター  団体コミュニティ  Open Geospatial Consortium 地理情報関係の標準化団体 OSGeo.jp OSGeo4W WindowsのためにOSSのGIS関連ソフトをビルドしているらしい  学習サイト GIS  GIS基礎知識 Geocomputation with R Rでの地理情報解析の方法がめっちゃ充実、sfパッケージベース Spatial Data Science Spatio-Temporal Statistics with R Introduction to visualising spatial data in R 空間情報クラブ  CRS  PROJで利用できるCRSのリスト 地図投影法のリスト RSpatialGuides/OverviewCoordinateReferenceSystems どんな目的、スケールでどの投影法を使うかガイド 地域ごとの適切な投影法の検索エンジン？  リモセン  衛星観測の専門用語  データソース  STEP  ESAのポータルサイトらしい   MarineRegions.org 海洋関係の Shapefile を公開している GeoServer GeoNetwork Natural Earth 国土数値情報 OCEAN DATA VIEWER 気象データ高度利用ポータルサイト GISデータ入手先一覧  衛星データ  NOAA CLASS -NOAA CLASS Data access tutrial LAADS DAAC G-Portal  宇宙開発組織  NASA NOAA JAXA ESA  ファイル形式  GeoJSON WKT, WKB Shapefile PostGIS GeoTiff HDF5  衛星  衛星総覧 気象変動観測衛星しきさい Suomi-NPP JPSS1(NOAA20)  センサー  VIIRS SAR  衛星情報解析サービス  Google Earth Engine Tellus  企業  Orbital Insight  地図サービス JapanMapCompare\n日本の地図サービスの画像を比較できる\n"},{"idx":9,"href":"/notebook/gis/r_package/","title":"GIS関連Rパッケージ","content":"GIS関係のRパッケージの一覧  https://cran.r-project.org/web/views/Spatial.html https://cran.r-project.org/web/views/SpatioTemporal.html https://www.r-spatial.org/  一般   sp：地理空間のクラスやメソッドを提供している\n  sf : OGC Simple Featuresという標準に従って開発されている。sfパッケージの公式サイト、OGCが定義するファイル形式の読み書き（WKT, WKB）\n  starts : 多次元arrayで時空間データを表現\n  raster : ラスターデータの可視化と分析\n  spatial.tools : rasterを拡張して並列計算など\n  rgdal : GDALライブラリがサポートするラスターデータ, OGRライブラリがサポートするベクターデータへのバインディング、GeoJSON や Shapefile の読み書き、 PROJ.4がサポートする投影法\n  rgeos : Geometry Engine - Open Source (\u0026lsquo;GEOS\u0026rsquo;)ライブラリへのバインディング\n  GISTools : 空間データをマッピングしたり加工したり\n  metR : 気象関係のデータの扱いを容易にするパッケージらしい\n  データの読み書き  maps : mapdata, mapproj パッケージと共に地理空間データベースを提供 maptools : ESRI ArcGIS/ArcView shapefiles の読み書きなど、地理情報オブジェクトの扱い、GSHHGの海岸線データベースへのアクセス？？ shapefiles : ESRI ArcGIS/ArcView shapefiles の読み書き rpostgis : PostGISが有効化されたPostgreSQLへのコネクション rgrass7 : GRASS v7 とのコネクション RQGIS : RからQGISの機能を利用する  データの取得  geosapi : GeoServerのAPIからデータを取得する？ geonapi : GeoNetworkのAPIからデータを取得する？ rgbif : Global Biodiversity Information Facility (GBIF) にあるデータへのアクセス rnaturalearth : よく使う地図データをRからすぐにアクセスできるようにする sp, sf オブジェクトで提供  計算  lwgeom : PostGISで使われている liblwgeom へのバインディング gdalUtils : GDAL Utilityへのラッパー？ gdistance : grid間の距離やルートを計算する？ geosphere : 距離、面積、角度などの計算？ cshapes : 距離行列の計算 spsurvey : 地理的調査のためのサンプリング手法など？ trip : 動物のトラッキングデータ分析 magclass : 時空間解析 taRifx : ユーティリティ関数 geoaxe : オブジェクトを小さく分割する lawn : Turfjs ブラウザの地理空間解析ライブラリのクライアント rcosmo : 球体に対する計算を提供 areal : Areal Weighted Interpolation の実装  データのエラー調査  landsat : 衛星画像の補正？ cleangeo : 空間オブジェクトのエラーの調査？  カラーパレット  RColorBrewer : Mapでいい感じのカラーパレットを提供 viridis : 視覚障害者にも優しいカラーパレットを提供 classInt : １次元の変数の値を階級に分けるのに使う？？  可視化  ggspatial : ggplot2で可視化する時の便利ツール rasterVis : rasterの可視化 quickmapr : sp, sfオブジェクトをとりあえず可視化できる cartography : いろんな地図作成？ mapmisc : 軽量な地図作成？  可視化ウェブ  mapView, leaflet, leafletR インタラクティブな地図の可視化 RgoogleMaps : Googleマップに問い合わせしたり、Googleマップの画像を背景にしたプロットを描く plotGoogleMaps : Googleマップに描画する plotKML : オブジェクトをKMLで書き出してGoogle Earthで読み込めるようにする？？ ggmap : Google MapやOpen Street Map に描画する？ mapedit : Shinyでleafletで書いた地図を編集できるようにする？？  Cartograms 地図を変形させる可視化？ cartogram : 面積を風船のように伸び縮みさせる\nPoint Pattern Analysis  spatial, spatstat spatgraphs : 点のパターンのグラフ解析  空間統計  gstat, geoR, geoRglm : 統計量、統計モデル？ vardiag : Variogram を書く  Disease mapping and areal data analysis https://cran.r-project.org/web/views/Spatial.html\nSpatial regression https://cran.r-project.org/web/views/Spatial.html\nEcological analysis https://cran.r-project.org/web/views/Spatial.html\n"},{"idx":10,"href":"/notebook/mac/","title":"Mac","content":"Mac 便利なツール QuickLookプラグイン 以下のGithubレポジトリに開発者にとって便利な様々なQuickLookプラグインが紹介されている。どれも homebrew でインストールできるので簡単\nsindresorhus/quick-look-plugins\n"},{"idx":11,"href":"/notebook/maritime/","title":"Maritime","content":"海事関係 リンク  Asia Maritime Transparency Initiative: 各国  I think it\u0026rsquo;s good practice to keep code, data and the resulting documents in the same place,　and keep always working in that folder.\n"},{"idx":12,"href":"/notebook/gcp/bigquery_gis/","title":"BigQuery GIS","content":"BigQuery GIS 標準 SQL の地理関数\n地理オブジェクト作成 ST_GEOGPOINT(longitude, latitude) ST_CONTAINS ST_CONTAINS(geography_1, geography_2) geography_2 は geography_1 の内部にある（外にはみ出していない）なら TRUE\nST_WITHIN ST_WITHIN(geography_1, geography_2) geography_1 は geography_2 の内部にある（外にはみ出していない）なら TRUE\nST_CONTAINS とは geography_1 と geography_2 の順序が逆になっている\n"},{"idx":13,"href":"/notebook/gcp/bigquery_bq/","title":"BigQuery: bqコマンド","content":"BigQuery：bqコマンド Data set を作成する bq mk \\ --dataset \\ --location=US \\ --default_table_expiration 3600 \\ --default_partition_expiration 3600 \\ --description 'description' \\ project_id:dataset_name  location US など default_table_expiration テーブル自動削除までの秒数を指定する、デフォルトでは 3600 、0に設定すると自動削除しない default_partition_expiration テーブルに対するパーティションの自動削除までの秒数 description データセットの説明、' か \u0026quot; で括る project_id:dataset_name プロジェクトIDと、作成するデータセットの名前  クエリを実行する  bq query 'select * from hoge'  -n 0 : クエリの実行結果がひょゆ準出力されるのを抑制する  データをアップロードする ローカルファイル（.parquet）から\nbq load \\ --source_format=PARQUET \\ --autodetect \\ --replace=TRUE\\ PROJECT:DATASET.TABLE \\ LOCAL_FILE.parquet GCSから取り込み\nbq load \\ --source_format=PARQUET \\ PROJECT:DATASET.TABLE \\ \u0026quot;gs://mybucket/00/*.parquet\u0026quot;,\u0026quot;gs://mybucket/01/*.parquet\u0026quot; テーブルを削除する bq rm --table project_id:dataset.table  --table または -t でテーブルを削除 --force または -f は確認を省略する  複数のテーブルをまとめて削除する 特定のデータセット内の、名前が特定のパターン pattern に合致するテーブルを一括で削除する\nbq ls -a project_id:dataset_id | grep pattern | xargs -n 1 bq rm -t -f --project_id project_id --dataset_id dataset_id パーティションドテーブルを作成する bq mk --time_partitioning_type=DAY パーティションドテーブルのパーティションを指定してクエリ結果を書き込む # 日付でパーティションされたテーブルの2020-01-01のパーティションを置き換える `--replace` bq query --replace --destination_table=dataset.table$20200101 'SELECT * FROM hoge' "},{"idx":14,"href":"/notebook/gcp/bigquery/","title":"BigQuery: general","content":"BigQuery WebUI キーボードショートカット Mac の場合は Ctrl の代わりに Cmd にする\n   キー アクション     Ctrl Enter 現在のクエリを実行   Tab 現在の単語をオートコンプリート   選択 Tab 選択範囲のインデントを上げる   選択 Shift Tab 選択範囲のインデントを下げる   Ctrl テーブル名をハイライト表示   Ctrl テーブル名をクリック テーブル スキーマを開く   Ctrl E 選択項目からクエリを実行   Ctrl / 選択行をコメントアウト   Ctrl Shift F クエリを書式設定    Web IU のキーボードショートカット\n"},{"idx":15,"href":"/notebook/gcp/bigquery_sql/","title":"BigQuery: SQL","content":"BigQuery: SQL SQL レギュラーSQLをデフォルトにする クエリの前に #standardSQL の記述を追加する。逆にレガシーにしたい場合は #legacySQL を記述する。\n#standardSQL SELECT weight_pounds, state, year, gestation_weeks FROM `bigquery-public-data.samples.natality` ORDER BY weight_pounds DESC LIMIT 10; .bigqueryrc に以下を記述\n[query] --use_legacy_sql=false [mk] --use_legacy_sql=false 基本文法  標準SQLの演算子 標準SQLの関数と演算子  実数を丸める関数  ROUND() CEILING() FLOOR() TRUNC()  丸め関数の挙動\n行の抽出 WHERE column IN( sub_query ) 列の値が、サブクエリの出力結果と同じ値をもつ行だけを抽出する\nSELECT * FROM tableA WHERE item IN ( SELECT product FROM tableB ) WHERE EXISTS( sub_query ) サブクエリのなかのwhere句で抽出したいレコードの条件を指定する。\nSELECT * FROM tableA WHERE EXISTS ( SELECT product FROM tableB WHERE tableA.item = tableB.product AND tableA.price = tableB.price ) カラム名を取得する SELECT column_name FROM dataset_name.INFORMATION_SCHEMA.COLUMNS WHERE table_name = 'table_name' カラム名を取得して、INSERT SELECT 用の文字列を生成する\nWITH ColumnNames AS ( SELECT column_name FROM scratch_masaki.INFORMATION_SCHEMA.COLUMNS WHERE table_name = 'viirs_ais_matching_20200101_masaki' ) SELECT CONCAT( 'INSERT dataset.tablename (', ARRAY_TO_STRING(ARRAY(SELECT column_name FROM ColumnNames), ', '), ')'); arrayを展開する LEFT JOIN UNNEST を使用する\n下の例では ais_identity.n_imo が array\nこれをすると、 ais_identity.n_imo is null の行は除外される\nselect ssvid, best.best_flag, ais_identity.flag_mmsi, best.best_vessel_class, ais_identity.n_shipname, ais_identity.n_callsign, n_imo.value as n_imo_value, n_imo.count as n_imo_count, n_imo.type as n_imo_type from `world-fishing-827.gfw_research.vi_ssvid_v20200801` LEFT JOIN UNNEST(ais_identity.n_imo ) as n_imo "},{"idx":16,"href":"/notebook/miscellaneous/english/","title":"english","content":"English 数学  数学用語 英語 一覧 Exponents(指数)の英語表現  "},{"idx":17,"href":"/notebook/miscellaneous/jinjia2/","title":"jinja","content":"jinja2 jinja2 はテンプレートエンジン\nテンプレートエンジンとは、要するに、テンプレートの一部を、別の変数の値で置き換えて、新しいドキュメント（htmlとか）を作成するためのツール。 webページの生成とかにも使われているらしい。\njinja2は、コマンドラインツールとしても、pythonパッケージとしても存在する。\nインストール コマンドラインツールのインストール\n pip install jinja2-cli 使い方 Rの glueパッケージと同様に、テンプレートの中で {{変数}} で記述した部分が変数の値で置き換わる。\n"},{"idx":18,"href":"/notebook/miscellaneous/jupyter/","title":"jupyter","content":"Visual Studio Code ショートカット jupyter の起動\n 起動しているサーバーのリスト jupyter notebook list "},{"idx":19,"href":"/notebook/miscellaneous/pc/","title":"PC","content":"PC CPU 4Gamer: Zen 3アーキテクチャ採用の新世代CPUはゲームにおける性能が大きく向上した: Ryzen 9 5900X, Ryzen 7 5800X\nメモリ規格 見れば全部わかるDDR4メモリ完全ガイド、規格からレイテンシ、本当の速さまで再確認\nオーバークロックメモリの基本と実際の性能、見れば全部わかるDDR4メモリ完全ガイド\nsigle rank, dual rank\nhttp://blog.livedoor.jp/ocworks/archives/52125318.html\nDDR4 メモリ規格\nDIMM デスクトップ用メモリ、 SO-DIMM ノートパソコン用メモリ\nRDIMM/UDIMM メモリにレジスタを備えているか、いないか、RDIMMはサーバー用（マザボ）、一般的なやつはUDIMM\nECC/non-ECC メモリのエラーチェック機能、CPUやマザボ側も対応している必要がある。基本的にはサーバー向けの機能\nメモリクロック DDR4−3200 = 3200 MHz データ転送レート クロック信号の周波数(DRAM frequency) ＝データ転送レートの半分 1600 MHz\nメモリの帯域幅 PC4-25600 １秒間に転送可能なデータ量 25600 MB/s\nメモリタイミング、レイテンシ 16-18-18-38 先頭の数字 CASレイテンシ CL16と表記されることもある CL16 = メモリコントローラの要求に対してメモリが応答するまで16クロック分の時間がかかる 小さい方が早い\nメモリクロックは1600 MHz\n1クロック = 1/1600,000,000 秒 16クロックは 16/1600,000,000 = 1/100,000,000 秒 = 10ns\nメモリランク チップ構成、シングルランク、デュアルランク、シングルの方が良いらしい\nSPDとXMP SPDとは、Serial Presence Detect 2666MHz 1.20V メモリの仕様をシステム側に伝えるための仕組み、何もしないとこのUEFI側はこの仕様でメモリを動作させる\nメモリがXMPに対応していると、UEFIででXMPを有効化すると、自動でXMPデータに基づく動作設定（オーバークロック設定）が適用される\nSPDは標準仕様に準拠した設定（デフォルトではこちらの設定で動作する）、XMPを有効にするとオーバークロック設定で動作する XMPはインテルの技術だが、AMDマザボでもマザボメーカーの努力により利用可能\n部品   X570 AORUS ELITE (rev. 1.0)\n  F4-3200C16Q-128GVK\n  Ryzen 5900X\n  ZOTAC GAMING GeForce RTX 3080 Trinity\n  plextor px-512m9pey\n  Kingston SSD A2000 1000GB 1TB M.2 2280 NVMe PCIe 3D TLC\n  Intel SSD 660p Series [M.2 PCI-E SSD 1TB]\n  CPU : AMD Ryzen 9 3900X Matisse [3.8GHz/12Core/TDP105W] 搭載モデル（標準構成価格220,620円） CPUグリス: Noctua NT-H1 [マイクロ粒子ハイブリッド化合物、高熱伝導率タイプ]（+2,040円） CPU-FAN : サイコムオリジナルAsetek 670LS + Enermax UCTB12P x2 [240mm水冷ユニット] ※メンテナンスフリー（+7,460円） MOTHER : GIGABYTE X570 AORUS ELITE [AMD X570chipset]（標準） MEMORY : 64GB[16GB*4枚] DDR4-3200 [メジャーチップ・8層基板] Dual Channel（+28,550円） READER : なし（標準） HDD/SSD : Intel SSD 660p Series [M.2 PCI-E SSD 1TB]（+8,730円） SSD-Option: なし（標準） HDD/SSD2 : TOSHIBA MD05ACA800 [高信頼 8TB 7200rpm 128MB]（+26,330円） OptDrive : なし（-2,050円） VGA : なし（Ryzen はオンボードグラフィック非対応）（-44,900円） ExCard : オンボードサウンド（標準） LAN : Gigabit LAN [1000BASE T] オンボード CASE : 【黒】CoolerMaster CM694（標準） CASE-Option: なし（標準） POWER : SilverStone SST-ST75F-GS V3 [750W/80PLUS Gold]★フルモジュラー高品質電源がお買い得！★（標準） OS : Microsoft(R) Windows10 Home (64bit) DSP版（標準） "},{"idx":20,"href":"/notebook/miscellaneous/vscode/","title":"vscode","content":"Visual Studio Code ショートカット ショートカットエディタ Preferences \u0026gt;\u0026gt; Keyboard Shortcuts\n【Mac版】 VSCode キーボードショートカット\n コマンドパレット shift cmd P タブの移動 opt cmd → 単語単位でカーソル移動 opt →  正規表現検索・置換 検索置換のショートカット opt cmd F\n検索BOXの横の .* ボタンを押す\n正規表現の記号    記号 意味 例     ^ 行頭    $ 行末    . 任意の１文字    .* 任意の１文字が0文字以上連続する    .+ 任意の１文字が１文字以上連続する    [] []内の任意の１文字 [ABC]+ AまたはBまたはCが1文字以上連続する    検索で認識された文字列を置換の際に使う 検索BOXで正規表現を()でくくり、置換BOXの中で$1や$2で指定する。\n検索BOX： 「(.+)と(.+)」\n置換BOX： 【$1】が【$2】\n"},{"idx":21,"href":"/notebook/miscellaneous/windows/","title":"Windows","content":"Windows ディスクのマウント/アンマウント コマンドプロンプトを管理者権限で起動し、以下のコマンド\n# マウント mountvol D:\\ \\\\?\\Volume{b57198fe-cc68-4509-b771-f81d0fdae196}\\ # アンマウント（Dドライブをアンマウントする） mountvol D:\\ /P b57198fe-cc68-4509-b771-f81d0fdae196 の部分はボリューム名というらしい、mountvol コマンドをただ打つとヘルプと現在認識されているディスクのボリューム名が一覧で表示される。\nPS C:\\Users\\hoge\u0026gt; mountvol ボリューム マウント ポイントを作成、削除、一覧を表示します。 MOUNTVOL [ドライブ:]パス ボリューム名 MOUNTVOL [ドライブ:]パス /D MOUNTVOL [ドライブ:]パス /L MOUNTVOL [ドライブ:]パス /P MOUNTVOL /R MOUNTVOL /N MOUNTVOL /E MOUNTVOL drive: /S パス マウント ポイントを常駐させる既存の NTFS ディレクトリ を指定します ボリューム名 マウント ポイントのターゲットとなるボリューム名を指定しま す。 /D 指定されたディレクトリからボリューム マウント ポイント を削除します。 /L 指定されたディレクトリのマウントされているボリューム の一覧を表示します。 /P 指定されたディレクトリからボリューム マウント ポイントを削除 してボリュームをマウント解除し、ボリュームをマウントできな くします。 ボリューム マウント ポイントを作成して、もう一度ボリュームを マウントできるようにします。 /R システムに存在しないマウント ポイント ディレクトリとレジストリ 設定を削除します。 /N 新しいボリュームの自動マウントを無効にします。 /E 新しいボリュームの自動マウントを再び有効にします。 /S EFI システム パーティションを与えられたドライブにマウントします。 現在のマウント ポイントとボリューム名の考えられる値: \\\\?\\Volume{d2265820-bb12-01d6-1028-1369895deb00}\\ E:\\ \\\\?\\Volume{b57198fe-cc68-4509-b771-f81d0fdae196}\\ D:\\ \\\\?\\Volume{7f4f03ed-23d7-489e-81dc-1333ac3da622}\\ F:\\ \\\\?\\Volume{76a0f51c-c474-4ca5-a2ba-9d469890533e}\\ *** マウント ポイントなし *** \\\\?\\Volume{0d94bb6d-8073-4afe-a5ad-2ea4f4477814}\\ C:\\ \\\\?\\Volume{9b4800bb-ee55-4d7b-8d43-d27a508cb4b9}\\ H:\\ \\\\?\\Volume{0c910d1b-3fd3-4197-ab02-c4bc40f08a75}\\ G:\\ \\\\?\\Volume{9b35ef88-f1bc-4843-8386-aa63f8dd4f0f}\\ *** マウント ポイントなし *** \\\\?\\Volume{9d84eb26-32f1-11eb-acb3-001bdc0e914c}\\ I:\\ "},{"idx":22,"href":"/notebook/mac/homebrew/","title":"homebrew","content":"homebrew 基本コマンド brew search [TEXT|/REGEX/]\nbrew info [FORMULA...]\nbrew install FORMULA\nbrew update\nbrew upgrade [FORMULA...]\nbrew uninstall FORMULA...\nbrew list [FORMULA...]\nインストール先： /usr/local/Cellar\nもう少し詳細には：　/usr/local/Cellar/パッケージ名/バージョン\n"},{"idx":23,"href":"/notebook/miscellaneous/hugo/","title":"Hugo","content":"Hugo Hugoはマークダウンから静的サイトのためのhtmlを生成するソフトウェア\n Hugoフォルダの構成  \u0008/contents フォルダの中にサイトの内容を記述したマークダウン形式のファイルを配置する。\nマークダウンの内容の先頭には、ファイルの情報を記述する（YAML、TOML、JSONのいずれかの形式）。\n"},{"idx":24,"href":"/notebook/maritime/glossary/","title":"glossary","content":"用語集 組織 RFMO Regional Fisheries Management Office 地域の漁業管理組織 IMO International Maritime Organization 国際海事専門機関\nRFMO（マグロ類） WCPFC Western and Central Pacific Fisheries Commission 中西部太平洋まぐろ類委員会 IATTC Inter-American Tropical Tuna Commission 全米熱帯まぐろ類委員会 ICCAT International Commission for the Conservation of Atlantic Tunas 大西洋まぐろ類保存国際委員会 IOTC Indian Ocean Tuna Commission インド洋まぐろ類委員会 CCSBT Commission for the Conservation of Southern Bluefin Tuna みなみまぐろ保存委員会 tuna RFMO\nRFMO（その他） NPFC North Pacific Fisheries Commission 北太平洋漁業委員会 SPRMO South Pacific Regional Fisheries Management Organisation 南太平洋地域漁業管理機構 CCAMLR Commission for the Conservation of Antarctic Marine Living Resources 南極の海洋生物資源の保存に関する委員会 NEAFC North East Atlantic Fisheries Commission 北東大西洋漁業委員会 SEAFO South East Atlantic Fisheries Organisation 南東大西洋漁業機関 SIOFA Southern Indian Ocean Fisheries Agreement 南インド洋漁業協定 NPAFC North Pacific Anadromous Fish Commission 北太平洋溯河性魚類委員会\n組織その他 JFA Japan Fisheries Agency 水産庁 FRA Fisheries Research and Education Agency; 国立研究開発法人　水産研究・教育機構 ANCORS Australian National Centre for Ocean Resources and Security オーストラリアのFRAに相当する？ MRAG Asia Pacific オーストラリアにある漁業資源管理？コンサルタント MRAGが何の略なのかわからない Marine Resouce A. G. FFA Pacific Islands Forum Fisheries Agency JAFIC Japan Fisheries Information Service Center 日本漁業情報サービスセンター 社団法人？ VMSのデータベースはここにおいてあり、情報の解析などもやっているらしい 水産海洋学会 http://www.jsfo.jp/sympo/history.html\nRFMO用語 CPC ICCATに加盟している国、または、協力的加盟国、地域、組織、 Contracting Parties and Cooperating non-Contracting Parties, Entities or Fishing Entities (CPCs) CCM WCPFCに加盟している国、または、協力的加盟国、地域、組織 TCC Technical and Compliance Committee WCPFCなどの中にある委員会の一つ MCS tool Monitoring, Control and Surveilance 漁船の動きを監視するツール AIS VMS http://www.fao.org/3/V4250E/V4250E03.htm CDS Catch Documentation Schemes 漁獲された時点から、流通の間、水産品を追跡するシステム http://www.fao.org/in-action/globefish/fishery-information/resource-detail/en/c/426994/ ROP Regional Observer Programme RFMOが要請している船にオブザーバーを乗せるプログラム？ 海事用語 IMO番号 IMO Ship identification number 船に割り当てられて、廃船になるまで変わらない １ノット 船舶の速度の単位。1ノットは1時間に1海里（約1,852メートル）を進む速度。1海里は陸上での1マイル（約1,609メートル）とは異なる。 コールサイン 無線局に対して一意に割り当てられる番号。 船や飛行機などの無線機なども一意に振られている？？ 全長 LOA: Length Overall 総トン数 Gross Tonnage GT 国内総トン数：日本国内ではトンは船舶の体積を表す 1000⁄353 m3 = 1トン = 約2.832 861立方メートル、船舶のトン数の測度に関する法律 国際総トン数：重量の単位、IMOにより制定 FOC Flag of Convinience ship 便宜置籍船、船が登録された国（旗国）と、船を実際に運用している国が異なる船のこと 旗船 Flag State 登録された船の所属国 Port State Control PSC 入港時の立入検査 IMO でPSCについての監督手続きに関する決議 外国船舶監督官 PSCO Port State Control Officer 転載 transshipment 船から船への荷物や人の積み替え cargo ship 貨物船 Reefer refrigerated cargo ship 冷凍船 bunker （燃料）補給船 MMSI Maritime Mobile Service Identity 海上移動業務識別コード、AISなどの通信機器を識別するために発行されるID\n水産学・漁業用語 漁労体 ぎょろうたい Fishing unit 海面漁業経営体 かいめんぎょぎょうけいえいたい Marine fishery establishment 水揚機関 みずあげきかん Catch landing organization 航海数 こうかいすう Frequency of fishing trips 出漁日数 しゅつりょうにっすう Number of fishing days 漁労日数 ぎょろうにっすう Number of operating days 漁獲量 ぎょかくりょう Catch 漁獲努力あたりの漁獲量 CPUE catch per unit effort 操業水域区分 そうぎょうすいいきくぶん Major fishing areas for statistical purposes 指定湖沼 していこしょう Designated lakes（and marshes） 魚群 a school of fish 漁場 fishing grounds\n漁業・漁法 漁業種別 沿岸漁業 えんがんぎょぎょう Coastal fishery 沖合漁業 おきあいぎょぎょう Offshore fishery 遠洋漁業 えんようぎょぎょう Distant water fishery 海面漁業 かいめんぎょぎょう Marine fishery 内水面漁業 ないすいめんぎょぎょう Inland water fishery 養殖業　ようしょくぎょう Aquaculture\n漁法 Gear type 漁船の漁獲装置の種類 底びき網漁業 そこびきあみぎょぎょう Trawl fishery 船びき網漁業 ふなびきあみぎょぎょう Boat seine fishery 地びき網漁業 じびきあみぎょぎょう Dragnet fishery まき網漁業 まきあみぎょぎょう Purse seine fishery 刺網漁業 さしあみぎょぎょう Gill net fishery 敷網漁業 しきあみぎょぎょう Lift net fishery 定置網漁業 ていちあみぎょぎょう Set net fishery はえ縄漁業 はえなわぎょぎょう Long line fishery 釣漁業 つりぎょぎょう Angling fishery 遊漁 遊漁 ゆうぎょ Recreational fishing 遊漁案内業 ゆうぎょあんないぎょう Recreational fishing guide 海面船釣遊漁船業者 かいめんふなづりゆうぎょせんぎょうしゃ Recreational boat fishing guide 遊漁採捕量 ゆうぎょさいほりょう Catch quantity by recreational boat fishing 養殖 養殖施設 ようしょくしせつ Aquaculture facility 養殖施設面積 ようしょくしせつめんせき Facilities area for marine aquaculture 投餌量 とうじりょう Quantity of feeding 収獲量〔海面養殖業〕 しゅうかくりょう〔かいめんようしょくぎょう〕 Yield〔Marine aquaculture〕 種苗販売量 しゅびょうはんばいりょう Quantity of sold seedlings 漁の道具 FADs Fish Aggregating Devices マグロなどの魚を引きつけるために海に浮かべる機械、巻き網で使うらしい\n魚種 (Pacific) saury サンマ chub mackerel マサバ blue mackerel ゴマサバ Horse mackerel アジ billfishes カジキ一般 Marlin マカジキ Swordfish メカジキ groundfish 底生魚 toothfish マジェランアイナメ 南極周辺の深海魚、ギンダラの代わりに使われている\n海洋 ECS East China Sea WCPO Western Central Pacific Ocean\n持続可能な漁業  FIP : Fishery Improvement Project  持続可能な漁業を推進するために地域漁業者と進めているプロジェクト一般（シーフードレガシーなどが進めている）   IUU : Illegal Unreported Unregulated \u0026ldquo;three no\u0026rdquo; vessels : No boat name and number, No homeport, No boat certification. 主に中国漁船における用語、中国政府においても登録されていない、管理外になっている？？  観測技術 AIS Automatic Identification System 船が位置情報を発信している。基本的には船の衝突防止のためのシステムなのでAIS装置は他のAIS装置の信号を受信できる。International Maritime Organization (IMO) により一定サイズ（300GT）以上の船には搭載が義務付けられている。 人工衛星や地上の基地局により受信される。データを受信した企業がデータを売っている。数秒に１度発信されるが、受診する衛星側の問題で受信間隔は変わる。AIS CLASS-A と CLASS-B があり、CLASS-Bは電波弱い。 VMS vessel monitoring system VMSは一般名称なので実際に使われているシステムは国により異なる。 こちらも船が発信する信号を衛星で捉えている場合が多いらしい。 VMSの搭載の規制は国レベルで行われている。基本的には政府の内部データである。AISよりも時間解像度が低いことが多いらしい VIIRS Visible Infrared Imaging Radiometer Suite 可視近赤外放射計群 地表面から発せられる可視光や赤外線を検出する受動センサー、Suomi NPP衛星に搭載されている SAR Synthetic Aperture Radar 合成開口レーダー 人工衛星からレーダーを発しその反射波で地形を観測する。天気や昼夜に依存しないで観測できるが、ノイズもある。 DSC Digital Selective Calling デジタル選択呼出装置、AISや無線などで、MMSIにより識別される電波の送受信装置、救難信号などを発する\n人工衛星 ALOS だいち 陸域観測衛星 SARを搭載 ALOS-2＝大地２ AISとSARの両方のデータを受信できるから、AISとSARの検出のマッチングが容易にできる？ SUOMI NPP 2011年に打ち上げられた。VIIRSを搭載している。 NOAA/NASAにより運用されている。 JPSS-1（旧称 NOAA-20） VIIRSを搭載している\n"},{"idx":25,"href":"/notebook/gcp/","title":"Google Cloud Platform","content":"Google Cloud Platform  BigQuery  Google Cloud Strage  gsutil ls gs://BUCKET ローカルファイルをGCSにアップロードする gsutil cp LOCAL_FILE gs://MY_BUCKET/ # 並列でアップロードする場合（100M単位で分割） gsutil -o GSUtil:parallel_composite_upload_threshold=100M　cp LOCAL_FILE gs://MY_BUCKET/ "},{"idx":26,"href":"/notebook/r/leaflet/","title":"leaflet","content":"leaflet leaflet はインタラクティブな地図の可視化をするためのパッケージ\nhttps://rstudio.github.io/leaflet/\nleaflet::leaflet() %\u0026gt;% leaflet::addTiles() %\u0026gt;% leaflet::setView(lng=-100,lat=-2,zoom=5) df \u0026lt;- data.frame( lon = rnorm(10, 140, 1), lat = rnorm(10, 35.7, 1), val1 = LETTERS[1:10], val2 = runif(10, 1, 100) ) df %\u0026gt;% mutate(caption = paste(\u0026#34;lon=\u0026#34;, lon, \u0026#34;\u0026lt;br\u0026gt; lat = \u0026#34;, lat)) %\u0026gt;% leaflet::leaflet() %\u0026gt;% leaflet::addTiles() %\u0026gt;% leaflet::setView(lng = 140, lat = 35.7, zoom = 7) %\u0026gt;% leaflet::addCircleMarkers( lng = ~ lon, lat = ~ lat, color = rainbow(10), popup = ~ caption radius = 10, weight = 2, ) マーカーを表示する マーカーとは地図上の点に対して「アイコン」または「円」を表示する。マーカーをマウスでクリックすると情報が表示される。\n点の位置情報として使えるのは緯度経度の2次元\n sp パッケージの SpatialPoints か SpatialPointsDataFrame sf パッケージの POINT, sfc_POINT （MULTIPOINT はサポートされていない） 緯度・経度をカラムとして持つデータフレーム (カラム名が lat/latitude and lon/lng/long/longitude の場合は自動で見つかる) 緯度経度を格納したベクター  addMarkers() addMarkers(map, lng = NULL, lat = NULL, layerId = NULL, group = NULL, icon = NULL, popup = NULL, popupOptions = NULL, label = NULL, labelOptions = NULL, options = markerOptions(), clusterOptions = NULL, clusterId = NULL, data = getMapData(map)) addCircleMarkers() addCircleMarkers(map, lng = NULL, lat = NULL, radius = 10, layerId = NULL, group = NULL, stroke = TRUE, color = \u0026quot;#03F\u0026quot;, weight = 5, opacity = 0.5, fill = TRUE, fillColor = color, fillOpacity = 0.2, dashArray = NULL, popup = NULL, popupOptions = NULL, label = NULL, labelOptions = NULL, options = pathOptions(), clusterOptions = NULL, clusterId = NULL, data = getMapData(map)) addLabelOnlyMarkers addLabelOnlyMarkers(map, lng = NULL, lat = NULL, layerId = NULL, group = NULL, icon = NULL, label = NULL, labelOptions = NULL, options = markerOptions(), clusterOptions = NULL, clusterId = NULL, data = getMapData(map)) その他関数 addControl(map, html, position = c(\u0026quot;topleft\u0026quot;, \u0026quot;topright\u0026quot;, \u0026quot;bottomleft\u0026quot;, \u0026quot;bottomright\u0026quot;), layerId = NULL, className = \u0026quot;info legend\u0026quot;, data = getMapData(map)) addTiles(map, urlTemplate = \u0026quot;//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\u0026quot;, attribution = NULL, layerId = NULL, group = NULL, options = tileOptions(), data = getMapData(map)) addWMSTiles(map, baseUrl, layerId = NULL, group = NULL, options = WMSTileOptions(), attribution = NULL, layers = \u0026quot;\u0026quot;, data = getMapData(map)) addPopups(map, lng = NULL, lat = NULL, popup, layerId = NULL, group = NULL, options = popupOptions(), data = getMapData(map)) highlightOptions(stroke = NULL, color = NULL, weight = NULL, opacity = NULL, fill = NULL, fillColor = NULL, fillOpacity = NULL, dashArray = NULL, bringToFront = NULL, sendToBack = NULL) addCircles(map, lng = NULL, lat = NULL, radius = 10, layerId = NULL, group = NULL, stroke = TRUE, color = \u0026quot;#03F\u0026quot;, weight = 5, opacity = 0.5, fill = TRUE, fillColor = color, fillOpacity = 0.2, dashArray = NULL, popup = NULL, popupOptions = NULL, label = NULL, labelOptions = NULL, options = pathOptions(), highlightOptions = NULL, data = getMapData(map)) addPolylines(map, lng = NULL, lat = NULL, layerId = NULL, group = NULL, stroke = TRUE, color = \u0026quot;#03F\u0026quot;, weight = 5, opacity = 0.5, fill = FALSE, fillColor = color, fillOpacity = 0.2, dashArray = NULL, smoothFactor = 1, noClip = FALSE, popup = NULL, popupOptions = NULL, label = NULL, labelOptions = NULL, options = pathOptions(), highlightOptions = NULL, data = getMapData(map)) addRectangles(map, lng1, lat1, lng2, lat2, layerId = NULL, group = NULL, stroke = TRUE, color = \u0026quot;#03F\u0026quot;, weight = 5, opacity = 0.5, fill = TRUE, fillColor = color, fillOpacity = 0.2, dashArray = NULL, smoothFactor = 1, noClip = FALSE, popup = NULL, popupOptions = NULL, label = NULL, labelOptions = NULL, options = pathOptions(), highlightOptions = NULL, data = getMapData(map)) addPolygons(map, lng = NULL, lat = NULL, layerId = NULL, group = NULL, stroke = TRUE, color = \u0026quot;#03F\u0026quot;, weight = 5, opacity = 0.5, fill = TRUE, fillColor = color, fillOpacity = 0.2, dashArray = NULL, smoothFactor = 1, noClip = FALSE, popup = NULL, popupOptions = NULL, label = NULL, labelOptions = NULL, options = pathOptions(), highlightOptions = NULL, data = getMapData(map)) addGeoJSON(map, geojson, layerId = NULL, group = NULL, stroke = TRUE, color = \u0026quot;#03F\u0026quot;, weight = 5, opacity = 0.5, fill = TRUE, fillColor = color, fillOpacity = 0.2, dashArray = NULL, smoothFactor = 1, noClip = FALSE, options = pathOptions(), data = getMapData(map)) addTopoJSON(map, topojson, layerId = NULL, group = NULL, stroke = TRUE, color = \u0026quot;#03F\u0026quot;, weight = 5, opacity = 0.5, fill = TRUE, fillColor = color, fillOpacity = 0.2, dashArray = NULL, smoothFactor = 1, noClip = FALSE, options = pathOptions()) "},{"idx":27,"href":"/notebook/r/rhdf5/","title":"rhdf5","content":"rhdf5 HDF5形式のファイルを扱うためのパッケージ\nCRANにはなく bioconductor のパッケージらしいので、bioconductor のレポジトリからインストールする\nrhdf5 - HDF5 interface for R\nインストール install.packages(\u0026quot;BiocManager\u0026quot;) BiocManager::install(\u0026quot;rhdf5\u0026quot;) 基本的な関数 ファイルの構造を表示する h5ls(\u0026quot;myhdf5file.h5\u0026quot;) ファイルの中身を読み取る D = h5read(\u0026quot;myhdf5file.h5\u0026quot;,\u0026quot;foo/A\u0026quot;) ファイルを開く h5f = H5Fopen(\u0026quot;myhdf5file.h5\u0026quot;) # アクセス h5f$df "},{"idx":28,"href":"/notebook/r/ggplot2/","title":"ggplot2","content":"ggplot2 ggplot2: Elegant Graphics for Data Analysis\nその他、Rでの作図一般 The R graph gallery\n基本的な使い方 使い方の例\nset.seed(7) data_df \u0026lt;- data.frame( var1 = 1:10, var2 = rnorm(10), var3 = rep(LETTERS[1:3], length.out = 10)) ggplot(data = data_df) + geom_point(aes(x = var1, y = var2, color = var3)) + geom_line(aes(x = var1, y = var2, color = var3)) ggplot() 関数で ggplot オブジェクトを作成し、その ggplot オブジェクトに対して、 + 演算子を使って作図したいグラフの種類（geom_*() 関数）を指定する。 geom_*() 関数の中で aes() 関数を使って作図に使用する変数を指定する。\nggplot() ggplot(data = NULL, mapping = aes(), ...)  data : プロットしたいデータを含むデータフレームを渡す。データフレームは tidydata の形式になっていることが想定されている。 mapping : 作図に使用する列名を aes() 関数を介して指定する。 例 : aes(x = longitude, y = latitude, color = height, fill = height)  geom_*() グラフの種類ごとに geom_*() 関数が存在する\nちなみに geom は geometry （ジオメトリ）の略、グラフの基礎となる構造を指定する\ngeom_line() geom_point() aes() aes() 関数は、 ggplot() や geom_*() の中で使う。具体的には aes() の出力を ggplot() や geom_*() の　mapping 引数に渡す。基本的には geom_*() 関数の中で使うのが一般的。\naes() 関数は、散布図や折れ線などのグラフの座標（x, y）や線の色（color）、塗りつぶしの色（fill）、サイズ (size) などに 「使用する列」 を指定する。\nちなみに aes　は aesthetic（エステティック） の略、軸や色の指定に使用する変数を指定する\n# 都市の位置（ longitude, latitude）に点をプロット # 都市名 (city) で色分け # 点のサイズは人口 (population) に比例 data(data_df) + geom_point(aes(x = longitude, y = latitude, color = city, size = population)) データのグループ分け 例えば、 geom_path() などでグループごとに別々の線を書きたい場合などに使う\naes(x = X, y = Y, group = A) # 変数Aの値を使ってグループ分け aes(x = X, y = Y, group = interaction(A , B)) # 複数変数を使う場合 グラフの種類: geom_*() geom は geometry （ジオメトリ）の略、グラフの基礎となる構造を指定する\n散布図 geom_point() 折れ線 geom_line() 経路 geom_path() ヒストグラム 連続変数 x の値のビンごとの度数、頻度を、棒グラフ、曲線、折線で描画する\ngeom_histogram() # 棒グラフ geom_density() # なめらかな曲線 geom_freqpoly() # 折線 いずれの geom_* でも、aes() の中で y を指定することで縦軸をカウント ..count.. 、密度（%） ..density.. のどちらにも対応できる\ngeom_histogram(aes(x, y = ..density..)) geom_density( aes(x, y = ..count.. )) 色分けした変数の位置\nposition = \u0026quot;identity\u0026quot; # 重ね描き position = \u0026quot;stack\u0026quot; # 積み上げ position = \u0026quot;dodge\u0026quot;` # 隣接 position = \u0026quot;fill\u0026quot; # 割合 ビンの切り方: star_bin()\n次の２つの書き方は等価らしい\ngeom_histgram(aes(x), ) geom_histgram(aes(x)) + stat_bin(binwidth = 0.1) geom_histogram() geom_density() geom_freqpoly() は stat_bin() の引数をあらかじめ指定した特殊なバージョンらしい\nstat_bin() の binwidth から下の引数は geom_* の中で指定できる。\nstat_bin( mapping = NULL, data = NULL, geom = \u0026quot;bar\u0026quot;, position = \u0026quot;stack\u0026quot;, ..., binwidth = NULL, bins = NULL, center = NULL, boundary = NULL, breaks = NULL, closed = c(\u0026quot;right\u0026quot;, \u0026quot;left\u0026quot;), pad = FALSE, na.rm = FALSE, orientation = NA, show.legend = NA, inherit.aes = TRUE ) x が離散変数なら stat_count() の方がいい\n棒グラフ 1変数（カテゴリ変数）だけ指定すると、指定されたカテゴリ変数の値の数をそれぞれカウントした値が Y 軸になる。stat = \u0026quot;count\u0026quot; がデフォルトなので、暗黙に stat_count() が使用される。\nggplot(df) + geom_bar(aes(x = category)) # 以下は上と同義 #geom_bar(aes(x = category), stat = \u0026#34;count\u0026#34;) X 軸（カテゴリ変数）と Y 軸（量的変数）の値を別々に指定する場合 stat = \u0026quot;identity\u0026quot;\nggplot(df) + geom_bar(aes(x = category, y = value), stat = \u0026#34;identity\u0026#34;) stat = \u0026quot;identity\u0026quot; は geom_bar(aes(x = category, y = value)) + stat_identity() と同じ意味になる？\n積み上げ棒グラフ X 軸（カテゴリ変数）と Y 軸（量的変数）のほかに、変数Z（カテゴリ変数）で色分けした積み上げ棒グラフ aes(fill = z)\nggplot(df) + geom_bar(aes(x = category, y = value, fill = category2), stat = \u0026#34;identity\u0026#34;) 積み上げ棒グラフの縦軸を割合にする position = \u0026quot;fill\u0026quot;\nggplot(df) + geom_bar(aes(x = category, y = value, fill = category2), stat = \u0026quot;identity\u0026quot;, position = \u0026quot;fill\u0026quot;) + scale_y_continuous(labels = percent) 棒の順序を変える 数が多い順に棒を並べ替えるなど。基本的には、予め、x軸（カテゴリ）ごとのy軸の値（カウントなど）を計算しておく必要がある。つまり、 stat = \u0026quot;identity\u0026quot; を指定するやり方が前提。\n変数 y 軸の値を使って x 軸の順番を並べ替える。\n# 昇順の場合 ggplot(df, aes(x=reorder(x, y), y=y), stat = \u0026quot;identity\u0026quot;) # 降順の場合 ggplot(df, aes(x=reorder(x, desc(y)), y=y), stat = \u0026quot;identity\u0026quot;) 棒の向きを水平にする 横向きの棒グラフを作成するには最後に coord_flip() を付け加えるだけ\nggplot(df) + geom_bar(aes(x = category, y = value, fill = category2), stat = \u0026#34;identity\u0026#34;) + coord_flip() 箱ひげ図・バイオリンプロット geom_violin( mapping = NULL, data = NULL, stat = \u0026quot;ydensity\u0026quot;, position = \u0026quot;dodge\u0026quot;, ..., draw_quantiles = NULL, trim = TRUE, scale = \u0026quot;area\u0026quot;, na.rm = FALSE, orientation = NA, show.legend = NA, inherit.aes = TRUE ) 線分 多角形 色の指定 色分けに使用する変数は aes() の中で aes(color = var1, fill = var2) のように指定する。\ncolor に対しては scale_color_gradient()\nfill に対しては scale_fill_gradient()\nをそれぞれ使用する。\n連続値に対する色つけ : scale_*_gradient 連続値: integer, numeric\ninteger は factor にしないと離散値とはみなされない\nscale_colour_gradient(..., low = \u0026#34;#132B43\u0026#34;, high = \u0026#34;#56B1F7\u0026#34;, space = \u0026#34;Lab\u0026#34;, na.value = \u0026#34;grey50\u0026#34;, guide = \u0026#34;colourbar\u0026#34;, aesthetics = \u0026#34;colour\u0026#34;) scale_fill_gradient(..., low = \u0026#34;#132B43\u0026#34;, high = \u0026#34;#56B1F7\u0026#34;, space = \u0026#34;Lab\u0026#34;, na.value = \u0026#34;grey50\u0026#34;, guide = \u0026#34;colourbar\u0026#34;, aesthetics = \u0026#34;fill\u0026#34;) scale_colour_gradient2(..., low = muted(\u0026#34;red\u0026#34;), mid = \u0026#34;white\u0026#34;, high = muted(\u0026#34;blue\u0026#34;), midpoint = 0, space = \u0026#34;Lab\u0026#34;, na.value = \u0026#34;grey50\u0026#34;, guide = \u0026#34;colourbar\u0026#34;, aesthetics = \u0026#34;colour\u0026#34;) scale_fill_gradient2(..., low = muted(\u0026#34;red\u0026#34;), mid = \u0026#34;white\u0026#34;, high = muted(\u0026#34;blue\u0026#34;), midpoint = 0, space = \u0026#34;Lab\u0026#34;, na.value = \u0026#34;grey50\u0026#34;, guide = \u0026#34;colourbar\u0026#34;, aesthetics = \u0026#34;fill\u0026#34;) scale_colour_gradientn(..., colours, values = NULL, space = \u0026#34;Lab\u0026#34;, na.value = \u0026#34;grey50\u0026#34;, guide = \u0026#34;colourbar\u0026#34;, aesthetics = \u0026#34;colour\u0026#34;, colors) scale_fill_gradientn(..., colours, values = NULL, space = \u0026#34;Lab\u0026#34;, na.value = \u0026#34;grey50\u0026#34;, guide = \u0026#34;colourbar\u0026#34;, aesthetics = \u0026#34;fill\u0026#34;, colors) 離散値に対する色つけ 離散値: character, factor\ninteger は factor にしないと離散値とはみなされない\n複数のカラーパレットを使用する ggnewscale::new_scale(\u0026#34;color\u0026#34;) ggnewscale::new_scale(\u0026#34;fill\u0026#34;) ggnewscale::new_scale_color() ggnewscale::new_scale_fill() new_scale(new_aes)\nnew_scale_colour()\nタイトル labs( title = waiver(), # タイトル subtitle = waiver(), # サブタイトル caption = waiver(), # キャプション、右下 tag = waiver() # タグ、上左 ) タイトルとサブタイトル ggtitle(label, subtitle = waiver()) 軸のスケールや目盛 x軸、y軸、color軸、fill軸などそれぞれの軸 (AXIS) とそのデータ型 (DATATYPE) に対応した関数 (scale_AXIS_DATATYPE) が用意されている。\nscale_[x,y,color,fill]_[condinuous,descrete,date]( breaks=c(1,2,3,4), # 目盛位置 labels = c(\u0026#34;01\u0026#34;,\u0026#34;02\u0026#34;, \u0026#34;03\u0026#34;,\u0026#34;04\u0026#34;), # 目盛に表示する値のラベル limits=c(0,120), # 値の範囲 trans = \u0026#34;log10\u0026#34;, # 軸のスケールを変換する関数  ) geom_barやgeom_histgramでの軸の変換\n軸の名前 X軸ラベル、Y軸ラベル\nxlab(label) ylab(label) 凡例 特定の凡例（colour）を消す１\nguides(colour=FALSE) 特定の凡例（colour）を消す２\nscale_colour_discrete(guide=FALSE) 全ての凡例を消す\ntheme(legend.position = \u0026#39;none\u0026#39;) 凡例の点のサイズ変更 デフォルトでは geom_*() で指定したサイズで凡例の点も表示されるが、点が小さい時には困る。凡例だけで大きいサイズでプロットしたい場合。\nguides(color = guide_legend(override.aes = list(size = 5)))+ 画像として保存する : ggsave() gsave( filename, plot = last_plot(), device = NULL, path = NULL, scale = 1, width = NA, height = NA, units = c(\u0026#34;in\u0026#34;, \u0026#34;cm\u0026#34;, \u0026#34;mm\u0026#34;), dpi = 300, limitsize = TRUE, ...)  filename : ファイル名、拡張子で出力形式は自動で判別される plot : ggplotオブジェクト、デフォルトでは最後にプロットしたものが使われる device : 出力形式：\u0026ldquo;eps\u0026rdquo;, \u0026ldquo;ps\u0026rdquo;, \u0026ldquo;tex\u0026rdquo; (pictex), \u0026ldquo;pdf\u0026rdquo;, \u0026ldquo;jpeg\u0026rdquo;, \u0026ldquo;tiff\u0026rdquo;, \u0026ldquo;png\u0026rdquo;, \u0026ldquo;bmp\u0026rdquo;, \u0026ldquo;svg\u0026rdquo; or \u0026ldquo;wmf\u0026rdquo; (windows only) path : 保存先のパス filename と合体する scale\t: 指定した出力サイズを scale 倍する width : 幅 height : 高さ units : 幅と高さの単位 (\u0026ldquo;in\u0026rdquo;, \u0026ldquo;cm\u0026rdquo;, \u0026ldquo;mm\u0026rdquo;) dpi : ラスター画像の解像度 dot per inch、文字列でも指定できる \u0026ldquo;retina\u0026rdquo; (320), \u0026ldquo;print\u0026rdquo; (300), or \u0026ldquo;screen\u0026rdquo; (72) limitsize : TRUE だと 50x50インチより大きいサイズでプロットしない、エラーを防ぐため  複数のプロットを１つをまとめる patchwork パッケージを使うのが楽ちん\nhttps://qiita.com/nozma/items/4512623bea296ccb74ba\n基本的には + 演算子で複数の ggplot オブジェクトを1つにまとめる\nlibrary(ggplot2) library(patchwork) p1 \u0026lt;- ggplot(mtcars) + geom_point(aes(mpg, disp)) p2 \u0026lt;- ggplot(mtcars) + geom_boxplot(aes(gear, disp, group = gear)) p1 + p2 レイアウトの調整は plot_layout() 関数を使う\nplot_layout(ncol = 1, heights = c(3, 1)) 間隔を開けたいときは plot_spacer()\np1 + plot_spacer() + p2 特定の変数の値を使って図を分離する facet_grid() や facet_wrap() を使う\nfacet_grid() は図を2次元に配置する\nggplot(diamonds, aes(x=carat, y=price)) + geom_point(aes(colour=clarity)) + facet_grid(. ~ color) # Horizontal 横に並べる #facet_grid(color ~ .) # Vertical 縦に並べる "},{"idx":29,"href":"/notebook/miscellaneous/git/","title":"git","content":"Git Gitの基本コマンド\n.gitignoreの仕様詳解\nHappy Git and GitHub for the useR\nサブコマンド    サブコマンド 実行内容     clone リポジトリのクローンを作成する   init リポジトリを新規作成する、または既存のリポジトリを初期化する   remote リモートリポジトリを関連付けする   fetch リモートリポジトリの内容を取得する   pull リモートリポジトリの内容を取得し、現在のブランチに取り込む（「fetch」と「merge」を行う）   push ローカルリポジトリの変更内容をリモートリポジトリに送信する   add ファイルをインデックスに追加する（コミットの対象にする）   rm ファイルをインデックスから削除する   mv ファイルやディレクトリの名前を変更する   reset ファイルをインデックスから削除し、特定のコミットの状態まで戻す   status ワークツリーにあるファイルの状態を表示する   show ファイルの内容やコミットの差分などを表示する   diff コミット同士やコミットとワークツリーの内容を比較する   commit インデックスに追加した変更をリポジトリに記録する   tag コミットにタグを付ける、削除する、一覧表示する   log コミット時のログを表示する   grep リポジトリで管理されているファイルをパターン検索する   branch ブランチを作成、削除、一覧表示する   checkout ワークツリーを異なるブランチに切り替える   merge 他のブランチやコミットの内容を現在のブランチに取り込む   rebase コミットを再適用する（ブランチの分岐点を変更したり、コミットの順番を入れ替えたりできる）   config 現在の設定を取得、変更する    レポジトリを作成する、コピーする init : 既存のフォルダをレポジトリにする git init して登録されたフォルダが、大元のレポジトリになる？\n他の人は、このレポジトリをクローンすることで、開発に参加できる\nclone : 既存のレポジトリをコピーする 既にあるレポジトリの内容をコピーして、ローカルにレポジトリのクローンを作成する\nこのクローンに、変更を加えることで、元レポジトリの開発に参加できる\n既にあるレポジトリへの操作 add 作成したファイルを git の管理下に追加する（ステージングする）。一度追加したら同じファイルを再度追加する必要はない。\ngit は add されたファイルの変更を検出し、\ngit add path_to_file git add . # カレント以下の全てのファイル git add *.java # 拡張子が .java commit ステージングしたファイルの変更内容を記録する（コミットする）\ngit commit -m 'コメント' push リモートレポジトリに反映する 現状のローカルレポジトリの最新 コミット 状態を、リモートレポジトリに反映する（プッシュする）\ngit push レポジトリ ブランチ 例：git push origin master\nここで origin レポジトリは、このレポジトリの大本である、リモートにあるレポジトリを意味する（clone するもとになったやつ）\nファイルの削除・名前変更など 基本的に、名前の変更や削除など、ファイル・ディレクトリに対する操作は全て git を通して行う。そうしないと git がそれらの変更をトラッキングできない。\nファイル名を変えて内容を変更するとき、git を使わないでファイル名を変更すると、元のファイルとは別のファイルとして扱われるので、どこを変更したのかわからなくなる。\ngit rm git mv ブランチ ブランチとは１つのレポジトリで複数の状態（例えば正式版、開発版）などを保持しておく仕組み。ブランチを切り替えるとフォルダの中身も変化する。開発版のブランチに切り替えてファイルを編集すると正式版を壊すことなく、開発版を変更して、開発版が完成したら、それを正式版のブランチにマージするといったことができる。\nレポジトリを作成したときにできる最初のブランチが master\nローカルレポジトリとリモートレポジトリの\n既存のブランチを確認する git branch ブランチの作成 : branch git branch ブランチ名 # ブランチの作成 git checkout -b ブランチ名 #ブランチの作成とそのブランチへの切り替え git branch ブランチ名 だとブランチを作成するだけだが、git checkout -b ブランチ名 にするとブランチを作成して、そのブランチに切り替える（git branch ブランチ名; git checkout ブランチ名; と同じ）\nブランチの切り替え : checkout ブランチを切り替えると、フォルダの中身の状態が、そのブランチの最新の状態になる\ngit checkout ブランチ名 カレントブランチの確認 カレントブランチには * が付く\n$ git branch --contains * develop master ブランチの削除 ローカルにあるブランチを削除する\ngit branch \u0026ndash;delete [ブランチ名] git branch -d [ブランチ名] git branch -D [ブランチ名]\nリモートにあるブランチをローカルに持ってくる # 最初にリモートブランチにチェックアウトして git checkout origin/[ブランチ名] # 次にしそれをローカルブランチとしてコピーする git checkout -b [ブランチ名] 最初のコマンドを打った後に次のようなメッセージが出る。 最初のコマンドを売った状態だと、リモートレポジトリの内容を見て、試しに変更したりできるけど、変更を commit してもリモートにもローカルにも反映されないらしい。変更をローカルレポジトリとしてコピーするために2番目のコマンドを使う。その後は、commit した変更を保存することができる。\nNote: checking out 'origin/[ブランチ名]'. You are in 'detached HEAD' state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -b with the checkout command again. Example: git checkout -b \u0026lt;new-branch-name\u0026gt; HEAD is now at a3fe87c updated where the tables are saved, and fixed source and source_ssvid fields. Also made small edit to extract_top_match.sql.j2 ローカルブランチをコピーして別のローカルブランチを作る # コピーしたいローカルブランチ(hoge)にチェックアウトする git checkout hoge # そこで新しいレポジトリ(hoge_copied)を作る git checkout -b hoge_copied ブランチ同士をマージする branchA に branchB の内容を統合する\n# まずは branchA にチェックアウトして git checkout branchA # そこに branchB をマージする git merge branchB レポジトリの状態を表示する : status git status 新たに作成されたけど add されていないファイルや、編集されたけど commit されていないファイルなどを表示する。\nファイル/フォルダを復元する 以前にコミットした状態に復元する\ngit checkout [コミット番号] [ファイルパス] ファイルへの操作 ファイル名を変更する・移動する : git mv git mv ファイルを削除する ローカルで削除したファイル・フォルダを、リモートレポジトリからも削除する ローカルフォルダには、ファイルやフォルダがあっても、リモートレポジトリにはアップロードされないようにしたい。 既に、リモートレポジトリにアップロードされてしまったファイル・フォルダを、リモートレポジトリから削除したい\n 後から、.gitignore に追加削除した場合 .gitignore の内容と関係なく、後から、特定のファイル・フォルダを除外したい場合  .gitignore に新たにファイルやフォルダを追加した場合も同じ\nあるいは、以前は追跡していたファイル・フォルダの追跡をやめて、リモートレポジトリから削除したい場合\n# .gitignore に追加した後、あるいは、ファイルやフォルダを削除した後 # キャッシュを削除する git rm --cached /path/to/消したいファイル.txt git rm --cached -r /path/to/消したいフォルダ # .gitignore を編集したなら git add .gitignore # git status # コミット git commit -m \u0026quot;remove some files\u0026quot; # リモートに反映 git push origin master Githubへのプルリク プルリク（pull request）は git ではなく Github や Gitlab の機能\nブランチをマージする前にチェックして、Github 上でマージできる\n初心者向けGithubへのPullRequest方法\n  リモートリポジトリをローカルに clone する。\n git clone https://github.com/hoge/hoge.git    ローカルで、編集用に新しいブランチ new_branch を作成し、そこに切り替える\n cd hoge git branch new_branch git checkout new_branch    編集し、add commit する\n ファイルを編集する git add 変更したファイル git commit -m 'コメント'    リモートの push する\n  pull requestする\n  git clone https://github.com/teuder/test.git\n【cloneされたリモートリポジトリ】 = 【origin リポジトリ】\n"},{"idx":30,"href":"/notebook/r/raster/","title":"raster","content":"raster # GooTiffの読み込み data \u0026lt;- raster::raster(\u0026quot;path/data.tif\u0026quot;) 値と座標のデータフレームに変換する\ndf \u0026lt;- raster::rasterToPoint(data) 緯度経度と値のデータフレームからラスタデータを作成する test \u0026lt;- raster::rasterize(x = data[c(\u0026quot;lon\u0026quot;,\u0026quot;lat\u0026quot;)], # データフレームの経度、緯度のカラムの指定、n行2列の行列でもいい y = raster::raster(ncols=32512, nrows=24576), # 雛形となるラスタオブジェクト（ピクセル数を指定する） field = data$rad, # 値のベクター fun = mean, # セルに複数の点が含まれるとき、セルの値を計算する関数 filename = \u0026quot;./output/peru_20190828_32512_24576.tif\u0026quot; # ファイル出力も一緒にやる場合 ) #raster::writeRaster(test, \u0026quot;./output/peru_20190828_4608_6096.tif\u0026quot;, overwrite=TRUE) 関数 rasterize ## S4 method for signature 'matrix,Raster' rasterize(x, y, field, fun='last', background=NA, mask=FALSE, update=FALSE, updateValue='all', filename=\u0026quot;\u0026quot;, na.rm=TRUE, ...) ## S4 method for signature 'SpatialPoints,Raster' rasterize(x, y, field, fun='last', background=NA, mask=FALSE, update=FALSE, updateValue='all', filename=\u0026quot;\u0026quot;, na.rm=TRUE, ...) ## S4 method for signature 'SpatialLines,Raster' rasterize(x, y, field, fun='last', background=NA, mask=FALSE, update=FALSE, updateValue='all', filename=\u0026quot;\u0026quot;, ...) ## S4 method for signature 'SpatialPolygons,Raster' rasterize(x, y, field, fun='last', background=NA, mask=FALSE, update=FALSE, updateValue='all', filename=\u0026quot;\u0026quot;, getCover=FALSE, silent=TRUE, ...) ラスタの各セルに、空間データオブジェクト（点、線、ポリゴン）と紐づいた値を、付与する。\nポリゴン：ポリゴンがセルの中心に被っていたら値が転送される。線：その線と触れているセル全てに値が転送される。この挙動を、ポリゴンを線としてラスタ化した後で、ポリゴンをポリゴンとしてラスタ化することで、合体させることができる？？\nxが点を表す場合、各点がセルと紐づく。セル同士の境界に点が落ちる場合は右側あるいは下側のセルに配置される。セルに付与される値は、点の値と関数　fun により決定される。\nx 各セルに値を転送したい地物のオブジェクト 点を表すオブジェクト（SpatialPointsオブジェクト、２列行列、２列データフレーム）、 SpatialLinesオブジェクト、 SpatialPolygons*オブジェクト、 あるいはその拡張オブジェクト\ny 値を転送したいラスタオブジェクト（Raster*）（既存のラスタオブジェクト）\nfield 数値あるいは文字列。ラスタに転送する値。スカラ値、あるいは、数値ベクター（長さはxの地物の数と同じ） x が Spatial*DataFrame の場合は、カラム名を指定することができる。 指定しない場合、その属性インデックスが\nIf missing, the attribute index is used (i.e. numbers from 1 to the number of features). You can also provide a vector with the same length as the number of spatial features, or a matrix where the number of rows matches the number of spatial features\nfun function or character. To determine what values to assign to cells that are covered by multiple spatial features. You can use functions such as min, max, or mean, or one of the following character values: \u0026lsquo;first\u0026rsquo;, \u0026lsquo;last\u0026rsquo;, \u0026lsquo;count\u0026rsquo;. The default value is \u0026lsquo;last\u0026rsquo;. In the case of SpatialLines*, \u0026lsquo;length\u0026rsquo; is also allowed (currently for planar coordinate systems only).\nIf x represents points, fun must accept a na.rm argument, either explicitly or through \u0026lsquo;dots\u0026rsquo;. This means that fun=length fails, but fun=function(x,\u0026hellip;)length(x) works, although it ignores the na.rm argument. To use the na.rm argument you can use a function like this: fun=function(x, na.rm)if (na.rm) length(na.omit(x)) else (length(x), or use a function that removes NA values in all cases, like this function to compute the number of unique values per grid cell \u0026ldquo;richness\u0026rdquo;: fun=function(x, \u0026hellip;) {length(unique(na.omit(x)))} . If you want to count the number of points in each grid cell, you can use fun='count\u0026rsquo; or fun=function(x,\u0026hellip;){length(x)}.\nYou can also pass multiple functions using a statement like fun=function(x, \u0026hellip;) c(length(x),mean(x)), in which case the returned object is a RasterBrick (multiple layers).\nbackground numeric. Value to put in the cells that are not covered by any of the features of x. Default is NA\nmask logical. If TRUE the values of the input Raster object are \u0026lsquo;masked\u0026rsquo; by the spatial features of x. That is, cells that spatially overlap with the spatial features retain their values, the other cells become NA. Default is FALSE. This option cannot be used when update=TRUE\nupdate logical. If TRUE, the values of the Raster* object are updated for the cells that overlap the spatial features of x. Default is FALSE. Cannot be used when mask=TRUE\nupdateValue numeric (normally an integer), or character. Only relevant when update=TRUE. Select, by their values, the cells to be updated with the values of the spatial features. Valid character values are \u0026lsquo;all\u0026rsquo;, \u0026lsquo;NA\u0026rsquo;, and \u0026lsquo;!NA\u0026rsquo;. Default is \u0026lsquo;all\u0026rsquo;\nfilename character. Output filename (optional)\nna.rm If TRUE, NA values are removed if fun honors the na.rm argument\ngetCover logical. If TRUE, the fraction of each grid cell that is covered by the polygons is returned (and the values of field, fun, mask, and update are ignored. The fraction covered is estimated by dividing each cell into 100 subcells and determining presence/absence of the polygon in the center of each subcell\nsilent Logical. If TRUE, feedback on the polygon count is suppressed. Default is FALSE\n\u0026hellip; Additional arguments for file writing as for writeRaster\n"},{"idx":31,"href":"/notebook/r/sf/","title":"sf","content":"sf 定義されたクラス  sfg ：個別の地物オブジェクト sfc ： sfg オブジェクトのリスト、リストの各要素が１地物に相当 sf ： sfc オブジェクトを geometry 列としてもつデータフレーム、１行が１地物、 geometory 列以外の列は地物がもつ値  地物（sfgオブジェクト）の型  POINT：点 LINESTRING：線分 POLYGON：多角形 MULTIPOINT：点の集合 MULTILINESTRING：線分の集合 GEOMETORYCOLLECTION：様々な型のデータの集合  sfオブジェクトの読み込み・書き出し https://r-spatial.github.io/sf/articles/sf2.html\npref_simple_sf \u0026lt;- sf::st_read(\u0026#34;../output/pref_simple.shp\u0026#34;) sf::st_write(pref_simple_sf, \u0026#34;../output/pref_simple.shp\u0026#34;, layer_options = \u0026#34;ENCODING=UTF-8\u0026#34;) sf オブジェクトの作成 データフレームから作成する : st_as_sf() １行が１点を表すデータフレーム df （緯度 lat 経度 lon）から sf オブジェクトを作成する。coords = c(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;)、はXY座標に相当するカラム名を指定している。crs = 4326 で座標参照系を設定している。EPSGコード 4326 は WGS84 を表す。 remove = FALSE は coords = c(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;) で指定した列を削除しないという意味。\n# df は 各行が1POINTを表していて、その座標が \u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot; という列名で保持されているデータフレーム sf \u0026lt;- df %\u0026gt;% st_as_sf(coords = c(\u0026quot;lon\u0026quot;, \u0026quot;lat\u0026quot;), dim = \u0026quot;XY\u0026quot;, remove = FALSE, crs = 4326) データフレームから作成する : st_set_geometry() すでにある sfc オブジェクトを、データフレームに、geometry 列として追加する。\ndata_sf \u0026lt;- st_set_geometry(data_df, geom_sfc)\nWKT形式のテキストから作成 area_wkt \u0026lt;- \u0026#34;Polygon ((165.06947186552821449 48.95836601433594382, 169.89568502076190271 45.13709077274640435, -149.95899646070347444 44.98161022438796408, -149.95031841566071762 53.86930346517581114, -163.23108263759604597 52.2378116240458894, -168.71541576854343703 50.45207289660321948, 164.95978520290927349 51.55926363240160981, 165.06947186552821449 48.95836601433594382))\u0026#34; # SFC の作成 target_sfc \u0026lt;- sf::st_as_sfc(area_wkt, crs=4326) # SFC から　SF への変換 # st_set_geometry()を使った方法 # この方法ならsfcの列の名前が x になる target_sf \u0026lt;- sf::st_as_sf(target_sfc) # st_set_geometry()を使った方法 # この方法ならsfcの列の名前が geometry になる data_df \u0026lt;- tibble::tibble(data=1) target_sf \u0026lt;- st_set_geometry(data_df, target_sfc) 処理関数 sfオブジェクトの切り抜き st_crop() \nland_crop \u0026lt;- st_crop(land, c(xmax=180, xmin=-180, ymin = -80, ymax=80)) 日付変更線で地物に切れ目を作る -180:180 の経度で作成された地物は、日付変更線をまたいでいるときにうまく描画・処理できない。そこで、日付変更線のラインで地物を分断する。\nsf::st_wrap_dateline(options = c(\u0026quot;WRAPDATELINE=YES\u0026quot;, \u0026quot;DATELINEOFFSET=0\u0026quot;), quiet = FALSE) 日付変更線の切れ目をつなぐ https://stackoverflow.com/questions/56146735/visual-bug-when-changing-robinson-projections-central-meridian-with-ggplot2/56155662#56155662\n型の変換 地物の型の変換 st_cast()\nsf を data.frame に変換する sfオブジェクトをデータフレームに変換する（この時、df には geometry 列（リスト列=sfcオブジェクト）が残る）\ndata_df \u0026lt;- as.data.frame(data_sf) sf オブジェクトから geometory 列を削除すると、ただのデータフレームになる。そのために、st_set_geometry() はデータフレームにgeometry列をくっつける関数だけど、NULLを渡すとsfオブジェクトからgeomeotry列を削除できる。\ndf \u0026lt;- st_set_geometry(data_sf, NULL)\nWKT形式に変換する lwgeom::st_astext(x, digits = options(\u0026quot;digits\u0026quot;), ..., EWKT = FALSE) lwgeom::st_asewkt(x, digits = options(\u0026quot;digits\u0026quot;)) 地物同士の位置関係の判定 predicate whether x touches/contains/within/ y\nsparse=FALSE にすると論理値ベクトルを返す。\nst_intersects(x, y, sparse = TRUE, ...)\nst_disjoint(x, y = x, sparse = TRUE, prepared = TRUE)\nst_touches(x, y, sparse = TRUE, prepared = TRUE)\nst_crosses(x, y, sparse = TRUE, prepared = TRUE)\nst_within(x, y, sparse = TRUE, prepared = TRUE)\nst_contains(x, y, sparse = TRUE, prepared = TRUE)\nst_contains_properly(x, y, sparse = TRUE, prepared = TRUE)\nst_overlaps(x, y, sparse = TRUE, prepared = TRUE)\nst_equals(x, y, sparse = TRUE, prepared = FALSE)\nst_covers(x, y, sparse = TRUE, prepared = TRUE)\nst_covered_by(x, y, sparse = TRUE, prepared = TRUE)\nst_equals_exact(x, y, par, sparse = TRUE, prepared = FALSE)\nst_is_within_distance(x, y, dist, sparse = TRUE)\nsf::st_wrap_dateline(options = c(\u0026ldquo;WRAPDATELINE=YES\u0026rdquo;, \u0026ldquo;DATELINEOFFSET=180\u0026rdquo;), quiet = FALSE)\nggplot2 プロット時に投影図法を指定する\nlibrary(sf) library(ggplot2) land \u0026lt;- read_sf(\u0026#34;data/ne_10m_land/ne_10m_land.shp\u0026#34;) land_crop \u0026lt;- st_crop(land, c(xmax=180, xmin=-180, ymin = -80, ymax=80)) ggplot(land_crop)+ geom_sf()+ #coord_sf(crs = sf::st_crs(\u0026#39;+proj=moll\u0026#39;)) # モルワイデ図法 coord_sf(crs = sf::st_crs(\u0026#39;+proj=wag6\u0026#39;)) # Wagner VI projection 30 day map challenge Twitter の #30DayMapChallenge で地理情報可視化の例がたくさんある\nR の sf \u0026amp; ggplot メインで挑戦した人がBookdownでまとめてくれている\nhttps://twitter.com/tjukanov/status/1187713840550744066\nhttps://rud.is/books/30-day-map-challenge/\nデータベースへの読み書き https://r-spatial.github.io/sf/articles/sf2.html\n"},{"idx":32,"href":"/notebook/r/bigrquery/","title":"bigrquery","content":"bigrquery R から BigQuery を操作するためのパッケージ\nコネクションの作成 ds \u0026lt;- DBI::dbConnect( drv = bigrquery::bigquery(), project = \u0026quot;project_name\u0026quot;, dataset = \u0026quot;dataset_name\u0026quot;, use_legacy_sql = FALSE ) ds \u0026lt;- bq_dataset( project = \u0026quot;project_name\u0026quot;, dataset = \u0026quot;dataset_name\u0026quot;,) 大きなデータをダウンロードしようとするとエラーが起きる。以下を設定すると回避できる。最新版では修正されているかもしれない\noptions(scipen = 20) 自動で認証するためのアクセストークン https://gargle.r-lib.org/articles/non-interactive-auth.html\nbigrquery::bq_auth(path = \u0026quot;access_token.json\u0026quot;) "},{"idx":33,"href":"/notebook/r/dplyr/","title":"dplyr","content":"dplyr データフレームに対する操作\n列の選択 select()\n列選択のヘルパー関数 これらの関数は select() の中で使う\nall_of(x) 変数 x に記録されたカラムを選択する\n行の選択 行の並べ替え グループ化 集計 レコード数のカウント 以下の3つは全て同じ結果を返す。カラム x の値ごとにレコード数をカウントする。\ndf %\u0026gt;% group_by(x) %\u0026gt;% summarize(n = n()) df %\u0026gt;% group_by(x) %\u0026gt;% tally() df %\u0026gt;% count(x) add_count() と add_tally() は、上の例の summarize() を mutate() に変えたもの、元の df の行数は変えずにグループごとのカウント列を追加する\ndf %\u0026gt;% group_by(x) %\u0026gt;% mutate(n = n()) df %\u0026gt;% group_by(x) %\u0026gt;% add_tally() df %\u0026gt;% add_count(x) "},{"idx":34,"href":"/notebook/r/future/","title":"future","content":"future future パッケージは１つのPC内での並列計算、および、複数PCをまたいだ分散計算を実行するためのパッケージ\n"},{"idx":35,"href":"/notebook/r/gganimate/","title":"gganimate","content":"gganimate transition_manual() コマ撮りアニメのように、１枚１枚のコマが切り替わるようなアニメーション\ntransition_manual(frames, ..., cumulative = FALSE)  frames でコマ分けに使いたいカラムを指定する。 cumulative = TRUE なら前のコマに重ねて次のコマを描画する  また、次の変数が定義される\n previous_frame 前のフレームの値 current_frame 現在のフレームの値 next_frame 次のフレームの値  使用例\nanim \u0026lt;- ggplot(mtcars, aes(factor(gear), mpg)) + geom_boxplot() + transition_manual(gear) + ggtitle('Now showing {current_frame}') transition_states() コマとコマの間を補完する画像（データ）を生成する\ntransition_states(states, transition_length = 1, state_length = 1, wrap = TRUE)  states ベースとなるコマ分けに使用するカラム transition_length ベースとなるコマとコマの間で、補完されるコマを表示する相対的な長さ state_length ベースとなるコマを表示する相対的な長さ wrap アニメーションを循環させるか、TRUE なら最後のコマは最初のコマに遷移する  "},{"idx":36,"href":"/notebook/r/lubridate/","title":"lubridate","content":"lubridate 日付を扱うパッケージ\n文字列から　Date, POSIXct オブジェクトの生成 ymd(..., quiet = FALSE, tz = NULL, locale = Sys.getlocale(\u0026quot;LC_TIME\u0026quot;), truncated = 0) ymd_hms(..., quiet = FALSE, tz = \u0026quot;UTC\u0026quot;, locale = Sys.getlocale(\u0026quot;LC_TIME\u0026quot;), truncated = 0) ymd_hm(..., quiet = FALSE, tz = \u0026quot;UTC\u0026quot;, locale = Sys.getlocale(\u0026quot;LC_TIME\u0026quot;), truncated = 0) ymd_h(..., quiet = FALSE, tz = \u0026quot;UTC\u0026quot;, locale = Sys.getlocale(\u0026quot;LC_TIME\u0026quot;), truncated = 0) 他にも\nタイムゾーン tz() tz(x) tz(x) \u0026lt;- value 日付・時刻オブジェクト x について設定されたタイムゾーンを出力する。\n日付・時刻オブジェクト x のタイムゾーンを変更する。この結果、下の例のように「xの絶対的な時刻が変化する」ことに注意する。\n\u0026ldquo;2016-04-19 16:49:00 UTC\u0026rdquo; → \u0026ldquo;2016-04-19 16:49:00 JST\u0026rdquo;\nx \u0026lt;- ymd_hms(\u0026#34;2016-04-19 16:49:00\u0026#34;, tz=\u0026#34;UTC\u0026#34;) print(x) # [1] \u0026#34;2016-04-19 16:49:00 UTC\u0026#34; tz(x) \u0026lt;- \u0026#34;Japan\u0026#34; # [1] \u0026#34;2016-04-19 16:49:00 JST\u0026#34; with_tz() with_tz(time, tzone = \u0026quot;\u0026quot;) time で指定された特定の絶対時刻について、tzone で指定したタイムゾーンでのローカル時刻を表示する。\nこの場合、time 元の絶対的な時刻は変化しない\nx \u0026lt;- ymd_hms(\u0026#34;2016-04-19 16:49:00\u0026#34;, tz=\u0026#34;UTC\u0026#34;) print(x) # [1] \u0026#34;2016-04-19 16:49:00 UTC\u0026#34; with_tz(x, tz = \u0026#34;Japan\u0026#34;) # [1] \u0026#34;2016-04-20 01:49:00 JST\u0026#34; "},{"idx":37,"href":"/notebook/r/purrr/","title":"purrr","content":"purrr ベクトルやリストなどの各要素に対する繰り返し処理\n基本関数 map_*\nmap2_*\nreduce()\naccumulate\nその他関数 pmap\npartial\nデータフレームの各行に対する繰り返し "},{"idx":38,"href":"/notebook/r/package_development/","title":"R Package Development","content":"Rパッケージ開発 パッケージにデータを含める  data/  ユーザーもアクセスできる。例データなど   R/sysdata.rda に含める  ユーザーはアクセスできない。関数内で使用するデータなど   生データを保存したい場合？  inst/extdata におく    data/  データは .RData ファイルとして保存する。 save() を使う。 ファイルには１つのオブジェクトだけを含める。 ファイル名は保存されたオブジェクト名と一致する必要がある。  devtools::use_data() を使うと上のルールに従ったデータを簡単に作成できる。\n他のデータ形式でもいいらしい\nDESCRIPTION ファイルに　LazyData: true の記述があるとデータは、実際に使用される時までRにロードされることはない。\nR/sysdata.rda sysdata.rda に保存されたオブジェクトはユーザーからは見えない。\ndevtools::use_data(internal = TRUE) を使う。\n"},{"idx":39,"href":"/notebook/r/reticulate/","title":"reticulate","content":"reticulate R から Python を呼び出すためのパッケージ\n使用する Python を指定する .Rprofile で RETICULATE_PYTHON 環境変数に、使用するPythonを指定するのが安全\nSys.setenv(RETICULATE_PYTHON = '/usr/local/bin/python3') pipenv で Python 環境を管理している場合\nPipenv で作成した環境を利用する R のプロジェクトフォルダに pipenv の環境が作成されている前提\nカレントフォルダはR のプロジェクトフォルダにいる前提\nPipenv で作成した環境を reticulate で利用する。\nvenv \u0026lt;- system(\u0026#34;pipenv --venv\u0026#34;, inter = TRUE) reticulate::use_virtualenv(venv, required = TRUE) reticulate::py_config() Windows の場合は venv のパスの文字列をRの書式に合わせるために変換をかませる\nvenv \u0026lt;- system(\u0026#34;pipenv --venv\u0026#34;, inter = TRUE) venv \u0026lt;- stringr::str_replace_all(stringr::str_sub(venv, 1, -2), \u0026#34;\\\\\\\\\u0026#34;, \u0026#34;/\u0026#34;) reticulate::use_virtualenv(venv, required = TRUE) reticulate::py_config() reticulate で使用されているPython環境を確認する reticulate::py_config() "},{"idx":40,"href":"/notebook/r/rmarkdown/","title":"Rmarkdown","content":"Rmarkdown Rmarkdown (.Rmd) から、html などに出力するパッケージは２つある\nknitr と rmarkdown、rmarkdown は内部で knitr を使用している。rmarkdown は knitr の後継となるべく開発されているのかも知れない。\nドキュメント  公式: R Markdown: The Definitive Guide rmarkdownパッケージで楽々ドキュメント生成  用語  chunk YAMLヘッダ  出力形式の指定 公式 2.4 Output formats\n.Rmd ファイルの先頭の YAML ヘッダの output: セクションに記述する\n--- title: \u0026quot;Title of this document\u0026quot; output: rmarkdown::github_document --- output: で指定できるオプションは以下の通り\n html_document github_document pdf_document word_document latex_document md_document odt_document rtf_document context_document powerpoint_presentation beamer_presentation ioslides_presentation slidy_presentation  出力先の変更 デフォルトでは .Rmd ファイルと同じフォルダに出力される。\nrmarkdown::render() を使って出力する場合には、以下のようにする。\nrmarkdown::render('my.Rmd', output_file = 'folder/my.html') RStudio の Knit ボタンを押したときの出力先を指定するには .Rmd ファイルの先頭の YAML ヘッダの knit: セクションに以下のように output_dir を指定する。パスの指定は .Rmd ファイルからの相対パス。\n--- title: \u0026quot;Title of this document\u0026quot; output: rmarkdown::github_document knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_dir = \u0026quot;../share/markdown\u0026quot;) }) --- "},{"idx":41,"href":"/notebook/r/rpart/","title":"rpart","content":"rpart 決定木\nrpart.object\nhttps://rdrr.io/cran/rpart/man/rpart.object.html\nrpart() による学習結果のオブジェクト\nrpart.object の要素\n frame: データフレーム、tree を表現する  var: 分割に使用される変数、\u0026lt;leaf\u0026gt; は末端ノード n wt dev yval complexity ncompete nsurrogate yval2   where: 整数ベクトル、訓練データのレコード数と同じ長さ。訓練データの各レコードがどのリーフに落ちたかを表す。値は frame の行番号 call: このオブジェクトを作成するときに記述されたRコードのイメージ terms: terms.object フォーミュラをサマライズしたもの、ユーザーは基本使わない splits: 分割を記述する matrix。列は、\u0026ldquo;count\u0026rdquo; \u0026ldquo;ncat\u0026rdquo; \u0026ldquo;improve\u0026rdquo; \u0026ldquo;index\u0026rdquo; \u0026ldquo;adj\u0026rdquo;、各行は分割に使われる変数名 csplit: 整数行列、少なくとも１つの分割変数が factor である場合に作成される method: 文字列、(\u0026ldquo;class\u0026rdquo;, \u0026ldquo;exp\u0026rdquo;, \u0026ldquo;poisson\u0026rdquo;, \u0026ldquo;anova\u0026rdquo; or \u0026ldquo;user\u0026quot;のうちのどれか)、この tree を作成するときに使われた方法 cptable: 数値行列、complexityパラメタに基づいて決定された最適な枝刈りの情報？ 列名 \u0026ldquo;CP\u0026rdquo; \u0026ldquo;nsplit\u0026rdquo; \u0026ldquo;rel error\u0026rdquo; \u0026ldquo;xerror\u0026rdquo; \u0026ldquo;xstd\u0026rdquo; variable.importance: 名前付きベクトル、変数重要度、 numresp: 整数スカラー、目的変数の値の数、factorのレベルの数 parms: 学習時に与えられたパラメタの値 control: 学習時に与えられたパラメタの値 functions: rpartオブジェクトのMethodとして使われる関数 summary(), print() and text() ordered: 名前付き論理ベクトル、要素名は変数名、値はその変数が順序付きfactorであるかを表す na.action: stats::model.frame から返される値、NA の取り扱いを決める  rpart.object の属性 attributes()\n xlevels: 説明変数にある factor 型のレベル ylevels: 目的変数の factor 型のレベル  rpart:::predict.rpart() rpart.object を使って predict() したときに呼び出されるメソッド\nhttps://www.rdocumentation.org/packages/rpart/versions/4.1-15/topics/predict.rpart\npredict(object, newdata, type = c(\u0026quot;vector\u0026quot;, \u0026quot;prob\u0026quot;, \u0026quot;class\u0026quot;, \u0026quot;matrix\u0026quot;), na.action = na.pass, …) type: 分類ではデフォルトは prob クラス確率, class なら予測された目的変数 (facor) の値、vector なら目的変数 factor の levels 属性の要素番号、回帰の場合は デフォルトは vector で目的変数の値（多分）\ntyppe=\u0026quot;vector\u0026quot;　の時は、rpart.object$frame$yval の値が返されるらしい（各データが落ちたリーフの yval の値） これを使ってリーフ番号を取得できる\nrpart関係の別のライブラリ  rpart.plot itree: rpartの拡張らしい、rpartの作者もかかわっている treeClust  rpart.plot::rpart.predict() 新しいデータに対して予測する\nrpart::predict.rpart() と同じだが、予測値のノード番号とルールを出力できる\nrpart.plot::rpart.predict(object, newdata, type = c(\u0026quot;vector\u0026quot;, \u0026quot;prob\u0026quot;, \u0026quot;class\u0026quot;, \u0026quot;matrix\u0026quot;), na.action = na.pass, nn=FALSE, rules=FALSE, ...)  nn: TRUE なら ノード番号の列も返す rules: TRUE なら、ルールを文字列で記述した列も返す ...: rpart.rules() に渡される引数、例えば clip.facs=TRUE  rpart.plot::rpart.rules rpart.object から 各リーフに至るルールを表示する\nrpart.rules(x = stop(\u0026quot;no 'x' argument\u0026quot;), style = \u0026quot;wide\u0026quot;, cover = FALSE, nn = FALSE, roundint = TRUE, clip.facs = FALSE, varorder = NULL, ...) しかし、リーフの最大値は1000に制限されているので、それ以上に複雑なtreeのルールを生成したいときは、パッケージをいじる必要がある\nrpart.plot/R/rpart.rules.R の中の maxrules \u0026lt;- 1e3 をもっと大きい値に書き換える\nそして、ローカルのソースからインストールする install.packages(\u0026quot;./rpart.plot/\u0026quot;, type=\u0026quot;source\u0026quot;, repos = NULL)\nstyle = \u0026ldquo;tall\u0026rdquo;\n出力される値（class c(\u0026quot;rpart.rules\u0026quot;, \u0026quot;data.frame\u0026quot;)）\n target : 分類：クラス確率の文字列  行名: おそらく rpart.object$frame の行名に対応する？\ntreeClust::rpart.predict.leaves() rpart.predict.leaves(rp, newdata, type = \u0026quot;where\u0026quot;)  type  \u0026quot;where\u0026quot; : \u0026ldquo;rpart.object\u0026rdquo; の要素 frame の行番号を返す \u0026quot;leaf\u0026quot; : 実際のリーフ番号（frame の行名）を返す    "},{"idx":42,"href":"/notebook/r/rstudio/","title":"RStudio","content":"RStudio インストール WSL の Ubuntu 18.04 にインストール Ubuntuの場合と同じ、事前に Rをインストールしておく\nsudo apt-get install gdebi-core wget https://download2.rstudio.org/server/bionic/amd64/rstudio-server-1.3.959-amd64.deb sudo gdebi rstudio-server-1.3.959-amd64.deb しかし、次のようなエラーが出た\n$sudo gdebi rstudio-server-1.3.959-amd64.deb Traceback (most recent call last): File \u0026quot;/usr/bin/gdebi\u0026quot;, line 38, in \u0026lt;module\u0026gt; from GDebi.GDebiCli import GDebiCli File \u0026quot;/usr/share/gdebi/GDebi/GDebiCli.py\u0026quot;, line 103 def get_dependencies_info(self): ロケールを設定して再度インストールして解決\nexport LC_ALL=en_US.UTF-8 RStudio Server の起動 sudo service rstudio-server start その後、ブラウザから http://localhost:8787/ にアクセスする\nおまけ\nWSLにインストールしたRStudio Serverの起動を楽にする\nRStudio Server のアップデート RStudio Server のパージョンを新しくする\n実行中のRセッションの確認\nsudo rstudio-server active-sessions 実行中のRセッションの停止\nsudo rstudio-server suspend-all オフラインモードにする\nsudo rstudio-server offline 新しいバージョンのRStudio Serverをインストール\nDownload RStudio Server\nsudo gdebi \u0026lt;rstudio-server-package.deb\u0026gt; # or sudo yum install --nogpgcheck \u0026lt;rstudio-server-package.rpm\u0026gt; アップデートしたサーバーを再起動\nsudo rstudio-server restart sudo rstudio-server online "},{"idx":43,"href":"/notebook/r/stars/","title":"stars","content":"stars 時空間のハイパーキューブを扱うためのパッケージらしい\nstars\nsfオブジェクトをラスター化する\nst_rasterize(sf, template = st_as_stars(st_bbox(sf), values = NA_real_, ...), file = tempfile(), driver = \u0026quot;GTiff\u0026quot;, options = character(0), ...)   template : ラスターのグリッドを指定する\n st_as_stars(.x = st_bbox(sf), nx = 4064, ny = 3232,values = NA_real_) .x にはバウンディングボックスを指定する nx, ny には縦横のグリッドの数 value は欠測値を埋める    SELECT date ,id_eog ,id_fra ,lon ,lat ,rad ,Lon_DNB ,Lat_DNB ,Rad_DNB ,distance ,IF(distance \u0026lt; 100, 1, 0) AS match_flg FROM scratch_masaki.eog_fra_distance_info_ecs\n "},{"idx":44,"href":"/notebook/r/tidymodels/","title":"tidymodels","content":"tidymodels tidymodels は統一的なインターフェースでモデリングを行うための、いくつかのパッケージをまとめたもの。\n rsample ：訓練データ/テストデータの分割 recipe ：データの前処理（変数変換、サンプリングなど） parsnip ：モデリング yardstic ：モデルの精度評価  前処理 : recipes パッケージ 訓練や新規データへのモデルの適用の前に実施する変数変換やデータサンプリングの手順をレシピオブジェクトとして定義する。\nrecipes パッケージで実施する前処理とは、特徴量作成というよりも、対数変換や標準化などデータの本質的な意味は変えないが、アルゴリズムの学習を適切に行えるようにするための処理のことを指す。\nそのため、この前処理は特徴量作成の後モデリングの直前に実施する。\nレシピオブジェクトの作成 : recipe() 最初に、変数加工のためのパラメタ（標準化のための平均と分散など）を決めるために使う訓練データ（一般には訓練データ）\nそのデータに含まれる目的変数と説明変数をフォーミュラとして与える。\nモデル式（目的変数と説明変数）を指定\nrecipes::recipe(x, formula = NULL, ..., vars = NULL, roles = NULL)  x, data : 変数加工のためのパラメタ（標準化のための平均と分散など）を決めるために使う訓練データ formula : モデル式、目的変数と説明変数の指定。log(x) とか x:y などの関数を含めてはいけない。マイナス符号をつけるのもダメ。 vars : 変数名を格納した文字列ベクトル roles : vars と同じ長さの文字列ベクトル。各変数の役割を指定する。\u0026quot;outcome\u0026quot;, \u0026quot;predictor\u0026quot;, \u0026quot;case_weight\u0026quot;, or \u0026quot;ID\u0026quot; 他の値でもいい  mod_rec \u0026lt;- df_rental %\u0026gt;% recipes::recipe(formula = Y ~ Area) 各前処理ステップの定義 : step_*() データに対する加工は step_*() 関数を使う\ndplyr\n step_filter\tFilter rows using dplyr step_arrange\tSort rows using dplyr step_mutate\tAdd new variables using \u0026lsquo;mutate\u0026rsquo; step_mutate_at\tMutate multiple columns step_rename\tRename variables by name step_rename_at\tRename multiple columns step_slice\tFilter rows by position using dplyr  数値データ変換\n step_center\tCentering numeric data step_cut\tCut a numeric variable into a factor step_discretize\tDiscretize Numeric Variables step_normalize\tCenter and scale numeric data step_log\tLogarithmic Transformation step_logit\tLogit Transformation step_BoxCox\tBox-Cox Transformation for Non-Negative Data step_hyperbolic\tHyperbolic Transformations step_inverse\tInverse Transformation step_invlogit\tInverse Logit Transformation step_range\tScaling Numeric Data to a Specific Range step_relu\tApply (Smoothed) Rectified Linear Transformation step_scale\tScaling Numeric Data step_sqrt\tSquare Root Transformation step_YeoJohnson\tYeo-Johnson Transformation  カテゴリ変数 factor\n step_factor2string\tConvert Factors to Strings step_integer\tConvert values to predefined integers step_bin2factor\tCreate a Factors from A Dummy Variable step_novel\tSimple Value Assignments for Novel Factor Levels step_num2factor\tConvert Numbers to Factors step_other\tCollapse Some Categorical Levels step_ordinalscore\tConvert Ordinal Factors to Numeric Scores step_relevel\tRelevel factors to a desired level step_string2factor\tConvert Strings to Factors step_unknown\tAssign missing categories to \u0026ldquo;unknown\u0026rdquo; step_unorder\tConvert Ordered Factors to Unordered Factors  変数取捨選択\n  step_shuffle\tShuffle Variables\n  step_rm\tGeneral Variable Filter\n  step_zv\tZero Variance Filter\n  step_nzv\tNear-Zero Variance Filter\n  step_corr\tHigh Correlation Filter\n  step_lincomb\tLinear Combination Filter\n  #変数追加\n step_count\tCreate Counts of Patterns using Regular Expressions step_ratio\tRatio Variable Creation  時系列\n step_date\tDate Feature Generator step_holiday\tHoliday Feature Generator step_lag\tCreate a lagged predictor  サンプリング\n step_downsample\tDown-Sample a Data Set Based on a Factor Variable step_sample\tSample rows using dplyr step_upsample\tUp-Sample a Data Set Based on a Factor Variable  統計モデリング\n step_dummy\tDummy Variables Creation step_interact\tCreate Interaction Variables step_intercept\tAdd intercept (or constant) column step_regex\tCreate Dummy Variables using Regular Expressions  地理情報\n step_geodist\tDistance between two locations step_spatialsign\tSpatial Sign Preprocessing  欠損値\n step_naomit\tRemove observations with missing values  補完 : imputation\n step_knnimpute\tImputation via K-Nearest Neighbors step_lowerimpute\tImpute Numeric Data Below the Threshold of Measurement step_meanimpute\tImpute Numeric Data Using the Mean step_medianimpute\tImpute Numeric Data Using the Median step_modeimpute\tImpute Nominal Data Using the Most Common Value step_impute_linear\tImputation of numeric variables via a linear model. step_bagimpute\tImputation via Bagged Trees step_rollimpute\tImpute Numeric Data Using a Rolling Window Statistic  抽出？ : Extraction\n step_ica\tICA Signal Extraction step_kpca\tKernel PCA Signal Extraction step_kpca_poly\tPolynomial Kernel PCA Signal Extraction step_kpca_rbf\tRadial Basis Function Kernel PCA Signal Extraction step_nnmf\tNNMF Signal Extraction step_pca\tPCA Signal Extraction step_pls\tPartial Least Squares Feature Extraction  その他\n  step_bs\tB-Spline Basis Functions\n  step_ns\tNatural Spline Basis Functions\n  step_poly\tOrthogonal Polynomial Basis Functions\n  step_window\tMoving Window Functions\n  step_classdist\tDistances to Class Centroids\n  step_depth\tData Depths\n  step_isomap\tIsomap Embedding\n  step_profile\tCreate a Profiling Version of a Data Set\n  変数変換 step 対数変換 : step_log() step_log(Y, Area)\n交互作用変数の追加 : step_interact() step_interact(terms = ~ Solar.R:Wind)\nようやく tidymodels のお気持ちがわかってきた気がする\nサンプリング・ステップ # サンプリングを含むレシピを作成 (prep()しない) sampling_recipe \u0026lt;- recipes::recipe(Y ~ ., data = train_df) %\u0026gt;% # 目的変数 Y の値に基づいてサンプリングする # 訓練データに対してはサンプリングするが # テストデータや新規データに対してはサンプリングしない場合は skip = TRUE を指定する。 # この時点でサンプリングのための乱数 seed は固定される themis::step_downsample(Y, under_ratio = 0.5, skip = TRUE) %\u0026gt;% # prep() を実行すると skip = TRUE を指定したステップも実行される # retain = TRUE で、最初に渡した train_df にレシピが適用されたデータを # レシピオブジェクトの中に保持する recipes::prep(retain = TRUE) # サンプリングが適用されたデータの作成方法 # 1. レシピオブジェクトの中に存在する # レシピ適用済みデータを取り出す sampled_train_df \u0026lt;- sampling_recipe$template # 2. レシピオブジェクトの中に存在する # レシピ適用済みデータを bake(newdata=NULL) で取り出す sampled_train_df \u0026lt;- sampling_recipe %\u0026gt;% bake(newdata=NULL) # 注意：bake() に新データを渡して新データにレシピを適用したときには、サンプリングステップ は実行されない。(正確は `skip = TRUE` が指定されたステップは実行されない)  NOT_sampled_test_df \u0026lt;- sampling_recipe %\u0026gt;% bake(newdata=test_df) # newdata で渡したデータにはサンプリングは適用されない サンプリングを含んだクロスバリデーション クロスバリデーションをするときには、Validation データにはサンプリングを実施しないが、学習データに対してはサンプリングを実施したい。\n# CVのために（サンプリングをされていない）訓練データを分割する df_cv \u0026lt;- rsample::vfold_cv(train_df, v = 5, repeats = 1) # サンプリングを含むレシピを作成 () # 注意するのはここでは prep() しないということ sampling_recipe \u0026lt;- recipes::recipe(Y ~ ., data = train_df) %\u0026gt;% # テストデータや新規データに対してはサンプリングしない場合は skip = TRUE を指定する。 themis::step_downsample(Y, under_ratio = 0.5, skip = TRUE) # このレシピを使ってクロスバリデーションをする場合には、ここでは prep() しない # おそらく rsample::vfold_cv() の中で prep() される # CV を実行する cv_result_df \u0026lt;- tune::tune_grid( object = rnd_forest_model, # parsnip で作成したモデルオブジェクト preprocessor = sampling_recipe, resamples = df_cv, grid = params_grid_df, # 探索したいパラメータの値が格納されたデータフレーム metrics = yardstick::metric_set(yardstick::pr_auc, yardstick::roc_auc), control = tune::control_grid(verbose = TRUE) ) skip=TRUE について skip=TRUE が指定されたステップは、 recipes::prep(retain = TRUE) を実行したときには適用されるが、recipes::bake(newdata = new_df) を実行したときには適用されない。\n次のように考えてもよいかもしれない\n recipes::prep() は訓練データ作成用にレシピを実行する。 recipes::bake() は新データ・テストデータ・検証データ作成用にレシピを実行する  skip = TRUE を指定された処理は訓練データを作成するときには使用されるが、新データやテストデータや検証データに対しては使用されない。不均衡データ対策のためなど、目的変数を使ったサンプリングを実施するステップに対しては skip = TRUE を指定する。\nstep_*() の中での変数の指定の仕方 step_log(all_predictors(), all_outcomes())\ndplyrの　starts_with() contains() 等も使える\nprep() パラメータを持つステップの訓練・更新 rec_trained \u0026lt;- prep(mod_rec, retain = TRUE, verbose = TRUE)  retain : 処理済みの訓練データをオブジェクト内に保持する (template スロット)。後からステップを追加したいが、既存のステップの再トレーニングを避けたい場合に良いアイデア。skip = FALSE オプションを使用しているステップがある場合は、retain = TRUE を使用することをお勧めします。 training : データ処理のパラメタ（標準化のための平均と分散とか）の訓練のために使うデータフレームを別に指定する場合 fresh : TRUE なら、新たに訓練データを与えてパラメータを更新する。同時に、training に新たな訓練データを与えること。fresh=TRUE は前に prep() した step_*() の方も更新する。fresh=FALSE なら まだ prep() されていない step_*() を更新する。  レシピを新しいデータに適用する bake() 以前は bake() は新データにレシピを適用する。 juice() はレシピ内に保持されたデータに対してレシピを適用するという意味だったが。今は juice() の代わりに bake(newdata=NULL) を使うことが推奨されている。\nデータの分割 : rsample パッケージ Train/Test 分割 : initial_split() set.seed(10) dm_split = rsample::initial_split(processed_dm_df, p = 0.8) train_df = rsample::training(dm_split) test_df = rsample::testing(dm_split) 交差検証　Train/Validation 分割 : vfold_cv() rsample::vfold_cv(data, v = 10, repeats = 1, strata = NULL, breaks = 4, ...)  data : データフレーム v : CVの分割数 (fold数) repeats : CVの分割を何回繰り返すか。総計算回数は v*repeats になる。 strata : 層別サンプリング。CV分割の時に、fold間で割合を一緒にしたい変数を指定する（例えば目的変数がカテゴリ変数の時に指定する） breaks : 整数スカラー。層別サンプリングしたい変数が連続変数の時に、連続地をいくつのビンに分けるか指定する。  Foldのデータを参照する  analysis() : 訓練データを参照する assessment() : 検証データを参照する  df_cv \u0026lt;- rsample::vfold_cv(data, v = 10, repeats = 1) train \u0026lt;- rsample::analysis(df_cv$splits[[1]]) validation \u0026lt;- rsample::assessment(df_cv$splits[[1]]) モデリング : parsnip パッケージ まずは目的に合わせてアルゴリズムを指定して、モデルオブジェクトを作成する。\nparsnip は異なるパッケージのアルゴリズムを同じインターフェースで扱えるようにする。\nモデルオブジェクトの作成 線形回帰 linear_regression_model \u0026lt;- parsnip::linear_reg() %\u0026gt;% parsnip::set_engine(\u0026#34;lm\u0026#34;) 決定木 # 分類木 decision_tree_model \u0026lt;- parsnip::decision_tree( mode = \u0026#34;classification\u0026#34;, cost_complexity = cost_complexity, tree_depth = tree_depth, min_n = min_n) ランダムフォレスト parsnip::rand_forest(mode = \u0026#34;unknown\u0026#34;, mtry = NULL, trees = NULL, min_n = NULL) ## S3 method for class \u0026#39;rand_forest\u0026#39; update( object, parameters = NULL, mtry = NULL, trees = NULL, min_n = NULL, fresh = FALSE, ... )  mode : Possible values for this model are \u0026ldquo;unknown\u0026rdquo;, \u0026ldquo;regression\u0026rdquo;, or \u0026ldquo;classification\u0026rdquo;. mtry: The number of predictors that will be randomly sampled at each split when creating the tree models. trees: The number of trees contained in the ensemble. min_n: The minimum number of data points in a node that are required for the node to be split further.  エンジン\nset_engine()\n R: \u0026ldquo;ranger\u0026rdquo; (the default) or \u0026ldquo;randomForest\u0026rdquo; Spark: \u0026ldquo;spark\u0026rdquo;  モデル学習 モデル学習の実行 : fit() lm_fit \u0026lt;- # 学習済みモデルオブジェクト lm_mod %\u0026gt;% fit(width ~ initial_volume * food_regime, # 目的変数と説明変数をフォーミュラで指定 data = data_mart_df ) 学習済みモデルオブジェクトの内容を確認する tidy(lm_fit) # 線形回帰の学習済みオブジェクトを tidy() で処理した出力を dotwhisker::dwplot() に入力して結果を見る tidy(lm_fit) %\u0026gt;% dotwhisker::dwplot(dot_args = list(size = 2, color = \u0026#34;black\u0026#34;), whisker_args = list(color = \u0026#34;black\u0026#34;), vline = geom_vline(xintercept = 0, colour = \u0026#34;grey50\u0026#34;, linetype = 2)) 予測 : predict mean_pred \u0026lt;- predict( object = lm_fit, new_data = new_points, type = \u0026#34;conf_int\u0026#34; )   object : 学習済みモデルオブジェクト model_fit クラス\n  new_data : 予測を適用するデータ\n  type: 予測で出力する値のタイプ \u0026quot;numeric\u0026quot;, \u0026quot;class\u0026quot;, \u0026quot;prob\u0026quot;, \u0026quot;conf_int\u0026quot;, \u0026quot;pred_int\u0026quot;, \u0026quot;quantile\u0026quot;, or \u0026quot;raw\u0026quot;、指定しない場合はモデルの mode に合わせて適切に選ばれる\n  opts : type = \u0026quot;raw \u0026quot; のときに使用される引数の値を指定したリスト\n  ... : その他の引数\n level : type が \u0026quot;conf_int\u0026quot; または \u0026quot;pred_int\u0026quot; の時、ここで level=0.95 のように区間の値を指定する。 std_error : type が　\u0026quot;conf_int\u0026quot; and \u0026quot;pred_int\u0026quot; のときに、誤差の値も出力するか指定する。 TRUE なら出力する。 FALSE なら出力しない。デフォルトは FALSE  add the standard error of fit or prediction (on the scale of the linear predictors) for types of \u0026ldquo;conf_int\u0026rdquo; and \u0026ldquo;pred_int\u0026rdquo;. Default value is FALSE.\n quantile : the quantile(s) for quantile regression (not implemented yet) time : the time(s) for hazard probability estimates (not implemented yet)    ハイパーパラメータ・チューニング : tune() クロスバリデーションのデータの分割 # Partitioning data for CV df_cv \u0026lt;- rsample::vfold_cv(sampled_train_df, v = 5) チューニングしたいモデルとハイパーパラメータを指定する parsnip パッケージでモデルオブジェクトを作成する際に、交差検証で探索したいハイパーパラメータを選ぶ。そのために、探索したいパラメータの値として tune::tune() を指定する。\n# モデルオブジェクトの作成と # チューニングするハイパーパラメータの指定 rnd_forest_model \u0026lt;- parsnip::rand_forest( mode = \u0026#34;classification\u0026#34;, trees = tune::tune(), min_n = tune::tune(), mtry = tune::tune() ) %\u0026gt;% parsnip::set_engine(\u0026#34;ranger\u0026#34;, num.threads = 10, seed = 42) # 探索するパラメータの範囲を指定した後で、 # その範囲の中からランダムに100個の点を抽出する params_grid_df \u0026lt;- list( min_n = dials::min_n(range = c(100, 1000)), mtry = dials::mtry(range = c(3, 5)), trees = dials::trees(range = c(100, 1000)) ) %\u0026gt;% tune::parameters() %\u0026gt;% dials::grid_random(size = 100) クロスバリデーションの実行 tune::tune_grid() cv_result_df \u0026lt;- tune::tune_grid( object = rnd_forest_model, preprocessor = flag_detected_by_viirs ~ ., resamples = df_cv, grid = params_grid_df, metrics = yardstick::metric_set(yardstick::pr_auc, yardstick::roc_auc), control = tune::control_grid(verbose = TRUE) )  object : parsnip パッケージで作成したモデルオブジェクト、あるいは 内部でモデルを保持した workflows::workflow() で作成したワークフローオブジェクト preprocessor : モデルフォーミュラ、あるいは、recipes::recipe() で作成したレシピオブジェクト resamples : rset オブジェクト、rsample::vfold_cv() の出力結果 param_info : 探索したいハイパーパラメータの値の範囲、dials::parameters() オブジェクト、あるいは NULL grid : 1. param_info=NULLの時、探索したい具体的なパラメータの値が記録されたデータフレーム（カラム名はハイパーパラメータ名と一致する）、 2. 正の整数値、 param_info に値が渡されたとき （param_info の範囲から grid で指定された数のハイパーパラメータ点が探索される） metrics : 計算したい精度指標を yardstick::metric_set() を使って指定する。 NULL なら自動で選ばれる control : チューニングをいじるために使われるオブジェクト。control = tune::control_grid(verbose = TRUE) でCVの経過を出力する  ## S3 method for class \u0026#39;model_spec\u0026#39; tune_grid( object, preprocessor, resamples, ..., param_info = NULL, grid = 10, metrics = NULL, control = control_grid() ) ## S3 method for class \u0026#39;workflow\u0026#39; tune_grid( object, resamples, ..., param_info = NULL, grid = 10, metrics = NULL, control = control_grid() ) tune::tune_bayes() tune::fit_resamples() CVのなかの1つの計算は tune::fit_resamples() が使われている？？\n## S3 method for class \u0026#39;model_spec\u0026#39; fit_resamples( object, preprocessor, resamples, ..., metrics = NULL, control = control_resamples() ) ## S3 method for class \u0026#39;workflow\u0026#39; fit_resamples( object, resamples, ..., metrics = NULL, control = control_resamples() ) クロスバリデーションの結果の確認 tune::collect_metrics(cv_result_df) %\u0026gt;% filter(.metric == \u0026#34;pr_auc\u0026#34;) %\u0026gt;% arrange(desc(mean)) ベストのパラメターで再学習\nbest_param \u0026lt;- cv_result_df %\u0026gt;% select_best(metric = \u0026#34;pr_auc\u0026#34;) %\u0026gt;% select(-.config) model_best \u0026lt;- update(rnd_forest_model, best_param) fitted_model \u0026lt;- fit(model_best, formula = flag_detected_by_viirs ~ ., data = sampled_train_df) 精度検証 : yardstick  yardstick::roc_auc() yardstick::pr_auc() yardstick::gain_curve() yardstick::lift_curve() yardstick::roc_curve()  "},{"idx":45,"href":"/notebook/r/wk/","title":"wk","content":"wk 地理データで登場する Well Known Text (WKT) や Well Known Binary (WKB) 形式のデータを扱うためのパッケージ\nこのパッケージをインストールすると bigrquery パッケージで BigQuery から GEOGRAPHY 型のカラムを含むテーブルをそのままダウンロードすることができるようになる。 wkutils パッケージも一緒にインストールすると良い\n https://cran.r-project.org/package=wk https://paleolimbot.github.io/wk/  GEOMETRY カラムを含む BigQuery テーブルを取得して、それを sf オブジェクトに変換する\n# テーブルの取得 geometry_df \u0026lt;- DBI::dbGetQuery(con, query) # GEOMETRY カラム (`geometry_wkt`) を `sfc` オブジェクトに変換 geometry_sfc \u0026lt;- sf::st_as_sfc(geometry_df$geometry_wkt, crs=4326) # データフレームから GEOMETRY カラムを削除 data_df \u0026lt;- geometry_df %\u0026gt;% select(-geometry_wkt) # データフレームと `sfc` オブジェクトを合体して `sf` オブジェクトを作成する geometry_sf \u0026lt;- sf::st_set_geometry(data_df, geometry_sfc) wkutil https://paleolimbot.github.io/wkutils/index.html\n"},{"idx":46,"href":"/notebook/miscellaneous/","title":"miscellaneous","content":"miscellaneous tmux : ターミナル多重化\n"},{"idx":47,"href":"/notebook/python/environment/","title":"environment","content":"Python 環境設定 https://www.medi-08-data-06.work/entry/python_env\npython と pip の関係\nおそらく Python 実行ファイルには常に対になるpipがある。Pythonをインストールするとpipも一緒にインストールされる。\npyenv python本体のバージョン管理、インストール\nイントールと設定 ~/.zshrc などに以下の記述を追加\nexport PYENV_ROOT=\u0026quot;$HOME/.pyenv\u0026quot; export PATH=\u0026quot;$PYENV_ROOT/bin:$PATH\u0026quot; eval \u0026quot;$(pyenv init -)\u0026quot; Mac brew install pyenv Ubuntu 18.04 sudo apt update \u0026amp;\u0026amp; sudo apt install -y --no-install-recommends \\ build-essential \\ libffi-dev \\ libssl-dev \\ zlib1g-dev \\ libbz2-dev \\ libreadline-dev \\ libsqlite3-dev \\ git # Download pyenv git clone https://github.com/pyenv/pyenv.git ~/.pyenv Windows pyenv-win をインストールする。\nコマンドプロンプトで以下のコマンドを実行する\n以下の場所に pyenv がインストールされる\nC:\\Users\\USERNAME\\.pyenv\\pyenv-win\\bin\npip install pyenv-win --target %USERPROFILE%/.pyenv 新しい環境変数 :\npipenv に pyenv を認識させるため\nPYENV_ROOT : %USERPROFILE%.pyenv PYENV : %USERPROFILE%.pyenv\\pyenv-win\n環境変数PATHの先頭に追加 %PYENV%\\bin %PYENV%\\shims\n環境変数 PYENV_ROOT に %USERPROFILE%\\.pyenv\\pyenv-win をセットすると、pipenv が pyenv を認識するようになるが、pipenv install などを実行するとエラーになる。そのため結局使えない。\nなので pipenv から pyenv を使って自動でPythonをインストールするのはあきらめる。pipenv の環境で使われる Python は pyenv を直接使って別途インストールする。\npyenv を使ったPython本体のインストール pyenv を使ったPython本体のインストール方法\nインストールできるPythonの一覧\npyenv install -l バージョンを指定してインストール\npyenv install 3.8.2 デフォルトで使う Python バージョンを指定する\npyenv local 3.8.2 pyenv rehash # 2系の最新版のバージョン番号を取得 python2=$(pyenv install -l | grep -v '[a-zA-Z]' | grep -e '\\s2\\.?*' | tail -1) # 3.6系の最新版のバージョン番号を取得 python36=$(pyenv install -l | grep -v '[a-zA-Z]' | grep -e '\\s3\\.6?*' | tail -1) # 3.7系の最新版のバージョン番号を取得 python37=$(pyenv install -l | grep -v '[a-zA-Z]' | grep -e '\\s3\\.7?*' | tail -1) # 3.8系の最新版のバージョン番号を取得 python38=$(pyenv install -l | grep -v '[a-zA-Z]' | grep -e '\\s3\\.8?*' | tail -1) # pipenvを使ってインストール pyenv install $python2 pyenv install $python36 pyenv install $python37 pyenv install $python38 # python2 python3 のデフォルトのバージョンに指定 # ログアウトしても引き継がれる pyenv global $python2 $python37 pyenv global $python36 pyenv rehash pipenv プロジェクトで使用する Python とそのパッケージのインストールと管理を行う。\n基本的には１つのフォルダを１つのプロジェクトとして、そのフォルダの中で使いたい python 環境（pythonインタープリタのバージョン、パッケージのバージョン）を pipenv を使って構築すると、構築された Python のバージョン、パッケージのバージョンの情報が Pipfile と Pipfile.lock ファイルに記録される。\nPipfile と Pipfile.lock ファイルはプロジェクトのトップレベルに生成される。後で他の人が Pipfile や Pipfile.lock を使って、同じPython環境を再構築できる。\n Pipfile ユーザーが自分で編集することもできる。 Pipfile.lock こちらは Pipenv により生成されるのでユーザーは自分で編集しない。pipenv lock で生成する。  Pipfile.lock には実際にインストールされた全ての依存パッケージとバージョンが記録される。なので完全に同じ環境が再現できる。\npipenv は、プロジェクトで使う Python インタープリタのインストール・アンインストール、バージョンの切替を行うために、pyenv に依存している。pipenv では最初にプロジェクトで使う Python インタープリタを指定するが、 pipenv は pyenv を使って指定されたPython インタープリタをインストールする。\n github Pipenv: 人間のためのPython開発ワークフロー Pipenvの進んだ使い方  環境とは 環境とは、特定のPython インタープリタとそのパッケージのバージョンが有効になった状態。\npipenv では、1プロジェクト、1フォルダ、1環境になっていると想定する。\nまずはプロジェクトフォルダを作成し、そのフォルダの中に移動する、そしてそのフォルダ内で環境を作成し、その環境に入る、そして環境内で様々なパッケージをインストールし、実際の開発を行う。\nしかし、ある環境に入った時、この環境はこのプロジェクトフォルダの中でだけ有効になるわけではない。一度環境に入ればそのターミナルで作業している限り、プロジェクトフォルダの外でもその環境は有効になっている。なぜなら「環境に入る」と書いたが、正確には pipenv は pipenv shell でそのPython環境が有効になった新たな shell を起動する。つまり、実行中のそのターミナルがその環境に乗っ取られるということ。この環境は、別のターミナルウィンドウで開いた shell には影響しない、あくまでその環境に入ったターミナルの中でだけ有効になる。ただし、プロジェクトフォルダの外で pipenv コマンドを使ってしまうと、そのフォルダに新たな環境を作成してしまうらしい。\nとりあえず、OSにインストールされているPythonとは別に、独立した特定のPythonバージョンを使った開発環境を作成したいということであれば、ダミーのフォルダを作成して、そこをプロジェクトフォルダとして好きなPython環境を構築し、毎回シェルにログインするときに自動的にそのプロジェクト環境が有効になるように .zshrc などにコマンドを記述しておけばよいだろう。\nインストール brew install pipenv ubuntu\nubuntu では pip を使って pipeenv をインストールする。\nsudo apt install python3-pip ~/.local/bin に pip3 がインストールされる\npip3 install pipenv Windows\nWindows では pip を使って pipeenv をインストールする。\nコマンドプロンプト\npip install pipenv 環境の作成 プロジェクト用のフォルダを作成し、移動する\nmkdir myproject cd myproject  このプロジェクトで使用する Python バージョンを指定する。 同時にこのプロジェクト用の仮想環境が作成される この時 pyenv がインストール＆設定されていれば自動でインストールされる。 これにより Pipfile が作成される  pipenv --python 3.7.6 # 下記のような指定も可能 # Python 3を使う場合 # pipenv --three # Python 2を使う場合 # pipenv --two 既存の Pipfile Pipfile.lock を参照して、Pythonとパッケージをインストールすることもできる。\n# Pipfile.lock を参照してパッケージをインストールする pipenv install Pipenv では、開発者が Pipfile にインストールしたいバージョンの希望を書き、Pipfile.lock に実際にインストールしたバージョンが記録される（らしい）。\n Pipfile ユーザーが自分で編集することもできる。 Pipfile.lock こちらは Pipenv により生成されるのでユーザーは自分で編集しない。このファイルには実際にインストールされた全ての依存パッケージとバージョンが記録されている。なので完全に同じ環境が再現できる。  Pipfile，Pipfile.lockから環境の再現 プロジェクトフォルダを作成し、その中に、既存の Pipfile を配置する。そして次のコマンドを実行すると Pipfile に記述された環境が構築される。\npipenv install pipenv install --dev # 通常のパッケージの他に開発用パッケージもインストールしたい場合 同様に、Pipfile.lock が手元にある場合は、以下のコマンド\npipenv sync pipenv sync --dev # 開発用パッケージもインストールする場合 仮想環境の確認 # 現在のプロジェクトのために作成された仮想環境のパスを表示する pipenv --venv 仮想環境の削除 # 現在のプロジェクトの仮想環境を削除 pipenv --rm 既存の環境に対する操作 環境の中に入る 環境が作成されたプロジェクトフォルダに移動して環境の中に入る。正確にはPython環境が有用になったシェルを起動している。実行中のターミナルのpython環境が、ここで入った環境に乗っ取られる。\npipenv shell この環境はこのプロジェクトフォルダの外に出ても有効ではある。\nただし、プロジェクトフォルダの外で pipenv コマンドを使うとそのフォルダに新しい環境が作成されてしまうので注意する。\nライブラリをインストールする pipenv ではライブラリをインストールするときに通常のライブラリと開発用のライブラリを分けることができる。開発用パッケージとは、プログラムの開発の時には必要だが、そのプログラムを実際に動かす運用の時には不要になるライブラリのこと。\n例えば、autopep8 はコードを成形するためのライブラリなので、プログラムの動作そのものには必要ない。\n# 通常のライブラリのインストール pipenv install numpy # 開発向けライブラリのインストール pipenv install --dev autopep8 インストール済パッケージを列挙\npipenv graph 古いパッケージを探す\npipenv update --outdated 古いパッケージを更新\npipenv update pip パッケージのインストール先 pipでインストールされたパッケージのインストール先の確認\npip show [パッケージ名] [user@local] /usr/local/bin/pip3 show matplotlib Name: matplotlib Version: 3.1.2 Summary: Python plotting package Home-page: https://matplotlib.org Author: John D. Hunter, Michael Droettboom Author-email: matplotlib-users@python.org License: PSF Location: /usr/local/lib/python3.7/site-packages Requires: pyparsing, cycler, python-dateutil, kiwisolver, numpy Required-by: Pythonパッケージの検索先 パッケージの検索先の確認法 import sys import pprint # print() より見やすい pprint.pprint(sys.path) インストールされている Python\n/usr/bin/python3 /usr/local/bin/python3 /usr/local/opt/python/bin/python3.7 $HOME/.pyenv/versions/3.7.6/bin/python3.7 Mac 標準の python3\n/usr/bin/python3\nPython 3.7.3 (default, Dec 13 2019, 19:58:14) [Clang 11.0.0 (clang-1100.0.33.17)] on darwin ['', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python37.zip', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/lib-dynload', '/Users/tsuda/Library/Python/3.7/lib/python/site-packages', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/site-packages'] Homebrew でいれた Python3\n/usr/local/bin/python3\nPython 3.7.4 (default, Sep 7 2019, 18:27:02) [Clang 10.0.1 (clang-1001.0.46.4)] on darwin ['', '/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python37.zip', '/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7', '/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload', '/Users/tsuda/Library/Python/3.7/lib/python/site-packages', '/usr/local/lib/python3.7/site-packages', '/usr/local/lib/python3.7/site-packages/geos', '/usr/local/Cellar/numpy/1.16.4_1/libexec/nose/lib/python3.7/site-packages'] Homebrew でいれた Python3?\n/usr/local/opt/python/bin/python3.7\nPython 3.7.4 (default, Sep 7 2019, 18:27:02) [Clang 10.0.1 (clang-1001.0.46.4)] on darwin ['', '/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python37.zip', '/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7', '/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload', '/Users/tsuda/Library/Python/3.7/lib/python/site-packages', '/usr/local/lib/python3.7/site-packages', '/usr/local/lib/python3.7/site-packages/geos', '/usr/local/Cellar/numpy/1.16.4_1/libexec/nose/lib/python3.7/site-packages'] pyenv で入れた Python\n.pyenv/versions/3.7.6/bin/python3.7\nPython 3.7.6 (default, Mar 14 2020, 19:17:31) [Clang 11.0.0 (clang-1100.0.33.17)] on darwin ['', '/Users/tsuda/.pyenv/versions/3.7.6/lib/python37.zip', '/Users/tsuda/.pyenv/versions/3.7.6/lib/python3.7', '/Users/tsuda/.pyenv/versions/3.7.6/lib/python3.7/lib-dynload', '/Users/tsuda/.pyenv/versions/3.7.6/lib/python3.7/site-packages'] パッケージの検索先への追加 sys.path.append() を使う方法\nこれはこのときの実行限りで有効になる\n# スクリプトファイルの１階層上のディレクトリを追加 import os import sys sys.path.append(os.path.join(os.path.dirname(__file__), \u0026#39;..\u0026#39;)) PYTHONPATH に追加する方法\nPython2 と Python3 で PYTHONPATH を使い分けられない\nexport PYTHONPATH=\u0026#34;/path/to/add:$PYTHONPATH\u0026#34;` '/usr/local/lib/python3.7/site-packages', import os import sys sys.path.append('/usr/local/lib/python3.7/site-packages') "},{"idx":48,"href":"/notebook/python/jupyter/","title":"jupyter","content":"jypyter インストール Jupyter インストール プロジェクトフォルダに移動して、仮想環境に入ってから\ncd Project pipenv shell pipenv install jupyter jupyter を起動する jupyter notebook pipenv の仮想環境の中の jupyter を起動する場合。\npipenv プロジェクトフォルダの「外」でこのコマンドを打つと、新しい pipenv 環境が作成されてしまう\npipenv run jupyter notebook jupytext jupytext は jupyter 経由で .ipynb と .py .Rmd などを同期するツール\nインストール\ncd Project pipenv shell pipenv install jupytext 設定\njupyter の設定ファイル（~/.jupyter/jupyter_notebook_config.py）を生成する。\npipenv run jupyter notebook --generate-config 以下の記述を追加\n# jupytext を使用可能にする設定 c.NotebookApp.contents_manager_class = \u0026quot;jupytext.TextFileContentsManager\u0026quot; # jupytext で、 `.ipynb` と `.py` と `.Rmd` が連動（同期）するようにする設定 c.ContentsManager.default_jupytext_formats = \u0026quot;ipynb,py,Rmd\u0026quot; 既存の notebook を編集する場合 jupyter の Edit \u0026gt; Edit Notebook Metadata から、以下の記述を先頭に追加する。\n\u0026quot;jupytext\u0026quot;: {\u0026quot;formats\u0026quot;: \u0026quot;ipynb,py,Rmd\u0026quot;}, ここで .py, .ipynb 以外にも、md, Rmd, jl, R などのフォーマットが使える。\n既存の notebook のメタデータを上記のように編集し、上書き保存すると .ipynb に対応する .py が生成されている。\n.ipynb を編集すると .py に変更が反映され、.py を編集すると .ipynb に変更が反映されるらしい。\n"},{"idx":49,"href":"/notebook/python/vscode/","title":"Python in VScode","content":"Python in VScode VScode で Python を使う時の Tips\n参考：[VS Code でPython，Jupyter を動かす](https://qii ta.com/surei/items/9f25d7efa7c67d55d98f)\n拡張機能  Python 必須  はじめに コマンドパレットで select interpreter と入力\nVScodeと一緒に使いたい python インタープリタを選択する\n"},{"idx":50,"href":"/notebook/python/anaconda/","title":"anaconda","content":"Anaconda minoconda\nhttps://conda.io/projects/conda/en/latest/user-guide/concepts/environments.html\n"},{"idx":51,"href":"/notebook/r/dbi/","title":"DBI","content":"DBI DBIパッケージはRからデータベース（DB）とやりとりするためのインターフェースを提供している。これにより、ドライバーを切り替えるだけで、共通のインターフェースを用いて様々な種類のDBサーバーとやりとりすることができる。\nDBIパッケージはRとデータベース（DB）がやりとりするための基本的な関数を提供している。それぞれの関数の引数などは各DBのドライバーを提供している別のパッケージ（ RPostgreSQL や bigquery など）によって動作が細かく指定できるように拡張されているので、そちらのマニュアルを参照すること。\nオブジェクト DBIパッケージでは主に下の３種類のオブジェクトが登場する。\nDBIDriver: ドライバー・オブジェクト drv\n dbDriver() RSQLite::RSQLite(), RPostgreSQL::RPostgreSQL(), RMySQL::RMySQL(), bigrquery::bigquery() などの関数が返すオブジェクト  DBIConnection: DBコネクション・オブジェクト con\ndbConnect()が返すオブジェクト\nDBIResult: クエリ結果のオブジェクト res\ndbSendquery()が返すオブジェクト\nドライバ・コネクション・クエリ結果の情報：dbGetInfo() dbGetInfo()\n接続したいDBへのドライバーをを用意する DBIパッケージに対応した、各DBへのドライバを、提供するパッケージをインストールする。\n RPostgreSQL RMySQL RSQLite bigquery  ドライバを用意する。下の２つの方法は等価。\ndrv \u0026lt;- PostgreSQL() drv \u0026lt;- dbDriver(\u0026quot;PostgreSQL\u0026quot;) ドライバを閉じる：dbUnloadDriver\ndbUnloadDriver(drv) DBサーバーへ接続する：dbConnect ドライバーは例えば以下がある。各パッケージをインストールする。\n RSQLite::RSQLite() RPostgreSQL::RPostgreSQL() RMySQL::RMySQL()  例）PostgreSQLへの接続\ndrv \u0026lt;- PostgreSQL() con \u0026lt;- dbConnect(drv, host=\u0026quot;localhost\u0026quot;, user= \u0026quot;edd\u0026quot;, password=\u0026quot;.....\u0026quot;, dbname=\u0026quot;...\u0026quot;) パスワードをRのソースに直接記述するのはセキュリティ上よろしくない。ファイルに書いておいてそれを読み出すようにする。そうすればRのソースを共有する場合にも安心である。\n例えば \u0026ldquo;.pgpass\u0026rdquo; というファイルにパスワードを保存してきそれを読み出す場合\npassword \u0026lt;- scan(\u0026quot;.pgpass\u0026quot;, what=\u0026quot;\u0026quot;) 接続の設定\nPostgreSQL() は接続の設定を変えられる。\nPostgreSQL(max.con = 16, fetch.default.rec = 500, force.reload = FALSE)  max.con：最大コネクション数 fetch.default.rec：データを取得するときに一度に送信するレコード数。fetch()はこの値を利用する。 force.reload：クライアントのコードをリロードするか。イミフ  コネクションの情報を表示する summary(con) コネクションを解除する dbDisconnect(con) ## Closes the connection データフレームの内容からテーブルを作成する：dbWriteTable dbWriteTable(con, \u0026quot;iris\u0026quot;, iris, row.names=FALSE) dbWriteTable(conn, name, value, ...)  overwrite=TRUE : テーブルを上書きする append=TRUE : 新しい行を追加する  dbWriteTable(con, name = \u0026quot;sillytable\u0026quot;, #テーブル名 value = data.frame( #値 time=seq(Sys.time(), by=\u0026quot;1 day\u0026quot;, length=10), value=rnorm(10)), row.names=FALSE) テーブルのリスト：dbListTables dbListTables(con) テーブルのカラム名：dbListFields dbListFields(con, \u0026quot;iris\u0026quot;) DBのデータを取得する テーブルを指定して読み込む：dbReadTable iris1 \u0026lt;- dbReadTable(con, \u0026quot;iris\u0026quot;) クエリの結果を読み込む：dbGetQuery data \u0026lt;- dbGetQuery(con, \u0026quot;SELECT * FROM iris ORDER BY weighted DESC LIMIT 5\u0026quot;) クエリの送信とデータの取得を分離する：dbSendQuery \u0026amp; fetch 上と同様クエリの結果を取得するが、クエリの送信とデータの取得を分離する。\nクエリを送信する\nrs \u0026lt;- dbSendQuery(con, \u0026quot;SELECT * FROM iris\u0026quot;) 最初の10レコードだけ取得する\niris3.first10 \u0026lt;- fetch(rs, 10) 残りを全て取得する\niris3.rest \u0026lt;- fetch(rs, -1) fetch はカーソルを移動させながらデータを取得する。なので上記の場合には iris3.first10 と iris3.rest 合体させると全レコードになる。\nrbind(iris3.first10, iris3.rest) 【重要】ローカルとリモート確保されたリソースを開放する\ndbSendQueryの結果はリモートのリソースを消費するので必要がなくなったら dbClearResult(rs) すること。\ndbClearResult(rs) ファイルからクエリを読み込んで実行する fileName\u0026lt;-\u0026quot;test.sql\u0026quot; q\u0026lt;-readChar(fileName, file.info(fileName)$size) res \u0026lt;- dbSendQuery(con, q) クエリ結果のリソースを開放する：dbClearResult 前のクエリの結果を全て取得していないうちに、同じコネクションで、次のクエリを実行しようとしてもできない。\nrs \u0026lt;- dbSendQuery(con, \u0026quot;SELECT * FROM iris\u0026quot;) #前のクエリ rs \u0026lt;- dbSendQuery(con, \u0026quot;SELECT * FROM hoge\u0026quot;) #次のクエリ（エラー） 実行する場合には、前のクエリの結果をクリアする必要がある。\ndbClearResult(con, rs) rs \u0026lt;- dbSendQuery(con, \u0026quot;SELECT * FROM hoge\u0026quot;) #OK dbSendQuery() するとサーバーでクエリが実行され、サーバー上に結果が保存されるらしい、そのままにしておくとメモリなどのリソースを消費するので、必要なくなった結果は適宜開放する。\nテーブルを削除する：dbRemoveTable dbRemoveTable(conn,\u0026quot;table1\u0026quot;) カラム情報を表示する：dbColumnInfo(res, \u0026hellip;) dbColumnInfo(rs) ## name Sclass type len precision scale nullOK ## 1 Sepal.Length double FLOAT8 8 -1 -1 TRUE ## 2 Sepal.Width double FLOAT8 8 -1 -1 TRUE ## 3 Petal.Length double FLOAT8 8 -1 -1 TRUE ## 4 Petal.Width double FLOAT8 8 -1 -1 TRUE ## 5 Species character TEXT -1 -1 -1 TRUE 結果の元クエリを表示：dbGetStatement dbGetStatement(rs) ## [1] \u0026quot;SELECT * FROM iris\u0026quot; ローカルにあるクエリ結果のレコード数：dbGetRowCount(rs) dbGetRowCount(rs) ## [1] 10 # ... just get the first 10 records 結果のうち、ローカルに送られてきたレコード数？\nテーブルの存在を確認：dbExistsTable dbExistsTable(con, c(\u0026quot;tmp\u0026quot;,\u0026quot;test_tbl\u0026quot;)) クエリ結果オブジェクトのリスト：dbListResults 現在のコネクションでアクティブな DBIResult のリストを返す。\ndbClearResults(dbListResults(con)[[1]]) 現在開いているコネクション・オブジェクトのリスト：dbListConnections オブジェクトの型を調べる：dbDataType DBのでの例外（エラー情報）を取得する：dbGetException データ変更クエリにより影響を受ける行数：dbGetRowsAffected クエリ結果に対する処理が完了しているか？：dbHasCompleted DBオブジェクトの状態が正常かチェックする：dbIsValid "},{"idx":52,"href":"/notebook/","title":"Introduction","content":"for my personal use "}];window.bookSearch={pages:pages,idx:lunr(function(){this.ref("idx");this.field("title");this.field("content");pages.forEach(this.add,this);}),}})();